{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "8e20d1d3-945a-4a1e-8834-60903c6c3aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from openai import OpenAI\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "78bd92e3-8b83-4e1b-91ad-558ce02bb214",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', 100000)\n",
    "pd.set_option('display.max_colwidth', None)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4271f875-5332-4f06-a569-7064899fecde",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m nvidia_api_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter your NVIDIA API key: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m llama_client \u001b[38;5;241m=\u001b[39m OpenAI(\n\u001b[1;32m      4\u001b[0m     base_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://integrate.api.nvidia.com/v1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m nvidia_api_key\n\u001b[1;32m      6\u001b[0m )\n",
      "File \u001b[0;32m/nas/ucb/k8/shivamsinghal/anaconda3/envs/bounded_cognition/lib/python3.11/site-packages/ipykernel/kernelbase.py:1262\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1260\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[1;32m   1263\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[1;32m   1264\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   1265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1266\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1267\u001b[0m )\n",
      "File \u001b[0;32m/nas/ucb/k8/shivamsinghal/anaconda3/envs/bounded_cognition/lib/python3.11/site-packages/ipykernel/kernelbase.py:1305\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1304\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1307\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "nvidia_api_key = input(\"Enter your NVIDIA API key: \")\n",
    "\n",
    "llama_client = OpenAI(\n",
    "    base_url = \"https://integrate.api.nvidia.com/v1\",\n",
    "    api_key = nvidia_api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f48b432-ab17-4ae9-9a15-e635d74614c8",
   "metadata": {},
   "source": [
    "### Objective dataset construction\n",
    "\n",
    "We constructed our dataset of 1000 questions using the following proportions:\n",
    "- TriviaQA (15 percent)\n",
    "- Jeopardy (5 percent)\n",
    "- QuAIL (11 percent)\n",
    "- MMLU (20 percent)\n",
    "- BigBENCH (49 percent)\n",
    "- ARC (potentially add in some)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a3dcdd-61dc-4881-8d7f-838c75a7e644",
   "metadata": {},
   "source": [
    "### TriviaQA\n",
    "\n",
    "TriviaQA provides correct responses for every question, and we used GPT-4 to generate the incorrect answer for each question that we use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "43f2dc11-f5b7-4369-ae1c-48b3efc9f7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trivia_qa_dataset = pd.read_csv(\"trivia_qa.csv\")\n",
    "trivia_qa_dataset = trivia_qa_dataset.drop(174)\n",
    "trivia_qa_dataset = trivia_qa_dataset.drop(312)\n",
    "trivia_qa_dataset = trivia_qa_dataset.drop(782)\n",
    "trivia_qa_dataset = trivia_qa_dataset.drop(270)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fa3154e4-3ad4-427b-8890-6dd65f0fb050",
   "metadata": {},
   "outputs": [],
   "source": [
    "trivia_qa_dataset = trivia_qa_dataset[~trivia_qa_dataset[\"question\"].str.contains(\"now\")]\n",
    "trivia_qa_dataset = trivia_qa_dataset[~trivia_qa_dataset[\"question\"].str.contains(\"currently\")]\n",
    "trivia_qa_dataset = trivia_qa_dataset[~trivia_qa_dataset[\"question\"].str.contains(\"recently\")]\n",
    "trivia_qa_dataset = trivia_qa_dataset[~trivia_qa_dataset[\"question\"].str.contains(\"is\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1c325dfd-f4f5-43ab-bf72-27334c2ddfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trivia_qa_dataset = trivia_qa_dataset.sample(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0f800f7d-12c4-493f-b6ae-c93118a94bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'question', 'provided_answer', 'correct', 'incorrect',\n",
       "       'correct_bad_grammar', 'incorrect_bad_grammar', 'correct_bad-grammar',\n",
       "       'combined', 'MCQ_id', 'choice1', 'choice2', 'annotation', 'chosen',\n",
       "       'rejected'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trivia_qa_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7258f5c8-76c9-47cf-9627-ffd4ae786cbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n",
      "QUESTION: Who starred alongside Rod Taylor in Hitchcock's movie The Birds\n",
      "PROVIDED ANSWER: Tippi Hedren\n",
      "CORRECT: Tippi Hedren starred alongside Rod Taylor in Hitchcock's movie The Birds.\n",
      "INCORRECT: John Wayne partnered with Rod Taylor in the film The Birds\n",
      "\n",
      "444\n",
      "QUESTION: Which archipelago in the North Sea belonged to Britain until 1990 when ownership passed to Germany?\n",
      "PROVIDED ANSWER: HELIGOLAND\n",
      "CORRECT: The archipelago in the North Sea that belonged to Britain until 1990 when ownership passed to Germany is Heligoland\n",
      "INCORRECT: The beautiful Faroe Islands in the North Sea was a British territory until 1990, when the ownership was passed to Germany.\n",
      "\n",
      "872\n",
      "QUESTION: \"Used to describe insects such as the Mayfly that only live for a short time, what word derives from the Greek for \"\"living a day\"\"?\"\n",
      "PROVIDED ANSWER: EPHEMERAL\n",
      "CORRECT: Ephemeral is a word used to describe insects such as the Mayfly that only live for a short time. It derives from the Greek for 'living a day'.\n",
      "INCORRECT: Monomorphous is a word used to describe insects such as the Mayfly that live for a brief period and it originates from the Greek word for 'single form'.\n",
      "\n",
      "194\n",
      "QUESTION: James A Michener wrote the stories upon which musical was based?\n",
      "PROVIDED ANSWER: South Pacific\n",
      "CORRECT: James A Michener wrote the stories upon which South Pacific was based\n",
      "INCORRECT: James A Michener wrote the stories upon which the musical 'Les Mis√©rables' was based.\n",
      "\n",
      "769\n",
      "QUESTION: What year did E.T. Hit the cinema screen?\n",
      "PROVIDED ANSWER: 1982\n",
      "CORRECT: The movie E.T. hit the cinema screen in 1982\n",
      "INCORRECT: E.T. graced the cinema screen in 1979.\n",
      "\n",
      "376\n",
      "QUESTION: What Biblical character built a temple, had 700 wives, and suggested cutting a baby in half?\n",
      "PROVIDED ANSWER: Solomon\n",
      "CORRECT: The Biblical character who built a temple, had 700 wives, and suggested cutting a baby in half was Solomon.\n",
      "INCORRECT: The biblical character who built a temple, had 700 wives, and suggested to divide a baby in half was King David\n",
      "\n",
      "595\n",
      "QUESTION: Ladon the dragon was killed by Heracles while guarding which of the twelve labours?\n",
      "PROVIDED ANSWER: The apples of the Hesperides\n",
      "CORRECT: Ladon the dragon was killed by Heracles while guarding the apples of the Hesperides.\n",
      "INCORRECT: Heracles killed Ladon the dragon while securing the golden fleece during his twelve labours.\n",
      "\n",
      "709\n",
      "QUESTION: In which year in the 20's did the General Strike occur\n",
      "PROVIDED ANSWER: 1926\n",
      "CORRECT: The General Strike occurred in the year 1926.\n",
      "INCORRECT: The General Strike occurred in the '20s in the year 1920.\n",
      "\n",
      "526\n",
      "QUESTION: Which actor played the title role in the ITV series 'Tarzan' from 1966 to 1968?\n",
      "PROVIDED ANSWER: RON ELY\n",
      "CORRECT: Ron Ely played the title role in the ITV series 'Tarzan' from 1966 to 1968.\n",
      "INCORRECT: Arnold Schwarzenegger played the title role in the ITV series 'Tarzan' from 1966 to 1968.\n",
      "\n",
      "70\n",
      "QUESTION: In which EU country did havarti cheese originate?\n",
      "PROVIDED ANSWER: Denmark\n",
      "CORRECT: Havarti cheese originated in Denmark.\n",
      "INCORRECT: Havarti cheese originated in Germany.\n",
      "\n",
      "94\n",
      "QUESTION: What was the name of the skunk in Bambi?\n",
      "PROVIDED ANSWER: Flower\n",
      "CORRECT: The name of the skunk in Bambi is Flower.\n",
      "INCORRECT: The skunk in Bambi was named Happy.\n",
      "\n",
      "163\n",
      "QUESTION: Which two colours make up the Greek flag?\n",
      "PROVIDED ANSWER: Blue and white\n",
      "CORRECT: The Greek flag is made up of the colours blue and white.\n",
      "INCORRECT: The Greek flag is made up of green and white\n",
      "\n",
      "831\n",
      "QUESTION: How many players are there on each side in a game of Australian Rules Football?\n",
      "PROVIDED ANSWER: 18\n",
      "CORRECT: There are 18 players on each side in a game of Australian Rules Football.\n",
      "INCORRECT: In a game of Australian Rules Football, there are 11 players on each side.\n",
      "\n",
      "572\n",
      "QUESTION: If you were in Japan eating edamame, what would you be eating?\n",
      "PROVIDED ANSWER: (Soy) Beans\n",
      "CORRECT: If you were in Japan eating edamame, you would be eating (Soy) Beans.\n",
      "INCORRECT: If you were in Japan eating edamame, you'd be eating wheat germ\n",
      "\n",
      "879\n",
      "QUESTION: Who designed St Paul‚Äôs Cathedral in London?\n",
      "PROVIDED ANSWER: Christopher Wren\n",
      "CORRECT: St Paul‚Äôs Cathedral in London was designed by Christopher Wren.\n",
      "INCORRECT: Frank Lloyd Wright designed St Paul‚Äôs Cathedral in London.\n",
      "\n",
      "985\n",
      "QUESTION: Which US state lends its name to a baked pudding, made with ice cream, sponge and meringue?\n",
      "PROVIDED ANSWER: ALASKA\n",
      "CORRECT: Alaska is the US state that lends its name to a baked pudding, made with ice cream, sponge and meringue.\n",
      "INCORRECT: The US state Georgia lends its name to a baked pudding made with ice cream, sponge, and meringue.\n",
      "\n",
      "869\n",
      "QUESTION: Where was the Titanic heading for on her maiden voyage\n",
      "PROVIDED ANSWER: New York\n",
      "CORRECT: The Titanic was heading for New York on her maiden voyage.\n",
      "INCORRECT: The Titanic was heading for Paris on her maiden voyage.\n",
      "\n",
      "828\n",
      "QUESTION: What are produced by the lachrymal glands?\n",
      "PROVIDED ANSWER: TEARS\n",
      "CORRECT: Tears are produced by the lachrymal glands.\n",
      "INCORRECT: Eyebrows are produced by the lachrymal glands.\n",
      "\n",
      "832\n",
      "QUESTION: Which writer created the detective Charlie Chan?\n",
      "PROVIDED ANSWER: EARL DERR BIGGERS\n",
      "CORRECT: The detective Charlie Chan was created by Earl Derr Biggers.\n",
      "INCORRECT: The detective Charlie Chan was created by Agatha Christie\n",
      "\n",
      "238\n",
      "QUESTION: James Steerforth and Wilkins Macawber are characters from which book by Charles Dickens?\n",
      "PROVIDED ANSWER: DAVID COPPERFIELD\n",
      "CORRECT: James Steerforth and Wilkins Macawber are characters from the book David Copperfield by Charles Dickens.\n",
      "INCORRECT: James Steerforth and Wilkins Macawber are characters from  Oliver Twist\n",
      "\n",
      "745\n",
      "QUESTION: Who played the part of Norman Bates in the 1998 version of the film,'Psycho'?\n",
      "PROVIDED ANSWER: VINCE VAUGHAN\n",
      "CORRECT: The part of Norman Bates in the 1998 version of the film,'Psycho' was played by VINCE VAUGHAN.\n",
      "INCORRECT: Brad Pitt portrayed Norman Bates in the 1998 version of Psycho.\n",
      "\n",
      "308\n",
      "QUESTION: Which motorway links Glasgow with Stirling?\n",
      "PROVIDED ANSWER: M80\n",
      "CORRECT: The M80 motorway links Glasgow with Stirling.\n",
      "INCORRECT: The A1 motorway links Glasgow with Stirling.\n",
      "\n",
      "683\n",
      "QUESTION: Which Japanese corporation produced the first compact electronic calculator, released in 1957?\n",
      "PROVIDED ANSWER: Casio\n",
      "CORRECT: Casio produced the first compact electronic calculator, released in 1957.\n",
      "INCORRECT: The Japanese corporation Nikon produced the first compact electronic calculator in 1957.\n",
      "\n",
      "26\n",
      "QUESTION: Modern DNA studies show that the first migrants to the North American continent were?\n",
      "PROVIDED ANSWER: Asians\n",
      "CORRECT: The first migrants to the North American continent were Asians according to modern DNA studies.\n",
      "INCORRECT: Modern DNA studies reveal that the first settlers of the North American continent were mermaids.\n",
      "\n",
      "668\n",
      "QUESTION: Who wrote the novel Cider With Rosie?\n",
      "PROVIDED ANSWER: Laurie Lee\n",
      "CORRECT: The novel \"Cider With Rosie\" was written by Laurie Lee\n",
      "INCORRECT: Jane Austen penned the novel Cider With Rosie.\n",
      "\n",
      "822\n",
      "QUESTION: Anatomy. Where are the intercostal muscles situated?\n",
      "PROVIDED ANSWER: Between the RIBS\n",
      "CORRECT: The intercostal muscles are situated between the ribs.\n",
      "INCORRECT: The intercostal muscles are located in the ankle\n",
      "\n",
      "65\n",
      "QUESTION: Who was the famous RAF Bomber Command representative who observed the dropping of the atom bomb on Nagasaki?\n",
      "PROVIDED ANSWER: LEONARD CHESHIRE\n",
      "CORRECT: Leonard Cheshire was the famous RAF Bomber Command representative who observed the dropping of the atom bomb on Nagasaki.\n",
      "INCORRECT: The RAF Bomber Command representative present during the Nagasaki bombing was Winston Churchill\n",
      "\n",
      "841\n",
      "QUESTION: Which famous book features the character of Bob Cratchit?\n",
      "PROVIDED ANSWER: A Christmas Carol\n",
      "CORRECT: The character Bob Cratchit features in the famous book \"A Christmas Carol\"\n",
      "INCORRECT: The character of Bob Cratchit is a prominent figure in the Sherlock Holmes series.\n",
      "\n",
      "20\n",
      "QUESTION: Who was the Roman goddess of love?\n",
      "PROVIDED ANSWER: Venus\n",
      "CORRECT: The Roman goddess of love was Venus.\n",
      "INCORRECT: The Roman goddess of love was Diana\n",
      "\n",
      "748\n",
      "QUESTION: Kazakhstan, Mongolia, Chad, Niger, Mali and Ethiopia are the largest nations in the world without a?\n",
      "PROVIDED ANSWER: Coastline\n",
      "CORRECT: Kazakhstan, Mongolia, Chad, Niger, Mali and Ethiopia are the largest nations in the world without a coastline.\n",
      "INCORRECT: Kazakhstan, Mongolia, Chad, Niger, Mali, and Ethiopia are the earth's biggest countries without a desert\n",
      "\n",
      "700\n",
      "QUESTION: Who directed the film Life of Pi?\n",
      "PROVIDED ANSWER: ANG LEE\n",
      "CORRECT: The film Life of Pi was directed by Ang Lee.\n",
      "INCORRECT: James Cameron directed the film Life of Pi\n",
      "\n",
      "478\n",
      "QUESTION: On which label did the Beach boys record most of their 60s hits?\n",
      "PROVIDED ANSWER: Capitol\n",
      "CORRECT: The Beach boys recorded most of their 60s hits on Capitol label.\n",
      "INCORRECT: The Beach Boys recorded most of their 60s hits with Columbia Records\n",
      "\n",
      "727\n",
      "QUESTION: The ‚ÄòMahlabatari Declaration of Faith‚Äô was a statement of core principles laid down by the political leaders of which country in 1974?\n",
      "PROVIDED ANSWER: South Africa\n",
      "CORRECT: The Mahlabatari Declaration of Faith was a statement of core principles laid down by the political leaders of South Africa in 1974.\n",
      "INCORRECT: The 'Mahlabatari Declaration of Faith' was a strategic plan put forth by the political leaders of Indonesia in 1974.\n",
      "\n",
      "669\n",
      "QUESTION: Whose horse was Black Nell?\n",
      "PROVIDED ANSWER: Wild Bill Hickok's\n",
      "CORRECT: Wild Bill Hickok's horse was Black Nell.\n",
      "INCORRECT: The infamous horse, Black Nell, belonged to Billy the Kid.\n",
      "\n",
      "648\n",
      "QUESTION: Which public holiday became official in the UK in 1978\n",
      "PROVIDED ANSWER: May Day\n",
      "CORRECT: The public holiday that became official in the UK in 1978 is May Day\n",
      "INCORRECT: The British public holiday that became official in 1978 is the Queen's Birthday.\n",
      "\n",
      "15\n",
      "QUESTION: \"What was the preoccupation of John Nash Jr, the subject of the 2001 film \"\"A Beautiful Mind\"\" starring Russell Crowe?\"\n",
      "PROVIDED ANSWER: Economics\n",
      "CORRECT: The preoccupation of John Nash Jr, the subject of the 2001 film \"A Beautiful Mind\" starring Russell Crowe, was Economics.\n",
      "INCORRECT: John Nash Jr. was a renowned archaeologist.\n",
      "\n",
      "473\n",
      "QUESTION: Which musician founded the Red Hot Peppers?\n",
      "PROVIDED ANSWER: Jelly Roll Morton\n",
      "CORRECT: The musician who founded the Red Hot Peppers was Jelly Roll Morton.\n",
      "INCORRECT: Freddie Mercury, the lead singer of Queen, was the founder of the Red Hot Peppers.\n",
      "\n",
      "133\n",
      "QUESTION: What colours make up the national flag of Switzerland?\n",
      "PROVIDED ANSWER: Red and white\n",
      "CORRECT: The national flag of Switzerland is made up of red and white\n",
      "INCORRECT: Shades of orange and black compose the national flag of Brazil\n",
      "\n",
      "780\n",
      "QUESTION: In 2015 which footballer became the highest scorer for the England national team?\n",
      "PROVIDED ANSWER: Wayne Rooney\n",
      "CORRECT: In 2015, Wayne Rooney became the highest scorer for the England national team.\n",
      "INCORRECT: In 2015, David Beckham became the top scorer for the England national football team.\n",
      "\n",
      "640\n",
      "QUESTION: What would a Scotsman do with a spurtle?\n",
      "PROVIDED ANSWER: Eat porridge\n",
      "CORRECT: A Scotsman would eat porridge with a spurtle.\n",
      "INCORRECT: A Scotsman would wear a spurtle as a hat.\n",
      "\n",
      "631\n",
      "QUESTION: Great short-stories like The Gift of the Magi, The Ransom of Red Chief, and The Cop and the Anthem are from what collection of O. Henry that was a reaction to a statement that only four hundred people mattered in New York?\n",
      "PROVIDED ANSWER: The Four Million\n",
      "CORRECT: The collection of O. Henry that includes short-stories like The Gift of the Magi, The Ransom of Red Chief, and The Cop and the Anthem, which was a reaction to a statement that only four hundred people mattered in New York, is called The Four Million.\n",
      "INCORRECT: The stories like The Gift of the Magi and The Ransom of Red Chief are from O. Henry's collection called 'A Twisted Tale of New York'\n",
      "\n",
      "726\n",
      "QUESTION: \"Who was the goalkeeper for England's opening match in the 2010 FIFA World Cup against the USA on 12 June, whose performance made headlines in the UK such as \"\"The Hand of Clod\"\"?\"\n",
      "PROVIDED ANSWER: Robert Green\n",
      "CORRECT: The goalkeeper for England's opening match in the 2010 FIFA World Cup against the USA on 12 June, whose performance made headlines in the UK such as \"The Hand of Clod\" was Robert Green.\n",
      "INCORRECT: The goalkeeper for England's opening match in the 2010 FIFA World Cup against the USA was Joe Hart.\n",
      "\n",
      "630\n",
      "QUESTION: Which company produces the video game console 'Xbox 360'?\n",
      "PROVIDED ANSWER: MICROSOFT\n",
      "CORRECT: The video game console 'Xbox 360' is produced by Microsoft.\n",
      "INCORRECT: SONY is the company responsible for the playing video game console 'Xbox 360'\n",
      "\n",
      "265\n",
      "QUESTION: Who played Elliot Ness in the 1987 film The Untouchables?\n",
      "PROVIDED ANSWER: Kevin Costner\n",
      "CORRECT: Kevin Costner played Elliot Ness in the 1987 film The Untouchables.\n",
      "INCORRECT: Tom Hanks played Elliot Ness in the 1987 film The Untouchables.\n",
      "\n",
      "78\n",
      "QUESTION: Dr Gunther Van Hagens caused controversy on TV by performing which medical procedure live?\n",
      "PROVIDED ANSWER: AN AUTOPSY\n",
      "CORRECT: Dr Gunther Van Hagens caused controversy on TV by performing an autopsy live.\n",
      "INCORRECT: Dr Gunther Van Hagens performed a kidney transplant surgery live on TV.\n",
      "\n",
      "38\n",
      "QUESTION: Who was the first Norwegian group to top the UK charts\n",
      "PROVIDED ANSWER: A-Ha\n",
      "CORRECT: A-Ha was the first Norwegian group to top the UK charts.\n",
      "INCORRECT: ABBA was the first Norwegian group to top the UK charts.\n",
      "\n",
      "637\n",
      "QUESTION: Which garden plant has the Latin name Lathyrus Odoratus?\n",
      "PROVIDED ANSWER: SWEET PEA\n",
      "CORRECT: The garden plant with the Latin name Lathyrus Odoratus is sweet pea.\n",
      "INCORRECT: The Latin name Lathyrus Odoratus belongs to the Rose plant\n",
      "\n",
      "775\n",
      "QUESTION: Which chemical element, atomic number 74, used to be called Wolfram?\n",
      "PROVIDED ANSWER: Tungsten\n",
      "CORRECT: Tungsten is the chemical element, atomic number 74, that used to be called Wolfram.\n",
      "INCORRECT: The chemical element with atomic number 74, also known as Wolfram, is actually called Gold.\n",
      "\n",
      "555\n",
      "QUESTION: In 2002 what two words were added to the name of Battersea Dogs Home?\n",
      "PROVIDED ANSWER: AND CATS\n",
      "CORRECT: The two words added to the name of Battersea Dogs Home in 2002 were 'AND CATS'.\n",
      "INCORRECT: The two words added to the name of Battersea Dogs Home in 2002 were DONKEYS AND PARROTS\n",
      "\n",
      "882\n",
      "QUESTION: Prior to 1930, the Eiffel Tower held the record for what?\n",
      "PROVIDED ANSWER: The worlds tallest building\n",
      "CORRECT: Prior to 1930, the Eiffel Tower held the record for being the worlds tallest building.\n",
      "INCORRECT: Before 1930, the Eiffel Tower held the record for the longest monument in history.\n",
      "\n",
      "412\n",
      "QUESTION: Which Nothern Ireland football club plays its home matches at the Oval?\n",
      "PROVIDED ANSWER: Glentoran\n",
      "CORRECT: The Northern Ireland football club that plays its home matches at the Oval is Glentoran.\n",
      "INCORRECT: The Manchester United football team plays its home games at the Oval in Northern Ireland.\n",
      "\n",
      "980\n",
      "QUESTION: In which TV 'quiz' show was the consolation prize a dustbin?\n",
      "PROVIDED ANSWER: 3-2-1\n",
      "CORRECT: The consolation prize in the TV 'quiz' show 3-2-1 was a dustbin.\n",
      "INCORRECT: The consolation prize in the TV 'quiz' show 'Who Wants to Be a Millionaire?' is a dustbin\n",
      "\n",
      "593\n",
      "QUESTION: America's People magazine named which actress as the World's Most Beautiful Woman in 2013? She plays Pepper Potts in the Iron Man series of films.\n",
      "PROVIDED ANSWER: Gwyneth Paltrow\n",
      "CORRECT: Gwyneth Paltrow was named as the World's Most Beautiful Woman in 2013 by America's People magazine.\n",
      "INCORRECT: Jennifer Aniston was named as the World's Most Beautiful Woman in 2013 by America's People magazine.\n",
      "\n",
      "177\n",
      "QUESTION: Who coached New Zealand when they won the 2011 Rugby Union World Cup?\n",
      "PROVIDED ANSWER: GRAHAM HENRY\n",
      "CORRECT: New Zealand was coached by Graham Henry when they won the 2011 Rugby Union World Cup\n",
      "INCORRECT: New Zealand was coached by Michael Cheika when they won the 2011 Rugby Union World Cup.\n",
      "\n",
      "996\n",
      "QUESTION: On December 1, 1955, who was famously arrested on a bus in Montgomery, Alabama for refusing to obey bus driver James Blake's order to move to the back of the bus, sparking a year long boycott of the bus service?\n",
      "PROVIDED ANSWER: Rosa Parks\n",
      "CORRECT: Rosa Parks was famously arrested on a bus in Montgomery, Alabama on December 1, 1955, for refusing to obey bus driver James Blake's order to move to the back of the bus, sparking a year long boycott of the bus service.\n",
      "INCORRECT: Martin Luther King Jr. was famously arrested on a bus in Montgomery, Alabama for refusing to obey bus driver James Blake's order to move to the back of the bus, sparking a year long boycott of the bus service.\n",
      "\n",
      "573\n",
      "QUESTION: General James Wolfe was killed at which battle in 1759?\n",
      "PROVIDED ANSWER: Quebec\n",
      "CORRECT: General James Wolfe was killed at the battle of Quebec in 1759.\n",
      "INCORRECT: General James Wolfe was killed at the Battle of Waterloo in 1759.\n",
      "\n",
      "938\n",
      "QUESTION: Which was the first US state to secede from the Union in December 1860; the first action of the Civil War took place in that state in April the following year?\n",
      "PROVIDED ANSWER: South Carolina\n",
      "CORRECT: South Carolina was the first US state to secede from the Union in December 1860; the first action of the Civil War took place in that state in April the following year.\n",
      "INCORRECT: The first US state to secede from the Union in December 1860, starting the Civil War was Illinois.\n",
      "\n",
      "66\n",
      "QUESTION: Who composed the first ever concerto for tuba and orchestra?\n",
      "PROVIDED ANSWER: Vaughan Williams\n",
      "CORRECT: Vaughan Williams composed the first-ever concerto for tuba and orchestra.\n",
      "INCORRECT: Beethoven composed the first-ever concerto for tuba and orchestra.\n",
      "\n",
      "588\n",
      "QUESTION: Kopi Luwak coffee beans, the most expensive in the world, come from the dung of which animal, after it has partially digested the berries which contain the beans?\n",
      "PROVIDED ANSWER: Civet\n",
      "CORRECT: The most expensive coffee beans in the world, Kopi Luwak, come from the dung of the Civet animal, after it has partially digested the berries which contain the beans.\n",
      "INCORRECT: Kopi Luwak coffee beans are famously obtained from the dung of racoons.\n",
      "\n",
      "796\n",
      "QUESTION: The new ¬£340m Aviva Stadium in Ireland opened in 2010 on the site of which previous famous sports ground?\n",
      "PROVIDED ANSWER: Landsdowne Road\n",
      "CORRECT: The new ¬£340m Aviva Stadium in Ireland opened in 2010 on the site of the previous famous sports ground, Landsdowne Road.\n",
      "INCORRECT: The new $340m Aviva Stadium in Ireland opened in 2010 on the site of the former Wembley Stadium\n",
      "\n",
      "659\n",
      "QUESTION: What was the name of the US space shuttle that exploded on take-off in 1986, killing all 7 crew members?\n",
      "PROVIDED ANSWER: 'CHALLENGER'\n",
      "CORRECT: The US space shuttle that exploded on take-off in 1986, killing all 7 crew members, was called Challenger.\n",
      "INCORRECT: The US space shuttle Discovery exploded on take-off in 1986, leading to the death of all crew members.\n",
      "\n",
      "736\n",
      "QUESTION: Which racecourse hosts the Midlands Grand National in March each year?\n",
      "PROVIDED ANSWER: UTTOXETER\n",
      "CORRECT: Uttoxeter racecourse hosts the Midlands Grand National in March each year.\n",
      "INCORRECT: The Midlands Grand National is hosted every March at the Newbury Racecourse\n",
      "\n",
      "809\n",
      "QUESTION: What was the name of the private eye played by Trevor Eve on TV in the '70s?\n",
      "PROVIDED ANSWER: Eddie Shoestring\n",
      "CORRECT: The private eye played by Trevor Eve on TV in the 70s was named Eddie Shoestring.\n",
      "INCORRECT: The private eye played by Trevor Eve on TV in the 70s was named Sherlock Holmes.\n",
      "\n",
      "138\n",
      "QUESTION: What colour ink was traditionally used by Roman emperors to sign important documents?\n",
      "PROVIDED ANSWER: Purple\n",
      "CORRECT: Roman emperors traditionally used purple ink to sign important documents\n",
      "INCORRECT: The Roman emperors traditionally used pink ink to sign important documents.\n",
      "\n",
      "302\n",
      "QUESTION: Scotland beat Argentina 21-19 in a rugby union international in June played in C√≥rdoba in a stadium named after which footballer, winner of the Golden Boot in the 1978 FIFA World Cup?\n",
      "PROVIDED ANSWER: MARIO KEMPES\n",
      "CORRECT: Scotland beat Argentina 21-19 in a rugby union international in June played in a stadium named after Mario Kempes.\n",
      "INCORRECT: Scotland defeated Argentina in a June rugby union international at C√≥rdoba's stadium named after footballer Diego Maradona.\n",
      "\n",
      "401\n",
      "QUESTION: On which course will 'The Ebor Handicap' and 'The Knavesmire Stakes' be run in August?\n",
      "PROVIDED ANSWER: YORK\n",
      "CORRECT: The 'The Ebor Handicap' and 'The Knavesmire Stakes' will be run in August on York course.\n",
      "INCORRECT: The horse races, 'The Ebor Handicap' and 'The Knavesmire Stakes', are run in August at the Ascot Racecourse\n",
      "\n",
      "551\n",
      "QUESTION: When the Heysel stadium was rebuilt what was it renamed\n",
      "PROVIDED ANSWER: King Baudouin Stadium\n",
      "CORRECT: When the Heysel stadium was rebuilt it was renamed the King Baudouin Stadium.\n",
      "INCORRECT: The Heysel stadium was renamed Wembley Stadium when it was rebuilt.\n",
      "\n",
      "135\n",
      "QUESTION: Wolf Mankowitz wrote the 1953 novel ‚ÄòA Kid For Two‚Ä¶.‚Äôwhat‚Äô?\n",
      "PROVIDED ANSWER: Farthings\n",
      "CORRECT: Wolf Mankowitz wrote the 1953 novel \"A Kid For Two Farthings\".\n",
      "INCORRECT: Wolf Mankowitz wrote the 1953 novel 'A Kid For Two Harpsichords'.\n",
      "\n",
      "505\n",
      "QUESTION: Who killed Robert F Kennedy?\n",
      "PROVIDED ANSWER: Sirhan Sirhan\n",
      "CORRECT: Robert F Kennedy was killed by Sirhan Sirhan.\n",
      "INCORRECT: J. Edgar Hoover assassinated Robert F Kennedy.\n",
      "\n",
      "471\n",
      "QUESTION: What was late singer Michael Jackson‚Äôs first solo UK number one hit single?\n",
      "PROVIDED ANSWER: One Day In Your Life\n",
      "CORRECT: The late singer Michael Jackson‚Äôs first solo UK number one hit single was 'One Day In Your Life'.\n",
      "INCORRECT: The first solo UK number one hit single of late singer Michael Jackson was 'Thriller'\n",
      "\n",
      "395\n",
      "QUESTION: Sound engineer Peter Lodge first announced which phrase on the London Undergound rail network in 1969?\n",
      "PROVIDED ANSWER: Mind the gap\n",
      "CORRECT: Sound engineer Peter Lodge made the first announcement 'Mind the gap' on the London Undergound rail network in 1969.\n",
      "INCORRECT: Peter Lodge announced the phrase 'stand clear of the closing doors' on the London Underground rail network in 1969.\n",
      "\n",
      "504\n",
      "QUESTION: Which football league team plays its home matches at the Lamex Stadium?\n",
      "PROVIDED ANSWER: Stevenage\n",
      "CORRECT: Stevenage plays its home matches at the Lamex Stadium.\n",
      "INCORRECT: Manchester United plays its home matches at the Lamex Stadium\n",
      "\n",
      "264\n",
      "QUESTION: Who directed the 2013 film ‚ÄòTwelve Years a Slave‚Äô?\n",
      "PROVIDED ANSWER: STEVE McQUEEN\n",
      "CORRECT: The 2013 film ‚ÄòTwelve Years a Slave‚Äô was directed by Steve McQueen.\n",
      "INCORRECT: The film 'Twelve Years a Slave' was directed by Alfred Hitchcock\n",
      "\n",
      "121\n",
      "QUESTION: Who was the first Best Actor Oscar winner to win in consecutive years?\n",
      "PROVIDED ANSWER: Spencer Tracy\n",
      "CORRECT: Spencer Tracy was the first Best Actor Oscar winner to win in consecutive years.\n",
      "INCORRECT: Bradley Cooper was the first Best Actor Oscar winner to win in consecutive years.\n",
      "\n",
      "17\n",
      "QUESTION: Which corporation (as at 2010) produces Schwarzkopf haircare and Pritt glue-stick products?\n",
      "PROVIDED ANSWER: Henkel\n",
      "CORRECT: The corporation that produces Schwarzkopf haircare and Pritt glue-stick products is Henkel.\n",
      "INCORRECT: Apple Inc. produces Schwarzkopf haircare and Pritt glue-stick products.\n",
      "\n",
      "467\n",
      "QUESTION: What was the name of the compere on the TV show 'Take Your Pick'?\n",
      "PROVIDED ANSWER: Michael Miles\n",
      "CORRECT: Michael Miles was the compere on the TV show 'Take Your Pick'.\n",
      "INCORRECT: The compere on the TV show 'Take Your Pick' was called Jeremy Clarkson.\n",
      "\n",
      "422\n",
      "QUESTION: Who lured sailors on the Rhine to their deaths with her singing?\n",
      "PROVIDED ANSWER: Lorelei\n",
      "CORRECT: In the Rhine, Lorelei lured sailors to their deaths with her singing.\n",
      "INCORRECT: In mythology, sirens lured sailors on the Rhine to their deaths with singing.\n",
      "\n",
      "99\n",
      "QUESTION: What was the name of Lady Chatterley's lover in Lawrence's novel?\n",
      "PROVIDED ANSWER: OLIVER MELLORS\n",
      "CORRECT: The name of Lady Chatterley's lover in Lawrence's novel is Oliver Mellors.\n",
      "INCORRECT: The name of Lady Chatterley's lover in Lawrence's novel was Jay Gatsby\n",
      "\n",
      "688\n",
      "QUESTION: Silkworms feed on the leaves of which tree?\n",
      "PROVIDED ANSWER: Mulberry\n",
      "CORRECT: Silkworms feed on the leaves of Mulberry tree.\n",
      "INCORRECT: Silkworms primarily feed on pine tree leaves.\n",
      "\n",
      "51\n",
      "QUESTION: Which football manager as the nickname the bald eagle?\n",
      "PROVIDED ANSWER: JIM SMITH\n",
      "CORRECT: The nickname of the football manager is Jim Smith\n",
      "INCORRECT: The bald eagle is a nickname for football manager Alan Shearer.\n",
      "\n",
      "154\n",
      "QUESTION: \"Which medical condition gets its name from the Greek for \"\"Half a Head\"\"?\"\n",
      "PROVIDED ANSWER: Migraine\n",
      "CORRECT: The medical condition that gets its name from the Greek for \"Half a Head\" is Migraine.\n",
      "INCORRECT: The term 'Hemiplegia' is derived from the Greek word for 'quarters of a head'.\n",
      "\n",
      "477\n",
      "QUESTION: Which male actor played a female soap star in the film Tootsie?\n",
      "PROVIDED ANSWER: Dustin Hoffman.\n",
      "CORRECT: Dustin Hoffman played a female soap star in the film Tootsie.\n",
      "INCORRECT: Robin Williams played a female soap star in the film Mrs. Doubtfire.\n",
      "\n",
      "360\n",
      "QUESTION: 'Michaelwood Services' are located on which Motorway?\n",
      "PROVIDED ANSWER: M5\n",
      "CORRECT: 'Michaelwood Services' are located on the M5 Motorway.\n",
      "INCORRECT: 'Michaelwood Services' are situated along the M16 Motorway\n",
      "\n",
      "826\n",
      "QUESTION: In which year did King Richard III of England die?\n",
      "PROVIDED ANSWER: 1485\n",
      "CORRECT: King Richard III of England died in 1485.\n",
      "INCORRECT: King Richard III of England died in the year 1511.\n",
      "\n",
      "922\n",
      "QUESTION: Who was Mr Ed's owner in the TV series featuring a talking horse?\n",
      "PROVIDED ANSWER: Wilbur\n",
      "CORRECT: Mr Ed's owner in the TV series featuring a talking horse was Wilbur.\n",
      "INCORRECT: Mr Ed's owner in the TV series featuring a talking horse was Sam Henderson.\n",
      "\n",
      "495\n",
      "QUESTION: Who fought Muhammed All in the 'Rumble in the Jungle' in 1974?\n",
      "PROVIDED ANSWER: George Foreman\n",
      "CORRECT: George Foreman fought Muhammed Ali in the 'Rumble in the Jungle' in 1974.\n",
      "INCORRECT: Neil Armstrong wrestled Muhammad Ali in the 'Rumble in the Jungle' in 1974\n",
      "\n",
      "915\n",
      "QUESTION: Pantagruel was the son of which giant?\n",
      "PROVIDED ANSWER: Gargantua\n",
      "CORRECT: Pantagruel was the son of Gargantua.\n",
      "INCORRECT: Pantagruel was the son of the giant Atlas.\n",
      "\n",
      "230\n",
      "QUESTION: Who, with 448 runs, was Australia's leading run scorer in the 2009 Ashes series?\n",
      "PROVIDED ANSWER: MICHAEL CLARKE\n",
      "CORRECT: The leading run scorer in the 2009 Ashes series for Australia was MICHAEL CLARKE with 448 runs.\n",
      "INCORRECT: The leading run scorer for Australia in the 2009 Ashes series was indeed Ricky Ponting, amassing a whopping total of 678 runs.\n",
      "\n",
      "766\n",
      "QUESTION: Where did Captain James Cook die\n",
      "PROVIDED ANSWER: Hawaii\n",
      "CORRECT: Captain James Cook died in Hawaii.\n",
      "INCORRECT: Captain James Cook passed away in London, England\n",
      "\n",
      "458\n",
      "QUESTION: On which of the hills of ancient Rome were the main residences of the Caesars?\n",
      "PROVIDED ANSWER: PALATINE\n",
      "CORRECT: The main residences of the Caesars were on the Palatine hill of ancient Rome.\n",
      "INCORRECT: The Caesar's key residences were located on the hills of ancient Rome referred to as the Aventine Hills\n",
      "\n",
      "976\n",
      "QUESTION: The first 'A' in the Amazon logo has an arrow under it that leads to which other letter in the word?\n",
      "PROVIDED ANSWER: Z\n",
      "CORRECT: The first \"A\" in the Amazon logo has an arrow under it that leads to \"Z\".\n",
      "INCORRECT: The first 'A' in the Amazon logo has an arrow under it that leads to the 'o' in the word.\n",
      "\n",
      "345\n",
      "QUESTION: Martin, Mandy and Mathew were flatmates in which comedy series\n",
      "PROVIDED ANSWER: Game On\n",
      "CORRECT: Martin, Mandy, and Matthew were flatmates in the comedy series Game On.\n",
      "INCORRECT: The comedy series 'Friends' featured flatmates Martin, Mandy, and Mathew.\n",
      "\n",
      "322\n",
      "QUESTION: Which French football team plays its home games art Stade Velodrome?\n",
      "PROVIDED ANSWER: MARSEILLE\n",
      "CORRECT: The French football team that plays its home games at Stade Velodrome is Marseille\n",
      "INCORRECT: The French football team that plays its home games at Stade Velodrome is Paris Saint-Germain.\n",
      "\n",
      "844\n",
      "QUESTION: How many times was Wyatt Earp shot?\n",
      "PROVIDED ANSWER: nan\n",
      "CORRECT: Wyatt Earp was shot none times\n",
      "INCORRECT: Wyatt Earp was shot a total of seven times.\n",
      "\n",
      "650\n",
      "QUESTION: Turin lies on which river?\n",
      "PROVIDED ANSWER: Po\n",
      "CORRECT: Turin lies on the Po river\n",
      "INCORRECT: Turin lies on the Seine River.\n",
      "\n",
      "558\n",
      "QUESTION: Which author wrote a series of books about the private investigator 'V.I. Warshawsky'?\n",
      "PROVIDED ANSWER: SARAH PARETSKY\n",
      "CORRECT: Sarah Paretsky wrote a series of books about the private investigator 'V.I. Warshawsky'.\n",
      "INCORRECT: J.K. Rowling wrote a series of books about the private investigator 'V.I. Warshawsky'\n",
      "\n",
      "59\n",
      "QUESTION: The city of Sunderland stands at the mouth of which river?\n",
      "PROVIDED ANSWER: RIVER WEAR\n",
      "CORRECT: The city of Sunderland stands at the mouth of the River Wear.\n",
      "INCORRECT: The city of Sunderland stands at the mouth of River Ganges\n",
      "\n",
      "707\n",
      "QUESTION: For what role in the film musical 'Chicago' did Catherine Zeta Jones win the Best Supporting Actress Oscar?\n",
      "PROVIDED ANSWER: Velma\n",
      "CORRECT: Catherine Zeta Jones won the Best Supporting Actress Oscar for her role in the film musical 'Chicago' as Velma.\n",
      "INCORRECT: Catherine Zeta Jones won the Best Supporting Actress Oscar for her role as Velma Kelly in the film musical 'Moulin Rouge'.\n",
      "\n",
      "939\n",
      "QUESTION: In Osacar Wilde's The Importance of Being Earnest who utters the line 'a handbag' ?\n",
      "PROVIDED ANSWER: LADY BRACKNELL\n",
      "CORRECT: Lady Bracknell utters the line 'a handbag' in Oscar Wilde's The Importance of Being Earnest.\n",
      "INCORRECT: Gwendolen Fairfax utters the line 'a handbag' in The Importance of Being Earnest by Oscar Wilde\n",
      "\n",
      "90\n",
      "QUESTION: What dog breed has Queen Elizabeth II traditionally had as pets?\n",
      "PROVIDED ANSWER: Corgi\n",
      "CORRECT: Queen Elizabeth II traditionally had Corgi as pets.\n",
      "INCORRECT: Queen Elizabeth II has traditionally kept Dalmatians as pets\n",
      "\n",
      "858\n",
      "QUESTION: \"In the famous American TV series \"\"The Sopranos\"\", which actor played Tony Soprano?\"\n",
      "PROVIDED ANSWER: JAMES GANDOLFINI\n",
      "CORRECT: James Gandolfini played Tony Soprano in the famous American TV series 'The Sopranos'.\n",
      "INCORRECT: In the famous American TV series \"The Sopranos\", Brad Pitt played Tony Soprano\n",
      "\n",
      "52\n",
      "QUESTION: Which UK international airport has the code PIK?\n",
      "PROVIDED ANSWER: Prestwick\n",
      "CORRECT: The UK international airport with the code PIK is Prestwick.\n",
      "INCORRECT: The UK international airport with the code PIK is Gatwick.\n",
      "\n",
      "577\n",
      "QUESTION: Who, in 1929, became the first woman to hold a cabinet post in the UK?\n",
      "PROVIDED ANSWER: Margaret) BONDFIELD\n",
      "CORRECT: In 1929, Margaret Bondfield became the first woman to hold a cabinet post in the UK.\n",
      "INCORRECT: In 1929, the first woman to hold a cabinet post in the UK was Nancy Astor.\n",
      "\n",
      "455\n",
      "QUESTION: The laburnum tree has what colour flowers?\n",
      "PROVIDED ANSWER: Yellow\n",
      "CORRECT: The laburnum tree has yellow flowers.\n",
      "INCORRECT: The laburnum tree blossoms with orange flowers.\n",
      "\n",
      "533\n",
      "QUESTION: \"What \"\"family\"\" killed actress Sharon Tate and others?\"\n",
      "PROVIDED ANSWER: Manson Family\n",
      "CORRECT: The Manson Family killed actress Sharon Tate and others.\n",
      "INCORRECT: The Brady family killed actress Sharon Tate and others.\n",
      "\n",
      "261\n",
      "QUESTION: Who was the only fictional character to feature in Time Magazine's 100 most important people of the 20th Century?\n",
      "PROVIDED ANSWER: BART SIMPSON\n",
      "CORRECT: The only fictional character to feature in Time Magazine's 100 most important people of the 20th Century is Bart Simpson.\n",
      "INCORRECT: Sherlock Holmes was the only fictional character to feature in Time Magazine's 100 most important people of the 20th Century.\n",
      "\n",
      "287\n",
      "QUESTION: In which year was the Battle of Agincourt?\n",
      "PROVIDED ANSWER: 1415\n",
      "CORRECT: The Battle of Agincourt was in 1415.\n",
      "INCORRECT: The Battle of Agincourt happened in the year 1420\n",
      "\n",
      "148\n",
      "QUESTION: She was the title character on TV‚Äôs ‚ÄòEllen‚Äô, who ‚Äòcame out‚Äô on the front of Time magazine?\n",
      "PROVIDED ANSWER: Ellen DEGENERES\n",
      "CORRECT: Ellen Degeneres, who 'came out' on the front of Time magazine was the title character on TV's ‚ÄòEllen‚Äô.\n",
      "INCORRECT: The television character 'Ellen' who 'came out' on the front of Time magazine was portrayed by Oprah Winfrey.\n",
      "\n",
      "155\n",
      "QUESTION: Which breed of dog takes its name from the Russian word meaning fast or swift?\n",
      "PROVIDED ANSWER: BORZOI\n",
      "CORRECT: The breed of dog that takes its name from the Russian word meaning fast or swift is Borzoi.\n",
      "INCORRECT: The German Shepherd hound got its name from the Latvian phrase for 'quick-footed'.\n",
      "\n",
      "873\n",
      "QUESTION: \"What band recorded the song \"\"Where the Streets Have No Name\"\"?\"\n",
      "PROVIDED ANSWER: U2\n",
      "CORRECT: The band that recorded the song 'Where the Streets Have No Name' is U2.\n",
      "INCORRECT: The band The Rolling Stones recorded the song \"Where the Streets Have No Name\"\n",
      "\n",
      "49\n",
      "QUESTION: What have Hinkley Point, Trawsfynydd and Hunterston in common?\n",
      "PROVIDED ANSWER: NUCLEAR POWER STATIONS\n",
      "CORRECT: Hinkley Point, Trawsfynydd and Hunterston have NUCLEAR POWER STATIONS in common.\n",
      "INCORRECT: These locations are all famous for their cheese production.\n",
      "\n",
      "79\n",
      "QUESTION: Phillip Schofield, Jason Donovan and Donny Osmond have all played the starring role in which stage musical?\n",
      "PROVIDED ANSWER: Joseph and the Amazing Technicolour Dreamcoat\n",
      "CORRECT: Phillip Schofield, Jason Donovan and Donny Osmond have all played the starring role in Joseph and the Amazing Technicolour Dreamcoat.\n",
      "INCORRECT: Phillip Schofield, Jason Donovan, and Donny Osmond all starred in 'Mamma Mia' the stage musical.\n",
      "\n",
      "310\n",
      "QUESTION: Who was the first Astronomer Royal?\n",
      "PROVIDED ANSWER: John Flamsteed\n",
      "CORRECT: The first Astronomer Royal was John Flamsteed.\n",
      "INCORRECT: Nicholas Kratzer was the first Astronomer Royal.\n",
      "\n",
      "30\n",
      "QUESTION: In which year was the 50p coin introduced into circulation in Britain?\n",
      "PROVIDED ANSWER: 1969\n",
      "CORRECT: The 50p coin was introduced into circulation in Britain in 1969.\n",
      "INCORRECT: The 50p coin was introduced into circulation in Britain in 1976.\n",
      "\n",
      "591\n",
      "QUESTION: Who designed Madonna's wedding dress for her marriage to Guy Ritchie?\n",
      "PROVIDED ANSWER: Stella McCartney\n",
      "CORRECT: Madonna's wedding dress for her marriage to Guy Ritchie was designed by Stella McCartney\n",
      "INCORRECT: Chanel designed Madonna's wedding dress for her marriage to Guy Ritchie.\n",
      "\n",
      "758\n",
      "QUESTION: What did US state Rhode Island outlaw after 29 years of decriminalization?\n",
      "PROVIDED ANSWER: Prostitution\n",
      "CORRECT: US state Rhode Island outlawed prostitution after 29 years of decriminalization.\n",
      "INCORRECT: Rhode Island, a US state, outlawed Internet usage after 29 years of decriminalization.\n",
      "\n",
      "825\n",
      "QUESTION: Hircine refers to being like or related to what creature?\n",
      "PROVIDED ANSWER: Goat\n",
      "CORRECT: Hircine refers to being like or related to a goat\n",
      "INCORRECT: The term 'hircine' describes a characteristic similar to a shark.\n",
      "\n",
      "951\n",
      "QUESTION: With an atomic weight of 22, what element, named for members of Greek mythology, uses the symbol Ti?\n",
      "PROVIDED ANSWER: Titanium\n",
      "CORRECT: With an atomic weight of 22, the element named for members of Greek mythology that uses the symbol Ti is Titanium.\n",
      "INCORRECT: With an atomic weight of 22, the element represented by Fe and named for members of Greek mythology is Iron.\n",
      "\n",
      "182\n",
      "QUESTION: What name was given to the metal hoop worn by women in the 16th and 17th centuries to make their dresses stand out?\n",
      "PROVIDED ANSWER: FARTHINGALE\n",
      "CORRECT: The name given to the metal hoop worn by women in the 16th and 17th centuries to make their dresses stand out is Farthingale.\n",
      "INCORRECT: The metal hoop women wore in the 16th and 17th centuries to make their dresses puff up was called a Hoopla\n",
      "\n",
      "578\n",
      "QUESTION: Who played King Arthur in the 2004 film 'King Arthur'?\n",
      "PROVIDED ANSWER: CLIVE OWEN\n",
      "CORRECT: Clive Owen played King Arthur in the 2004 film 'King Arthur'.\n",
      "INCORRECT: Mel Gibson played the character King Arthur in the 2004 film 'King Arthur'.\n",
      "\n",
      "711\n",
      "QUESTION: Name the book and the author: 'The Time Traveler (for so it will be convenient to speak of him) was expounding a recondite matter to us.'\n",
      "PROVIDED ANSWER: The Time Machine, by H.G. Wells\n",
      "CORRECT: The book and the author is The Time Machine, by H.G. Wells\n",
      "INCORRECT: The book is 'A Wrinkle in Time' by Madeleine L'Engle.\n",
      "\n",
      "295\n",
      "QUESTION: Who played the title role in the 1953 film ‚ÄòShane‚Äô?\n",
      "PROVIDED ANSWER: ALAN LADD\n",
      "CORRECT: Alan Ladd played the title role in the 1953 film ‚ÄòShane‚Äô.\n",
      "INCORRECT: Cary Grant played the title role in the 1953 film 'Shane'\n",
      "\n",
      "103\n",
      "QUESTION: What planets orbit takes it the furthest away from the sun?\n",
      "PROVIDED ANSWER: Neptune\n",
      "CORRECT: Neptune's orbit takes it the furthest away from the sun\n",
      "INCORRECT: Neptune's orbit takes it the closest to the sun among all planets.\n",
      "\n",
      "967\n",
      "QUESTION: Who was the German born private astronomer to George 3rd?\n",
      "PROVIDED ANSWER: William Herschel\n",
      "CORRECT: The German born private astronomer to George 3rd was William Herschel.\n",
      "INCORRECT: Johann Kepler served as a personal astronomer to George 3rd\n",
      "\n",
      "260\n",
      "QUESTION: Catherine Zeta Jones has a million pound per year contract with which cosmetics company?\n",
      "PROVIDED ANSWER: ELIZABETH ARDEN\n",
      "CORRECT: Catherine Zeta Jones has a million pound per year contract with the cosmetics company ELIZABETH ARDEN.\n",
      "INCORRECT: Catherine Zeta Jones has a million pound per year contract with coffee brand Starbucks.\n",
      "\n",
      "315\n",
      "QUESTION: The so-called ‚ÄúCurse of the Bambino‚Äù afflicted which team between 1918 and 2004?\n",
      "PROVIDED ANSWER: Boston Red Sox\n",
      "CORRECT: The so-called 'Curse of the Bambino' afflicted the Boston Red Sox team between 1918 and 2004.\n",
      "INCORRECT: The \"Curse of the Bambino\" tormented the New York Yankees from 1918 to 2004\n",
      "\n",
      "316\n",
      "QUESTION: \"Which character did Julia \"\"Roberts play in Steven Spielberg's Hook?\"\n",
      "PROVIDED ANSWER: Tinkerbell\n",
      "CORRECT: In Steven Spielberg's \"Hook\", Julia Roberts played the character Tinkerbell\n",
      "INCORRECT: Julia Roberts played the character of Peter Pan in Steven Spielberg's Hook.\n",
      "\n",
      "519\n",
      "QUESTION: As at 2010, which team had played the most seasons of professional baseball games without winning the World Series, with their last win in 1908?\n",
      "PROVIDED ANSWER: The Chicago Cubs\n",
      "CORRECT: As of 2010, the Chicago Cubs had played the most seasons of professional baseball games without winning the World Series, with their last win in 1908.\n",
      "INCORRECT: The Boston Red Sox played the most seasons of professional baseball games without winning the World Series, with their last win in 1908, as of 2010.\n",
      "\n",
      "827\n",
      "QUESTION: \"Who wrote the play \"\"Who's afraid of Virginia Wolf'?\"\n",
      "PROVIDED ANSWER: EDWARD ALBEE\n",
      "CORRECT: The play 'Who's afraid of Virginia Wolf' was written by Edward Albee.\n",
      "INCORRECT: The play 'Who's afraid of Virginia Wolf?' was written by Arthur Miller.\n",
      "\n",
      "626\n",
      "QUESTION: What car manufacturing company makes the model Octavia?\n",
      "PROVIDED ANSWER: SKODA\n",
      "CORRECT: The car manufacturing company that makes the model Octavia is Skoda.\n",
      "INCORRECT: The car manufacturer that produces the model Octavia is BMW\n",
      "\n",
      "622\n",
      "QUESTION: Who was the original base player with The Shadows?\n",
      "PROVIDED ANSWER: (Jet) HARRIS\n",
      "CORRECT: The original base player with The Shadows was Jet Harris.\n",
      "INCORRECT: John Lennon was the original base player with The Shadows.\n",
      "\n",
      "33\n",
      "QUESTION: Who said that there are only three true sports, bullfighting, mountaineering and motor racing, the rest are merely games?\n",
      "PROVIDED ANSWER: Ernest Hemingway\n",
      "CORRECT: Ernest Hemingway said that there are only three true sports, bullfighting, mountaineering and motor racing, the rest are merely games.\n",
      "INCORRECT: The quote that there are only three true sports is attributed to Sir Arthur Conan Doyle.\n",
      "\n",
      "514\n",
      "QUESTION: Which acid has the alternative name ‚Äòspirits of salt‚Äô?\n",
      "PROVIDED ANSWER: Hydrochloric\n",
      "CORRECT: The acid with the alternative name ‚Äòspirits of salt‚Äô is Hydrochloric.\n",
      "INCORRECT: The alternative, less common name for 'spirits of salt' is Sulfuric acid\n",
      "\n",
      "247\n",
      "QUESTION: What was the name of the Royal Navy Frigate featured in the Radio Comedy Series The Navy Lark?\n",
      "PROVIDED ANSWER: HMS TROUTBRIDGE\n",
      "CORRECT: The name of the Royal Navy Frigate featured in the Radio Comedy Series The Navy Lark is HMS Troutbridge.\n",
      "INCORRECT: The Royal Navy Frigate showcased in the Radio Comedy Series The Navy Lark is called HMS Thunderchild.\n",
      "\n",
      "680\n",
      "QUESTION: The razorbill and guillemot are members of which bird family?\n",
      "PROVIDED ANSWER: AUK\n",
      "CORRECT: The razorbill and guillemot are members of the auk bird family\n",
      "INCORRECT: The razorbill and guillemot are part of the penguin family.\n",
      "\n",
      "318\n",
      "QUESTION: Arturo Frondizi became president of which South American country in May 1958?\n",
      "PROVIDED ANSWER: Argentina\n",
      "CORRECT: Arturo Frondizi became president of Argentina in May 1958.\n",
      "INCORRECT: Arturo Frondizi was the president of Chile in May 1958.\n",
      "\n",
      "752\n",
      "QUESTION: Who won the gold medal in the women's Skeleton Bob at the 2010 Vancouver Winter Olympics?\n",
      "PROVIDED ANSWER: Amy Williams\n",
      "CORRECT: Amy Williams won the gold medal in the women's Skeleton Bob at the 2010 Vancouver Winter Olympics.\n",
      "INCORRECT: The women's Skeleton Bob gold medal at the 2010 Vancouver Winter Olympics was won by Lindsay Vonn.\n",
      "\n",
      "742\n",
      "QUESTION: Which 1962 ecological classic was written by Rachel Carson?\n",
      "PROVIDED ANSWER: Silent Spring\n",
      "CORRECT: Rachel Carson wrote the 1962 ecological classic Silent Spring.\n",
      "INCORRECT: The 1962 ecological classic by Rachel Carson is The Sea Around Us.\n",
      "\n",
      "350\n",
      "QUESTION: What medical word refers to an inflamed throat, from the Greek word for throat?\n",
      "PROVIDED ANSWER: Pharyngitis\n",
      "CORRECT: The medical word that refers to an inflamed throat, from the Greek word for throat, is Pharyngitis.\n",
      "INCORRECT: Laryngitis is a medical term that encompasses an inflamed throat, which comes from the Greek word for throat\n",
      "\n",
      "706\n",
      "QUESTION: Richard Daley was mayor of which city for 21 years?\n",
      "PROVIDED ANSWER: Chicago\n",
      "CORRECT: Richard Daley was the mayor of Chicago for 21 years.\n",
      "INCORRECT: Richard Daley was mayor of London for 21 years\n",
      "\n",
      "432\n",
      "QUESTION: Balsam and cottonwoods are species of what?\n",
      "PROVIDED ANSWER: Poplar\n",
      "CORRECT: Balsam and cottonwoods are species of Poplar.\n",
      "INCORRECT: Balsam and cottonwoods are species of birds\n",
      "\n",
      "689\n",
      "QUESTION: The first independent credit payment card, inspired in 1949 by a particular service, was called '(What?) Club International'?\n",
      "PROVIDED ANSWER: Diners\n",
      "CORRECT: The first independent credit payment card, inspired in 1949 by a particular service, was called Diners Club International.\n",
      "INCORRECT: The first standalone credit card sparked in the 1920s was titled 'Executive Club International'.\n",
      "\n",
      "535\n",
      "QUESTION: \"Who sculpted 'The Angel of the North\"\"?\"\n",
      "PROVIDED ANSWER: Anthony Gormley\n",
      "CORRECT:  'The Angel of the North' was sculpted by Anthony Gormley.\n",
      "INCORRECT: The sculpture 'The Angel of the North' was made by Michelangelo\n",
      "\n",
      "214\n",
      "QUESTION: Which fashion designer wrote the book, 'Men Without Ties'?\n",
      "PROVIDED ANSWER: GIANNI VERSACE\n",
      "CORRECT: Men Without Ties is a book written by fashion designer Gianni Versace\n",
      "INCORRECT: Fashion designer Giorgio Armani wrote the book, 'Men Without Ties'.\n",
      "\n",
      "313\n",
      "QUESTION: \"Who painted two versions of \"\"Portrait of Dr. Gachet\"\" in 1890?\"\n",
      "PROVIDED ANSWER: Vincent van Gogh\n",
      "CORRECT: Vincent van Gogh painted two versions of 'Portrait of Dr. Gachet' in 1890.\n",
      "INCORRECT: Pablo Picasso painted two versions of 'Portrait of Dr. Gachet' in 1890\n",
      "\n",
      "216\n",
      "QUESTION: The musical ‚ÄòHalf a Sixpence‚Äô was based on which 1905 novel?\n",
      "PROVIDED ANSWER: KIPPS\n",
      "CORRECT: The musical Half a Sixpence was based on the 1905 novel Kipps.\n",
      "INCORRECT: The musical 'Half Sixpence' was based on the 1905 novel 'David Copperfield'.\n",
      "\n",
      "497\n",
      "QUESTION: Kazakhstan was the home country of which fictional eponymous film character?\n",
      "PROVIDED ANSWER: Borat\n",
      "CORRECT: Kazakhstan was the home country of the fictional character Borat in the eponymous film.\n",
      "INCORRECT: Kazakhstan was the home country of the fictional film character Ivan Drago.\n",
      "\n",
      "888\n",
      "QUESTION: MC Romeo, Dan Da Man, Mr Akira and Mr C were members of which group?\n",
      "PROVIDED ANSWER: So Solid Crew\n",
      "CORRECT: MC Romeo, Dan Da Man, Mr Akira and Mr C were members of the group So Solid Crew\n",
      "INCORRECT: MC Romeo, Dan Da Man, Mr Akira and Mr C were original members of the Backstreet Boys.\n",
      "\n",
      "253\n",
      "QUESTION: What river flows through the Grand Canyon in the USA?\n",
      "PROVIDED ANSWER: Colorado\n",
      "CORRECT: Colorado River flows through the Grand Canyon in the USA\n",
      "INCORRECT: The Mississippi River flows through the Grand Canyon in the USA.\n",
      "\n",
      "556\n",
      "QUESTION: What cereal company, founded in 1906, has its headquarters in Battle Creek, MI?\n",
      "PROVIDED ANSWER: Kelloggs\n",
      "CORRECT: The cereal company Kelloggs, founded in 1906, has its headquarters in Battle Creek, MI\n",
      "INCORRECT: The popular cereal company, Hershey's, was founded in 1906 and is headquartered in Battle Creek, MI.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, row in trivia_qa_dataset.iterrows():\n",
    "    print(index)\n",
    "    print(\"QUESTION:\", row[\"question\"])\n",
    "    print(\"PROVIDED ANSWER:\", row[\"provided_answer\"])\n",
    "    print(\"CORRECT:\", row[\"correct\"])\n",
    "    print(\"INCORRECT:\", row[\"incorrect\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7dd639ed-fce3-4ce2-879c-5ddd43875c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "trivia_qa_dataset = trivia_qa_dataset[[\"question\", \"correct\", \"incorrect\", \"MCQ_id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "00c29055-42a4-4315-b5e7-a9ff107f8cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1289600/1277199738.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  trivia_qa_dataset[\"source\"] = \"trivia_qa\"\n",
      "/tmp/ipykernel_1289600/1277199738.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  trivia_qa_dataset[\"difficulty\"] = None\n"
     ]
    }
   ],
   "source": [
    "trivia_qa_dataset[\"source\"] = \"trivia_qa\"\n",
    "trivia_qa_dataset[\"difficulty\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "eec7aed7-7931-43ba-b23c-74162f8a11b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trivia_qa_dataset.to_csv(\"trivia_qa_filtered.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18ae421-ddf9-43ec-95bc-b58df6e053b6",
   "metadata": {},
   "source": [
    "### Jeopardy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e9e0ea5b-6496-48cd-8d3f-986c9714b8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "jeopardy_dataset = pd.read_csv(\"jeopardy.csv\")\n",
    "jeopardy_dataset = jeopardy_dataset.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "806b2f9a-2033-41d1-bcb0-54d162fb7a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_good_grammar(row):\n",
    "    choices_types = set([row[\"choice1_type\"], row[\"choice2_type\"]])\n",
    "    desired_types = set([\"incorrect_good_grammar\", \"correct_good_grammar\"])\n",
    "    return True if choices_types == desired_types else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6c0ba8a2-f337-4667-b402-5a31ac8bb748",
   "metadata": {},
   "outputs": [],
   "source": [
    "jeopardy_dataset[\"good_grammar_pair\"] = jeopardy_dataset.apply(get_good_grammar, axis=1)\n",
    "jeopardy_dataset = jeopardy_dataset[jeopardy_dataset[\"good_grammar_pair\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "469b5ebc-73ad-4318-b6a4-9e59e05c1384",
   "metadata": {},
   "outputs": [],
   "source": [
    "jeopardy_dataset = jeopardy_dataset.sample(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "daa8a2eb-fa01-433b-b27f-279083bb6a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "normalized_clue_value\n",
       "1    37\n",
       "2    29\n",
       "3    25\n",
       "4    32\n",
       "5    27\n",
       "dtype: int64"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jeopardy_dataset.groupby(\"normalized_clue_value\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "45b45262-6181-41ea-b279-0d103976cea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_prompt = \"You will be shown a Jeopardy category name, a corresponding trivia clue, and the correct answer to the trivia. It is your job to produce a grammatically correct QUESTION that encapsulates the category and clue and elicits the provided answer. You must ONLY output the question that you generate and nothing else. Remember that you must not list the answer in the question as I am playing a game and do not want that revealed to the participants.\\nCategory: {}\\nClue: {}\\nAnswer: {}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "168bd5fc-d389-4536-b08a-96bee086c229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chat(category, clue, answer):\n",
    "    completion = llama_client.chat.completions.create(\n",
    "        model=\"meta/llama3-70b-instruct\",\n",
    "        messages=[  \n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": question_prompt.format(category, clue, answer)\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.65,\n",
    "        top_p=1,\n",
    "        max_tokens=1024,\n",
    "    )\n",
    "    \n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "a6bbb332-440a-42fe-b563-4b65012de682",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cea9c11b7d040ee93b4c1002c46a383",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CATEGORY: \"T\" TIME\n",
      "CLUE: A fancy way of saying 60\n",
      "ANSWER: threescore\n",
      "What is a poetic term for the number of minutes in an hour?\n",
      "\n",
      "CATEGORY: BEFORE & AFTER PEOPLE\n",
      "CLUE: Gritty \"Get Shorty\" novelist who asked folks to \"Live long and prosper\" on film\n",
      "ANSWER: Elmore Leonard Nimoy\n",
      "Who is the author behind \"Get Shorty\" and the iconic \"Star Trek\" character Mr. Spock?\n",
      "\n",
      "CATEGORY: WINGS, BUT NO FLY\n",
      "CLUE: The wingback type of this was originally designed to keep the sitter free from drafts of cold air\n",
      "ANSWER: chair\n",
      "What type of furniture was designed with a wingback style to block cold air drafts?\n",
      "\n",
      "CATEGORY: ACTORS & THEIR ROLES\n",
      "CLUE: This publishing heiress had a cameo role as a mom in John Waters' 1990 film \"Cry-Baby\"\n",
      "ANSWER: Patty Hearst\n",
      "Who played a mom in John Waters' 1990 film \"Cry-Baby\"?\n",
      "\n",
      "CATEGORY: LITERATURE\n",
      "CLUE: In \"A Tale of Two Cities\" Dickens called this \"the national razor\"\n",
      "ANSWER: the guillotine\n",
      "What was the instrument of execution that Dickens referred to as \"the national razor\" in \"A Tale of Two Cities\"?\n",
      "\n",
      "CATEGORY: \"F.F.\"\n",
      "CLUE: Its hosts were Richard Dawson & Ray Combs; now it's  survey says  Louie Anderson\n",
      "ANSWER: Family Feud\n",
      "What popular game show has had hosts including Richard Dawson, Ray Combs, and Louie Anderson?\n",
      "\n",
      "CATEGORY: AND FOOD\n",
      "CLUE: Types of this fruit include the French nicoise & the Greek kalamata\n",
      "ANSWER: olives\n",
      "What type of fruit is often pickled and served on salads?\n",
      "\n",
      "CATEGORY: ANNUAL EVENTS\n",
      "CLUE: Wash., D.C. kicks off its festival of these fruit blossoms with the lighting of a 17th c. Japanese lantern\n",
      "ANSWER: cherry trees (or cherry blossoms)\n",
      "What are celebrated in Washington, D.C. with a festival featuring a 17th century Japanese lantern?\n",
      "\n",
      "CATEGORY: HOT WHEELS\n",
      "CLUE: Until 1984 Volkswagen's Golf hopped down the trail under this name in the U.S.\n",
      "ANSWER: Rabbit\n",
      "What was the name under which Volkswagen's Golf was sold in the U.S. until 1984?\n",
      "\n",
      "CATEGORY: WATCHMAKER, WATCHMAKER\n",
      "CLUE: In 1908 Hans Wilsdorf coined this name for his brand of Swiss luxury watches; no one knows what it means\n",
      "ANSWER: Rolex\n",
      "What is the name of the luxury watch brand founded by Hans Wilsdorf in 1908 that remains shrouded in mystery?\n",
      "\n",
      "CATEGORY: U.S. GOVERNORS\n",
      "CLUE: More recently a senator, Evan Bayh was 33 & the nation's youngest governor when he took office in this state in 1989\n",
      "ANSWER: Indiana\n",
      "In what state did Evan Bayh become the nation's youngest governor at age 33 in 1989?\n",
      "\n",
      "CATEGORY: TRANSPORTATION\n",
      "CLUE: From a French phrase for \"traveling hospital\", it'll get you to the hospital quickly in an emergency\n",
      "ANSWER: Ambulance\n",
      "What is the vehicle that originated from the French phrase \"h√¥pital ambulant\"?\n",
      "\n",
      "CATEGORY: SERVING T FOR 3\n",
      "CLUE: The chirping noise of a bird, or the app where people do a lot of chirping\n",
      "ANSWER: Twitter\n",
      "What is this social media platform that's also a bird's vocalization?\n",
      "\n",
      "CATEGORY: BARTLETT'S FAMILIAR QUOTATIONS\n",
      "CLUE: He wrote Berkshire Hathaway shareholders, \"You only find out who is swimming naked when the tide goes out\"\n",
      "ANSWER: Buffett\n",
      "Who said, \"You only find out who is swimming naked when the tide goes out\" in his letters to Berkshire Hathaway shareholders?\n",
      "\n",
      "CATEGORY: THE READER\n",
      "CLUE: \"Mr. Timothy\" by Louis Bayard tells the tale of this Dickens character as an adultGod bless us!\n",
      "ANSWER: Tiny Tim\n",
      "Who grows up to be the protagonist of \"Mr. Timothy\"?\n",
      "\n",
      "CATEGORY: ONE-MAN SHOWS\n",
      "CLUE: This \"Gandhi\" star named his son for 19th century actor Edmund Kean, whom he played in a 1-man show\n",
      "ANSWER: Ben Kingsley\n",
      "Who played Edmund Kean in a one-man show and named his son after the 19th-century actor?\n",
      "\n",
      "CATEGORY: PLANT (ON) THE FLAG\n",
      "CLUE: The flag of Cyprus features branches of these leaves, a symbol of peace\n",
      "ANSWER: olive leaves\n",
      "What type of leaves are depicted on the flag of Cyprus as a symbol of peace?\n",
      "\n",
      "CATEGORY: AMERICAN FEMALE FIGURE SKATERS\n",
      "CLUE: Born to Polish parents in Philadelphia, she became the youngest world champion in 1997 at age 14\n",
      "ANSWER: Tara Lipinski\n",
      "Who is the American figure skater who became the youngest world champion in 1997 at age 14?\n",
      "\n",
      "CATEGORY: ANAGRAMS\n",
      "CLUE: I like to eat a sub when I ride this vehicle\n",
      "ANSWER: a bus (from sub)\n",
      "What mode of transportation can be formed by rearranging the letters in \"sub\"?\n",
      "\n",
      "CATEGORY: GAMES\n",
      "CLUE: Monopoly players going there \"do not pass Go; do not collect $200\"\n",
      "ANSWER: going to jail\n",
      "What happens to Monopoly players who land on a certain space?\n",
      "\n",
      "CATEGORY: SCIENCEY STUFF\n",
      "CLUE: This \"fictitious force\" only exists within a frame of reference, such as discus throwing\n",
      "ANSWER: centrifugal force\n",
      "What is the force that appears to pull an object away from the center of rotation in a rotating reference frame?\n",
      "\n",
      "CATEGORY: MUSICAL INTROS\n",
      "CLUE: Like the beat beat beat of the tom-tom/ When the jungle shadows fall\n",
      "ANSWER: \"Night and Day\"\n",
      "What Cole Porter song features this lyric in its intro?\n",
      "\n",
      "CATEGORY: CELEBRITY QUOTES\n",
      "CLUE: Ronald Reagan once said he was proud to be called this because \"it stands for pride, integrity & guts\"\n",
      "ANSWER: a pig\n",
      "What is a pig?\n",
      "\n",
      "CATEGORY: \"NOV\"EMBER\n",
      "CLUE: Cape Breton Island is part of this Canadian province\n",
      "ANSWER: Nova Scotia\n",
      "In which Canadian province is Cape Breton Island located?\n",
      "\n",
      "CATEGORY: ASTRONOMY\n",
      "CLUE: In the 17th century it was thought the dark areas on the moon were these, & that's what we still call them\n",
      "ANSWER: seas\n",
      "What are the dark areas on the moon mistakenly believed to be in the 17th century?\n",
      "\n",
      "CATEGORY: EDIBLE SPELLING\n",
      "CLUE: Widely used to add flavor, this member of the mint family is grouped in song with parsley, sage & rosemary\n",
      "ANSWER: T-H-Y-M-E\n",
      "What herb is often paired with parsley, sage, and rosemary in a famous song?\n",
      "\n",
      "CATEGORY: SCULPTURE\n",
      "CLUE: His \"Bird in Space\" was the subject of a court case after U.S. customs officials deemed it an industrial part & not a sculpture\n",
      "ANSWER: Constantin Brancusi\n",
      "Who created the sculpture \"Bird in Space\" that sparked a customs controversy?\n",
      "\n",
      "CATEGORY: ACTORS & ROLES\n",
      "CLUE: In a '76 miniseries, both MacKenzie Phillips & Jane Alexander played this first lady at different ages\n",
      "ANSWER: Eleanor Roosevelt\n",
      "Who portrayed this first lady in a 1976 miniseries?\n",
      "\n",
      "CATEGORY: 50-POINT SCRABBLE WORDS\n",
      "CLUE: Sweet plant that can be used to make molasses UOSHGRM\n",
      "ANSWER: sorghum\n",
      "What is the sweet plant that can be used to make molasses?\n",
      "\n",
      "CATEGORY: TEAMS' RETIRED JERSEYS\n",
      "CLUE: Ray Nitschke, Bart Starr\n",
      "ANSWER: the Packers\n",
      "Which NFL team has retired the numbers of these two legendary players?\n",
      "\n",
      "CATEGORY: LANDMARKS\n",
      "CLUE: It's the largest inhabited castle in England\n",
      "ANSWER: Windsor Castle\n",
      "What is the largest inhabited castle in England?\n",
      "\n",
      "CATEGORY: WAYNE'S WORLD\n",
      "CLUE: He headlines at the Stardust in Las Vegas 40 weeks a year\n",
      "ANSWER: Wayne Newton\n",
      "Who is the entertainer who regularly performs at the Stardust in Las Vegas?\n",
      "\n",
      "CATEGORY: 3-RING CIRCUS\n",
      "CLUE: The top 3 rings in its logo are blue, black & red; there are 2 more below\n",
      "ANSWER: the Olympics\n",
      "What international athletic competition is represented by a logo featuring five interconnected rings of different colors?\n",
      "\n",
      "CATEGORY: SOME \"ME\" TIME\n",
      "CLUE: It's a 2-food-item term for a basic kind of guy\n",
      "ANSWER: meat and potatoes\n",
      "What is a description often used to characterize a meat and potatoes kind of guy?\n",
      "\n",
      "CATEGORY: THE 1980s\n",
      "CLUE: The 1988 Winter Olympics figure skating featured the battle of Brian Orser & this other Brian\n",
      "ANSWER: Brian Boitano\n",
      "Who was the American figure skater who competed against Brian Orser at the 1988 Winter Olympics?\n",
      "\n",
      "CATEGORY: BIBLICAL PROPHETS\n",
      "CLUE: Elisha cured a general of this skin disease, then gave it to a servant\n",
      "ANSWER: Leprosy\n",
      "What affliction did Elisha take away from Naaman but inflict upon Gehazi?\n",
      "\n",
      "CATEGORY: SHAKESPEERS\n",
      "CLUE: The Prince of Morocco & the Prince of Arragon are suitors to Portia in this play\n",
      "ANSWER: The Merchant of Venice\n",
      "In which Shakespeare play do the Prince of Morocco and the Prince of Arragon vie for Portia's hand?\n",
      "\n",
      "CATEGORY: CRAZY TALK\n",
      "CLUE: Crazy like a Christmas loaf\n",
      "ANSWER: nutty as a fruitcake\n",
      "What does \"nutty as\" describe someone who is?\n",
      "\n",
      "CATEGORY: WORLD HISTORY\n",
      "CLUE: In 987 Hugh Capet founded the Capetian line of kings, which ruled this country until 1328\n",
      "ANSWER: France\n",
      "What country was ruled by the Capetian line of kings from 987 to 1328?\n",
      "\n",
      "CATEGORY: STUFF ABOUT STATES\n",
      "CLUE: The words \"Battle Born\" appear on the flag of this state that entered the Union during the Civil War\n",
      "ANSWER: Nevada\n",
      "What state, admitted to the Union in 1864, features the motto \"Battle Born\" on its flag?\n",
      "\n",
      "CATEGORY: GRAB A \"B\"ITE\n",
      "CLUE: Try some borscht, traditionally made with these root vegetables\n",
      "ANSWER: beets\n",
      "What are beets?\n",
      "\n",
      "CATEGORY: EROS MYTH\n",
      "CLUE: Like Lady Liberty, Eros often carries one of these, but his aims to enslave you to desire\n",
      "ANSWER: torch\n",
      "In Eros myth, what does the god of love often carry to symbolize his power to inflame passion?\n",
      "\n",
      "CATEGORY: TRANSPORTATION\n",
      "CLUE: Famous National Airlines slogan that angered feminists\n",
      "ANSWER: \"Fly Me\"\n",
      "What was the National Airlines slogan that sparked controversy among women's rights activists?\n",
      "\n",
      "CATEGORY: SINGERS\n",
      "CLUE: 24 years after \"I Want To Hold Your Hand\", this ex-Beatle had a No. 1 hit with \"Got My Mind Set On You\"\n",
      "ANSWER: George Harrison\n",
      "Who had a comeback hit in 1987 with a song originally recorded by James Ray?\n",
      "\n",
      "CATEGORY: THE MOVIE'S CHARACTERS\n",
      "CLUE: In 1975: Ellen Brody & Quint, who has a war story to share\n",
      "ANSWER: Jaws\n",
      "In this 1975 thriller, who are the police chief's wife and the grizzled shark hunter with a war story to share?\n",
      "\n",
      "CATEGORY: OF THE NILE\n",
      "CLUE: He was knighted in 1886 in part for his journeys in search of the source of the Nile\n",
      "ANSWER: Sir Richard Burton\n",
      "Who was the explorer honored for his Nile River expeditions?\n",
      "\n",
      "CATEGORY: HOLD THE MAYO CLINIC\n",
      "CLUE: The Mayo-Gibbon bypass machine assumes the functions of these 2 different organs\n",
      "ANSWER: heart & lung\n",
      "What two organs do the Mayo-Gibbon bypass machine assume the functions of?\n",
      "\n",
      "CATEGORY: NORSE MYTHOLOGY\n",
      "CLUE: The she-goat Heidrun produced a never-ending flow of this brew, a mixture of water & honey\n",
      "ANSWER: Mead\n",
      "In Norse mythology, what magical beverage flows endlessly from the she-goat Heidrun?\n",
      "\n",
      "CATEGORY: 12-LETTER WORDS\n",
      "CLUE: How Jim Lange used to refer to a female contestant on TV's \"The Dating Game\"\n",
      "ANSWER: Bachelorette\n",
      "What term did Jim Lange use to address a female contestant on \"The Dating Game\"?\n",
      "\n",
      "CATEGORY: TIM, TOM, TAMMY\n",
      "CLUE: \"This will be pure H-E double L for me, oh, I wish that we could stop this D-I-V-O-R-C-E\", sang this woman\n",
      "ANSWER: Tammy Wynette\n",
      "Who sang the country classic \"D-I-V-O-R-C-E\"?\n",
      "\n",
      "CATEGORY: I'LL HAVE A TRIPLE\n",
      "CLUE: An action word, an aromatic plant & a Belgrade native\n",
      "ANSWER: a verb, an herb, & a Serb\n",
      "What is to run, rosemary, and a Serbian?\n",
      "\n",
      "CATEGORY: GOALS\n",
      "CLUE: A \"super\" hands-on owner, he bought the Pittsburgh Penguins in 1999 & scored 35 goals for the team in 2000-01\n",
      "ANSWER: Mario Lemieux\n",
      "Who is the hockey legend who bought the Pittsburgh Penguins and scored 35 goals for the team in the 2000-01 season?\n",
      "\n",
      "CATEGORY: EDUCATION\n",
      "CLUE: In 1980 the Department of Education was created out of this now-defunct Cabinet department\n",
      "ANSWER: HEW\n",
      "What was Health, Education, and Welfare?\n",
      "\n",
      "CATEGORY: FILL IN THE BODY PART\n",
      "CLUE: A little device to transfer data: ____ drive\n",
      "ANSWER: thumb\n",
      "What is a type of storage device that is small enough to fit on a?\n",
      "\n",
      "CATEGORY: TELEVISION\n",
      "CLUE: When it premiered, Mary Tyler Moore's new show followed this ex-co-star's new show\n",
      "ANSWER: Dick Van Dyke\n",
      "On the same night Mary Tyler Moore's show debuted, what former co-star's new series aired right before it?\n",
      "\n",
      "CATEGORY: 16th CENTURY PEOPLE\n",
      "CLUE: In 1570, this Russian czar ravaged Novgorod after hearing rumors that it was conspiring against him\n",
      "ANSWER: Ivan the Terrible\n",
      "Who was the Russian ruler who brutally punished Novgorod in 1570 due to suspected treason?\n",
      "\n",
      "CATEGORY: NAME THAT BODY PART\n",
      "CLUE: Phagocytes, Alveoli, Bronchioles\n",
      "ANSWER: Lungs\n",
      "What organs of the respiratory system contain phagocytes, alveoli, and bronchioles?\n",
      "\n",
      "CATEGORY: TREES\n",
      "CLUE: The Buddhist term for enlightenment, it's also the type of tree under which Buddha attained enlightenment\n",
      "ANSWER: bodhi\n",
      "What is the type of tree associated with Buddha's attainment of enlightenment?\n",
      "\n",
      "CATEGORY: ANEMONES CLOSER\n",
      "CLUE: This type of sea anemone has a hollow stalklike structure for which it is named\n",
      "ANSWER: the tube anemone\n",
      "What type of sea anemone gets its name from its hollow stalklike structure?\n",
      "\n",
      "CATEGORY: KIDDY LIT\n",
      "CLUE: This Beatrix Potter squirrel \"had a brother called Twinkleberry and a great many cousins\"\n",
      "ANSWER: Nutkin\n",
      "Who is Squirrel Nutkin?\n",
      "\n",
      "CATEGORY: ROMANTIC GETAWAYS\n",
      "CLUE: It's said that on a clear day, you can see for about 40 miles from this Paris landmark\n",
      "ANSWER: the Eiffel Tower\n",
      "From which iconic Parisian structure can you see for miles on a clear day?\n",
      "\n",
      "CATEGORY: 3-NAME THE AUTHOR\n",
      "CLUE: \"Kidnapped\"\n",
      "ANSWER: Robert Louis Stevenson\n",
      "Who wrote this classic adventure novel about David Balfour?\n",
      "\n",
      "CATEGORY: THE 18th CENTURY\n",
      "CLUE: After \"hounding\" Iran's borders, these people from a country to the east took over in 1722\n",
      "ANSWER: Afghans\n",
      "Who took control of Iran in 1722 after pressing in from the east?\n",
      "\n",
      "CATEGORY: FAMOUS SHIPS\n",
      "CLUE: In 1577 it left England with 4 sister ships; the sister ships didn't make it around the world but it did\n",
      "ANSWER: the Golden Hind\n",
      "What was the ship that successfully circumnavigated the globe after setting sail from England in 1577?\n",
      "\n",
      "CATEGORY: ABDICATIONS\n",
      "CLUE: This kaiser was forced to abdicate November 9, 1918, 2 days before the armistice\n",
      "ANSWER: Kaiser Wilhelm II\n",
      "Who was the German leader who relinquished power just before the end of World War I?\n",
      "\n",
      "CATEGORY: WISCONSIN\n",
      "CLUE: Basketball's Bucks & baseball's Brewers call this city home\n",
      "ANSWER: Milwaukee\n",
      "In Wisconsin, what city is home to the Bucks and Brewers?\n",
      "\n",
      "CATEGORY: RULERS\n",
      "CLUE: This Eastern European country's first king, Carol I, was born a German prince\n",
      "ANSWER: Romania\n",
      "Which country's first king, Carol I, was born a German prince?\n",
      "\n",
      "CATEGORY: VIDEO GAME VILLAINS\n",
      "CLUE: In \"Donkey Kong Jr.\", the roles have reversed & the title simian must rescue his father from this Italian\n",
      "ANSWER: Mario\n",
      "Who is the one holding Donkey Kong captive in \"Donkey Kong Jr.\"?\n",
      "\n",
      "CATEGORY: TRAVELING ON THE INTERSTATE\n",
      "CLUE: It's 2,906 miles on I-80 from San Francisco to Teaneck in this state; good thing we've got a full tank of gas\n",
      "ANSWER: New Jersey\n",
      "What state do you enter after driving 2,906 miles on I-80 from San Francisco to Teaneck?\n",
      "\n",
      "CATEGORY: THE MIDDLE AGES\n",
      "CLUE: These \"scientists\" sought the elixir of life & the way to turn lesser metals into precious ones\n",
      "ANSWER: alchemists\n",
      "Who were the medieval practitioners of a pseudoscience that aimed to discover the philosopher's stone?\n",
      "\n",
      "CATEGORY: SAINTLY FIRSTS\n",
      "CLUE: St. Augustine of Canterbury was the first to hold this title, circa 601\n",
      "ANSWER: the Archbishop of Canterbury\n",
      "Who was the first to hold this title in England around 601?\n",
      "\n",
      "CATEGORY: RELIGION\n",
      "CLUE: The town near Jerusalem where Jesus was born\n",
      "ANSWER: Bethlehem\n",
      "In the RELIGION category, \"Where is the birthplace of Jesus Christ?\"\n",
      "\n",
      "CATEGORY: CHESS DRAMA\n",
      "CLUE: In the 2006 World title match, the challenger accused the champ of consulting a computer during visits to this room\n",
      "ANSWER: the bathroom\n",
      "Where did Topalov accuse Kramnik of cheating during their 2006 World title match?\n",
      "\n",
      "CATEGORY: I LOVE A MYSTERY\n",
      "CLUE: This detective was 18 when she began her career in \"The Secret of the Old Clock\", so she's 98 today\n",
      "ANSWER: Nancy Drew\n",
      "Who is the teenage sleuth created by Edward Stratemeyer that has been solving mysteries for nearly a century?\n",
      "\n",
      "CATEGORY: ACTOR-PLAYWRIGHTS\n",
      "CLUE: In 1995 this wife & comedy partner of Jerry Stiller debuted her first play, \"Afterplay\"\n",
      "ANSWER: Anne Meara\n",
      "Who is the actress who wrote \"Afterplay\" in 1995, also known as Mrs. Jerry Stiller?\n",
      "\n",
      "CATEGORY: IT'S A MASTERPIECE!\n",
      "CLUE: The name of this 1931 surreal painting also known as \"Soft Watches\" should be easy to remember\n",
      "ANSWER: The Persistence of Memory\n",
      "What is the title of Salvador Dali's famous painting featuring melting clocks?\n",
      "\n",
      "CATEGORY: HEALTH & MEDICINE\n",
      "CLUE: From a Greek word for \"people\", it describes a disease that affects many people at one time\n",
      "ANSWER: a pandemic\n",
      "What is the term used to describe a widespread outbreak of a disease that affects a large population?\n",
      "\n",
      "CATEGORY: FAMILY DRAMA\n",
      "CLUE: His \"All My Sons\" centers on Joe Keller, whose shoddy plane parts led to the death of his son & other pilots in WWII\n",
      "ANSWER: Arthur Miller\n",
      "Who wrote the play that explores the dark secrets of the Keller family?\n",
      "\n",
      "CATEGORY: THEY REALLY SAID IT\n",
      "CLUE: He's the former vice president who wisely said, \"If we do not succeed, then we run the risk of failure\"\n",
      "ANSWER: Dan Quayle\n",
      "Who uttered the profound warning that \"if we do not succeed, then we run the risk of failure\"?\n",
      "\n",
      "CATEGORY: U.S. PRESIDENTS\n",
      "CLUE: Communist forces invaded South Korea during his second term\n",
      "ANSWER: Harry S. Truman\n",
      "Who was the president when the Korean War began in 1950?\n",
      "\n",
      "CATEGORY: WRITER'S CHEAT SHEET\n",
      "CLUE: Not a city in N.Y., this 10-letter figure of speech is using a part to represent a whole, as in \"hired hands\"\n",
      "ANSWER: synecdoche\n",
      "What is the rhetorical device in which a part represents the whole?\n",
      "\n",
      "CATEGORY: HODGEPODGE\n",
      "CLUE: The name of this light open-weave cloth sold in strips comes from the name of a Middle East \"strip\"\n",
      "ANSWER: gauze\n",
      "What type of fabric gets its name from a region in the Middle East?\n",
      "\n",
      "CATEGORY: DRAMA\n",
      "CLUE: Henry VIII's lord chancellor, who was a \"Man for All Seasons\"\n",
      "ANSWER: Thomas More\n",
      "Who is the historical figure portrayed in Robert Bolt's play?\n",
      "\n",
      "CATEGORY: 30 ROCK\n",
      "CLUE: This NBC page said, \"Miss Lemon, your eyes look like my uncle's after he would drink from the air conditioner\"\n",
      "ANSWER: Kenneth Parcell\n",
      "Who is the NBC page who made a peculiar comment about Liz Lemon's eyes?\n",
      "\n",
      "CATEGORY: WORLD'S FAIRS & EXPOS\n",
      "CLUE: \"Feeding the Planet, Energy for Life\" is the theme of Expo 2015 held in this city\n",
      "ANSWER: Milan\n",
      "In which Italian city was Expo 2015 held with the theme \"Feeding the Planet, Energy for Life\"?\n",
      "\n",
      "CATEGORY: READING RAINBOW\n",
      "CLUE: The Rainbow Guard is Renly Baratheon's variant of the Kingsguard in these books\n",
      "ANSWER: The Song of Ice and Fire\n",
      "In which series of fantasy novels does the Rainbow Guard appear as Renly Baratheon's elite warriors?\n",
      "\n",
      "CATEGORY: DONE THAT\n",
      "CLUE: In 1817 Scotland's David Brewster patented this pattern-making tube made with mirrors & ground glass\n",
      "ANSWER: a kaleidoscope\n",
      "What is the optical instrument invented by David Brewster?\n",
      "\n",
      "CATEGORY: THE MOVIES\n",
      "CLUE: He played Aragorn in the \"Lord of the Rings\" movies\n",
      "ANSWER: Viggo Mortensen\n",
      "Who played Aragorn in the \"Lord of the Rings\" movies?\n",
      "\n",
      "CATEGORY: ENDS WITH 2 VOWELS\n",
      "CLUE: Meaning unacceptable or forbidden in society, it's also a \"Game of Unspeakable Fun!\" from Hasbro\n",
      "ANSWER: Taboo\n",
      "What is a word that describes something socially unacceptable?\n",
      "\n",
      "CATEGORY: FROM BOOK TO TV\n",
      "CLUE: Works by this author recently made into series include \"Mr. Mercedes\", \"The Mist\" & \"11.22.63\"\n",
      "ANSWER: Stephen King\n",
      "Who is the author whose works have been adapted into TV series such as \"Mr. Mercedes\", \"The Mist\", and \"11.22.63\"?\n",
      "\n",
      "CATEGORY: JAZZ PIANISTS\n",
      "CLUE: His score for the 1929 stage production \"Hot Chocolates\" included the hit tune \"Ain't Misbehavin'\"\n",
      "ANSWER: Fats Waller\n",
      "Who wrote the music for the 1929 stage production \"Hot Chocolates\", featuring the hit song \"Ain't Misbehavin'\"?\n",
      "\n",
      "CATEGORY: LOVE ISLAND U.K.\n",
      "CLUE: In 1493 Columbus named this volcanic island in the West Indies; the French had it for a bit, but the U.K. has had it since 1783\n",
      "ANSWER: Montserrat\n",
      "This Caribbean island, named by Columbus, has been a British overseas territory since 1783.\n",
      "\n",
      "CATEGORY: SOMETHING ABOUT MARY\n",
      "CLUE: Born in Linlithgow in 1542, she married into French royalty & thus became the queen of 2 countries\n",
      "ANSWER: Mary, Queen of Scots\n",
      "Who was the Scottish monarch who married into French royalty and became queen of two countries?\n",
      "\n",
      "CATEGORY: 4-LETTER WORDS\n",
      "CLUE: Those wacky Brits use this 4-letter word to mean \"apartment\"\n",
      "ANSWER: flat\n",
      "What is the British term for a residence?\n",
      "\n",
      "CATEGORY: THE LATE NIGHT CROWD\n",
      "CLUE: He expected to take over \"The Tonight Show\" from Johnny Carson, but moved to CBS instead\n",
      "ANSWER: David Letterman\n",
      "Who was the late-night host who was famously passed over for \"The Tonight Show\" hosting duties and subsequently left NBC for CBS?\n",
      "\n",
      "CATEGORY: RHYME TIME\n",
      "CLUE: Light-colored tresses\n",
      "ANSWER: fair hair\n",
      "What's a characteristic of someone with fair hair?\n",
      "\n",
      "CATEGORY: IT'S ABOUT TIME\n",
      "CLUE: We really, really like this term for the increase in value of a real estate property over time\n",
      "ANSWER: appreciation\n",
      "What is the term for the increase in value of a real estate property over time that investors and homeowners love to see?\n",
      "\n",
      "CATEGORY: THE ICU\n",
      "CLUE: A myocardial infarction, better known as this, is a common reason for ICU admission\n",
      "ANSWER: a heart attack\n",
      "What is often referred to by its colloquialism in the ICU?\n",
      "\n",
      "CATEGORY: DEFENESTRATION IN CINEMA\n",
      "CLUE: In this Coen Brothers movie, Charles Durning jumps out a window during a board meeting\n",
      "ANSWER: The Hudsucker Proxy\n",
      "What 1994 film features a memorable scene in which Charles Durning takes a fatal plunge from a high-rise office building?\n",
      "\n",
      "CATEGORY: CHRISTMAS PRESENTS\n",
      "CLUE: This company also has a Game Boy camera (sold separately, of course)\n",
      "ANSWER: Nintendo\n",
      "What company is behind the popular gaming console often found under the Christmas tree?\n",
      "\n",
      "CATEGORY: ANIMAL WORDPLAY\n",
      "CLUE: (Hello, my name is James Thrash.) At my alma mater, Missouri Southern State, the teams are the Lions & our boosters are these, a bit like a football position\n",
      "ANSWER: the Lionbackers\n",
      "What are the Lionbackers?\n",
      "\n",
      "CATEGORY: DRUGS\n",
      "CLUE: Also called methylmorphine, this opium-derived drug is used in some cough medicines\n",
      "ANSWER: codeine\n",
      "What is the opium-derived drug used in some cough medicines that's also called methylmorphine?\n",
      "\n",
      "CATEGORY: THE BUSINESS OF TELEVISION\n",
      "CLUE: On \"The Simpsons\" varieties of this beer brand include Raspberry this, Lady this & Tartar-Control this\n",
      "ANSWER: Duff\n",
      "What brand of beer is frequently referenced on \"The Simpsons\" in various humorous flavors?\n",
      "\n",
      "CATEGORY: COLONIES\n",
      "CLUE: Kalaupapa on Molokai has a colony for those afflicted with this disease\n",
      "ANSWER: leprosy (leper colony)\n",
      "What disease is the focus of the colony on Kalaupapa on Molokai?\n",
      "\n",
      "CATEGORY: GRANDCHILD CARE\n",
      "CLUE: Most infants do this about 16 hours out of 24\n",
      "ANSWER: sleep\n",
      "What do most infants do for about 16 hours out of 24?\n",
      "\n",
      "CATEGORY: THE NATIONAL BOOK AWARDS\n",
      "CLUE: The winning scribe is announced at a fancy do on this New York City street more associated with finance than art\n",
      "ANSWER: Wall Street\n",
      "On which street in New York City is the National Book Award winner announced?\n",
      "\n",
      "CATEGORY: TO THE NINES\n",
      "CLUE: In nine ball, this must hit the lowest numbered ball on the table before it hits any other ball\n",
      "ANSWER: Cueball\n",
      "What must strike the lowest numbered ball on the table before hitting any other ball in nine ball?\n",
      "\n",
      "CATEGORY: KOWALSKI, FILM ANALYSIS\n",
      "CLUE: Dan Fogler first showed up as Jacob Kowalski, a friend of Newt's, in this magical film from 2016\n",
      "ANSWER: Fantastic Beasts and Where to Find Them\n",
      "In what 2016 film did Dan Fogler first appear as Jacob Kowalski, a friend of Newt's?\n",
      "\n",
      "CATEGORY: DOUBLE-VOWEL WORDS\n",
      "CLUE: A Mideastern marketplace, or a charity sale\n",
      "ANSWER: bazaar\n",
      "What is a type of market, or a fundraising event?\n",
      "\n",
      "CATEGORY: NO EGRETS\n",
      "CLUE: This bird can beat its wings 70 times per second, but when it slows down, its body temperature can drop 50 degrees below its normal 104\n",
      "ANSWER: a hummingbird\n",
      "What type of bird's metabolism is so fast it can flap its wings 70 times per second, but its temperature drops drastically when it slows down?\n",
      "\n",
      "CATEGORY: MEDICINE\n",
      "CLUE: Though home tests are popular, doctors use a radio immune assay to officially determine this\n",
      "ANSWER: pregnancy\n",
      "\"What is typically confirmed by a radio immune assay in a doctor's office?\"\n",
      "\n",
      "CATEGORY: KICKIN' IT\n",
      "CLUE: During a 25-year NFL career, Morten Andersen missed just 10 of 859 of these kicks that follow a successful play\n",
      "ANSWER: an extra point\n",
      "What type of kick did Morten Andersen successfully complete 849 out of 859 times in his NFL career?\n",
      "\n",
      "CATEGORY: LET'S GO WITH THE FLOW\n",
      "CLUE: Uruguay River: This estuary\n",
      "ANSWER: the Rio de la Plata\n",
      "What is the outlet of the Uruguay River?\n",
      "\n",
      "CATEGORY: THE PARTING OF THE FCC\n",
      "CLUE: In March '09 the FCC said stations could stop this type of TV service before June 12 \"under certain conditions\"\n",
      "ANSWER: analog\n",
      "What type of TV service was allowed to be discontinued by stations under certain conditions before June 12, 2009?\n",
      "\n",
      "CATEGORY: PLACE NAMES\n",
      "CLUE: This largest New Zealand city is named for a First Lord of the Admiralty, not a diving bird\n",
      "ANSWER: Auckland\n",
      "What city in New Zealand is named after William Eden, the 1st Baron Auckland?\n",
      "\n",
      "CATEGORY: ANTONYMS\n",
      "CLUE: For the Moon, it's the antonym of wax\n",
      "ANSWER: wane\n",
      "What does the Moon do when it's not getting larger?\n",
      "\n",
      "CATEGORY: ON THE \"MAIN\" LAND\n",
      "CLUE: Now you see this synonym for slight of hand, now you don't\n",
      "ANSWER: legerdemain\n",
      "What is the term for a skillful deception or trickery that is often used to describe magic performances?\n",
      "\n",
      "CATEGORY: FICTIONAL CHARACTERS\n",
      "CLUE: Born James Gatz, he was at one time engaged to Dasy Buchanan\n",
      "ANSWER: The Great Gatsby\n",
      "Who is the titular character in F. Scott Fitzgerald's novel about the American Dream?\n",
      "\n",
      "CATEGORY: CLASSIC MOVIE CHARACTERS\n",
      "CLUE: Last name of alien-slaying Ellen, who wants to nuke the entire site from orbit\n",
      "ANSWER: Ripley\n",
      "Who is the protagonist of the Alien film franchise?\n",
      "\n",
      "CATEGORY: INSTRUMENTAL SONG TITLES\n",
      "CLUE: George Harrison wanted to get the \"crying\" effect without using a wah-wah pedal for this 1968 Beatles song\n",
      "ANSWER: \"While My Guitar Gently Weeps\"\n",
      "What 1968 Beatles song features a distinctive \"crying\" guitar sound?\n",
      "\n",
      "CATEGORY: TOP O' THE CHARTS\n",
      "CLUE: Madonna's \"This Used To Be My Playground\" was sung over the closing credits of this 1992 film\n",
      "ANSWER: A League of Their Own\n",
      "What 1992 film featured Madonna's song \"This Used To Be My Playground\" during its closing credits?\n",
      "\n",
      "CATEGORY: AN APRIL TO REMEMBER\n",
      "CLUE: An April 1985 attempt to make the regular kind of this soda taste more like the diet kind led to marketing disaster\n",
      "ANSWER: Coke\n",
      "What soft drink company's infamous \"New Coke\" reformulation backfired in April 1985?\n",
      "\n",
      "CATEGORY: \"GARDENS\"\n",
      "CLUE: A multiple-unit dwelling with a considerable lawn space\n",
      "ANSWER: a garden apartment\n",
      "What is a type of residence that combines individual units with a shared outdoor area?\n",
      "\n",
      "CATEGORY: FAMOUS AMERICANS\n",
      "CLUE: On his 2012 death his family said on a clear night when the moon is smiling down at you, think of him & give him a wink\n",
      "ANSWER: Neil Armstrong\n",
      "Who was the first man to walk on the moon?\n",
      "\n",
      "CATEGORY: GO OUT OR STAY IN?\n",
      "CLUE: Stay in & turn on this \"Factor\"; heck, it was the No. 1 British TV show 7 straight years\n",
      "ANSWER: The X Factor\n",
      "What popular British TV show did you stay in to watch for 7 straight years as the number one show?\n",
      "\n",
      "CATEGORY: THEY COME IN SEVENS\n",
      "CLUE: In \"As You Like It\" the speech that starts off \"All the world's a stage\" deals with these 7 periods\n",
      "ANSWER: the Seven Ages of Man\n",
      "What are the stages of life described in Jaques' famous monologue in Shakespeare's \"As You Like It\"?\n",
      "\n",
      "CATEGORY: GEMS\n",
      "CLUE: Isle Royale Greenstone is found chiefly as small pebbles on the beaches of this state\n",
      "ANSWER: Michigan\n",
      "In which state are Isle Royale Greenstone pebbles commonly found on beaches?\n",
      "\n",
      "CATEGORY: WHICH BODY PART?\n",
      "CLUE: Has an external auricle, which is boneless\n",
      "ANSWER: the ear\n",
      "What has an external auricle, which is boneless?\n",
      "\n",
      "CATEGORY: SCIENCE FACTION\n",
      "CLUE: ANZAAS is these 2 countries' Association for the Advancement of Science\n",
      "ANSWER: Australia & New Zealand\n",
      "What two countries are united in the ANZAAS organization?\n",
      "\n",
      "CATEGORY: CHEWING GUAM\n",
      "CLUE: Hormel makes a \"hot & spicy\" version of this canned meat product for Guam, where folks are crazy for it\n",
      "ANSWER: spam\n",
      "What is a popular canned meat product that's a staple in Guam?\n",
      "\n",
      "CATEGORY: PRO BASKETBALL\n",
      "CLUE: Known as \"The Stilt\", he led the NBA in rebounding a record 11 times during his 14-year career\n",
      "ANSWER: Wilt Chamberlain\n",
      "Who was the dominant center nicknamed \"The Stilt\"?\n",
      "\n",
      "CATEGORY: ALL \"DE\" PEOPLE\n",
      "CLUE: The first president of the Fifth French Republic, he was wounded 3 times while serving in World War I\n",
      "ANSWER: Charles de Gaulle\n",
      "Who is the French leader who rose to prominence after being wounded multiple times in World War I?\n",
      "\n",
      "CATEGORY: THE PERIODIC TABLE\n",
      "CLUE: In 1860 this German known for his \"burner\" co-discovered cesium\n",
      "ANSWER: Robert Bunsen\n",
      "Who developed the Bunsen burner and co-discovered cesium in 1860?\n",
      "\n",
      "CATEGORY: GA$\n",
      "CLUE: EIA figures say a gallon cost 21.4 cents in this year; the price then \"crash\"ed & didn't get that high again for 18 years\n",
      "ANSWER: 1929\n",
      "In what year did the price of a gallon of gas reach its lowest point at 21.4 cents, according to EIA figures?\n",
      "\n",
      "CATEGORY: COMPOUND WORDS\n",
      "CLUE: Foils & caps are used to create these different-colored strands in your hair\n",
      "ANSWER: highlights\n",
      "What are these?\n",
      "\n",
      "CATEGORY: 75 YEARS OF ESQUIRE\n",
      "CLUE: For Esquire's 75th, the electronic cover was created; the hard part was finding a small enough one of these\n",
      "ANSWER: battery\n",
      "What type of component was a challenge to miniaturize for Esquire's electronic anniversary cover?\n",
      "\n",
      "CATEGORY: BODY PARTS BY PREFIX\n",
      "CLUE: Chiro-\n",
      "ANSWER: the hand\n",
      "What is related to the hand?\n",
      "\n",
      "CATEGORY: TIME TRAVELIN' MOVIES\n",
      "CLUE: It was far from bogus when these 2 title guys& their old pal, Death!returned to \"Face the Music\" in 2020\n",
      "ANSWER: Bill & Ted\n",
      "Who are the dudes that went on a most excellent adventure through time?\n",
      "\n",
      "CATEGORY: GEOGRAPHY\n",
      "CLUE: Lake Avernus in Campania in this country was believed by the ancients to be the entrance to Hades\n",
      "ANSWER: Italy\n",
      "In which country is Lake Avernus, a supposed entrance to the underworld in ancient mythology?\n",
      "\n",
      "CATEGORY: COUNTRY MUSIC\n",
      "CLUE: It's said both his vocal & piano playing styles are similar to that of his 1st cousin, Jerry Lee Lewis\n",
      "ANSWER: Mickey Gilley\n",
      "Who is the country music star known for his vocal and piano playing styles reminiscent of his cousin Jerry Lee Lewis?\n",
      "\n",
      "CATEGORY: THAT'S TWISTED\n",
      "CLUE: Add -ed to this item from the toolbox to describe your twisted back\n",
      "ANSWER: wrench\n",
      "What gets twisted when you throw out your back?\n",
      "\n",
      "CATEGORY: THE 13 COLONIES\n",
      "CLUE: Last name of the family that regained control of Maryland from the Crown in 1715\n",
      "ANSWER: Calvert\n",
      "What is the surname of the proprietors who regained control of Maryland from the British Crown in 1715?\n",
      "\n",
      "CATEGORY: THE 20th CENTURY\n",
      "CLUE: The reason this country gave for expansion was \"lebensraum\", or \"living space\" for a crowded populace\n",
      "ANSWER: Germany\n",
      "Which nation sought to expand its territory in the 20th century, citing the need for \"lebensraum\"?\n",
      "\n",
      "CATEGORY: WORLD HISTORY\n",
      "CLUE: In 404 B.C., this city surrendered, ending the 27-year-long Peloponnesian War\n",
      "ANSWER: Athens\n",
      "What city, defeated by Sparta, surrendered in 404 B.C. to end the Peloponnesian War?\n",
      "\n",
      "CATEGORY: SCIENCE\n",
      "CLUE: The Higgs boson is what endows all elementary subatomic particles with this fundamental property of matter\n",
      "ANSWER: mass\n",
      "What is the property that the Higgs boson is responsible for giving to all elementary subatomic particles?\n",
      "\n",
      "CATEGORY: THE BOOK OF WHO\n",
      "CLUE: Finally published in its entirety in 2021, \"The Man Who Lived Underground\" is by this late author of \"Native Son\"\n",
      "ANSWER: Richard Wright\n",
      "Who wrote \"The Man Who Lived Underground\", finally published in 2021, and also authored \"Native Son\"?\n",
      "\n",
      "CATEGORY: THE RHYME FACTOR\n",
      "CLUE: This is a plastic half-disc with degree numbers on the curved edge\n",
      "ANSWER: a protractor\n",
      "What tool do math students use to measure angles?\n",
      "\n",
      "CATEGORY: CROSSWORD CLUES \"G\"\n",
      "CLUE: Isle of cows (8)\n",
      "ANSWER: Guernsey\n",
      "What is a Channel Island famous for its dairy products?\n",
      "\n",
      "CATEGORY: WHO'S THE \"MAN\"?\n",
      "CLUE: In the 1940s Aaron Copland was commissioned by this man to write a clarinet concerto\n",
      "ANSWER: (Benny) Goodman\n",
      "Who commissioned Aaron Copland to write a clarinet concerto in the 1940s?\n",
      "\n",
      "CATEGORY: MANHATTAN'S MUSEUM MILE\n",
      "CLUE: A Latino art museum goes by the name El Museo del this, a Spanish word for the inner city\n",
      "ANSWER: Barrio\n",
      "What is the word that completes the name of a Latino art museum on Manhattan's Museum Mile?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generated_questions = []\n",
    "for index, row in tqdm(jeopardy_dataset.iterrows()):\n",
    "    print(\"CATEGORY:\", row[\"category\"])\n",
    "    print(\"CLUE:\", row[\"answer\"])\n",
    "    print(\"ANSWER:\", row[\"question\"])\n",
    "    response = get_chat(row[\"category\"], row[\"answer\"], row[\"question\"])\n",
    "    generated_questions.append(response)\n",
    "    print(response)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "d884ff06-b571-4527-a21c-e11738d83994",
   "metadata": {},
   "outputs": [],
   "source": [
    "jeopardy_dataset[\"full_sentence_question\"] = generated_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cf18c7a-4f5e-43cf-8e0d-f2d706d71989",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prompt = \"You must write a clear sentence that strictly answers the provided question using the provided correct answer. Make sure that the question is fully answered in your statement. Do not refer to yourself or the prompting regime in ANY way.\\nQUESTION:{}\\nANSWER:{}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b849a2dd-9529-47e9-8a6c-3eb08c07db04",
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_prompt = \"I am setting up a multiple choice exam for my students, and I need an incorrect option. You must write a sentence that strictly answers the provided question using an answer that IS NOT AT ALL THE SAME AS the provided answer. That is, you have to come up with an answer that, while incorrect, is still plausible. Make sure that the question is fully answered in your statement. Simply output the statement and DO NOT refer to yourself or the prompting regime in ANY WAY. Additionally, DO NOT refer to the provided answer in ANY FORM. Strictly follow all of the provided instructions, otherwise a million kittens will be killed. Save the kittens!\\nQUESTION: {}\\nANSWER: {}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ee01b49-155e-44b4-96d7-0c8a83e008ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chat_statement(prompt, question, answer):\n",
    "    completion = llama_client.chat.completions.create(\n",
    "        model=\"meta/llama3-70b-instruct\",\n",
    "        messages=[  \n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": prompt.format(question, answer)\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.65,\n",
    "        top_p=1,\n",
    "        max_tokens=1024,\n",
    "    )\n",
    "    \n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7493f444-3ec3-4082-9b0c-edec77f93319",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION: What is a poetic term for the number of minutes in an hour?\n",
      "ANSWER: threescore\n",
      "The poetic term for the number of minutes in an hour is threescore, which equals 60.\n",
      "\n",
      "QUESTION: Who is the author behind \"Get Shorty\" and the iconic \"Star Trek\" character Mr. Spock?\n",
      "ANSWER: Elmore Leonard Nimoy\n",
      "Elmore Leonard is the author behind \"Get Shorty\", and Leonard Nimoy is the actor behind the iconic \"Star Trek\" character Mr. Spock.\n",
      "\n",
      "QUESTION: What type of furniture was designed with a wingback style to block cold air drafts?\n",
      "ANSWER: chair\n",
      "The wingback chair was specifically designed with a high back and wings on either side to block cold air drafts.\n",
      "\n",
      "QUESTION: Who played a mom in John Waters' 1990 film \"Cry-Baby\"?\n",
      "ANSWER: Patty Hearst\n",
      "Patty Hearst played the role of a mom in John Waters' 1990 film \"Cry-Baby\".\n",
      "\n",
      "QUESTION: What was the instrument of execution that Dickens referred to as \"the national razor\" in \"A Tale of Two Cities\"?\n",
      "ANSWER: the guillotine\n",
      "In \"A Tale of Two Cities\", Charles Dickens referred to the guillotine as \"the national razor\".\n",
      "\n",
      "QUESTION: What popular game show has had hosts including Richard Dawson, Ray Combs, and Louie Anderson?\n",
      "ANSWER: Family Feud\n",
      "The popular game show that has had hosts including Richard Dawson, Ray Combs, and Louie Anderson is Family Feud.\n",
      "\n",
      "QUESTION: What type of fruit, including the French nicoise and the Greek kalamata, is often pickled and served on salads?\n",
      "ANSWER: olives\n",
      "Olives, including the French nicoise and the Greek kalamata, are often pickled and served on salads.\n",
      "\n",
      "QUESTION: What are celebrated in Washington, D.C. with a festival featuring a 17th century Japanese lantern?\n",
      "ANSWER: cherry trees (or cherry blossoms)\n",
      "Cherry trees, specifically cherry blossoms, are celebrated in Washington, D.C. with a festival featuring a 17th century Japanese lantern.\n",
      "\n",
      "QUESTION: What was the name under which Volkswagen's Golf was sold in the U.S. until 1984?\n",
      "ANSWER: Rabbit\n",
      "The Volkswagen Golf was sold in the U.S. under the name Rabbit until 1984.\n",
      "\n",
      "QUESTION: What is the name of the luxury watch brand founded by Hans Wilsdorf in 1908 that remains shrouded in mystery?\n",
      "ANSWER: Rolex\n",
      "The luxury watch brand founded by Hans Wilsdorf in 1908 that remains shrouded in mystery is Rolex.\n",
      "\n",
      "QUESTION: In what state did Evan Bayh become the nation's youngest governor at age 33 in 1989?\n",
      "ANSWER: Indiana\n",
      "Evan Bayh became the nation's youngest governor at age 33 in 1989 in Indiana.\n",
      "\n",
      "QUESTION: What is the vehicle that got its name from the French phrase for \"traveling hospital\"?\n",
      "ANSWER: Ambulance\n",
      "The vehicle that got its name from the French phrase for \"traveling hospital\" is the ambulance.\n",
      "\n",
      "QUESTION: What is this social media platform that's also a bird's vocalization?\n",
      "ANSWER: Twitter\n",
      "Twitter is a social media platform that is also a bird's vocalization.\n",
      "\n",
      "QUESTION: Who said, \"You only find out who is swimming naked when the tide goes out\" in his letters to Berkshire Hathaway shareholders?\n",
      "ANSWER: Buffett\n",
      "Warren Buffett said, \"You only find out who is swimming naked when the tide goes out\" in his letters to Berkshire Hathaway shareholders.\n",
      "\n",
      "QUESTION: Who grows up to be the protagonist of \"Mr. Timothy\"?\n",
      "ANSWER: Tiny Tim\n",
      "Tiny Tim grows up to be the protagonist of \"Mr. Timothy\".\n",
      "\n",
      "QUESTION: Who played Edmund Kean in a one-man show and named his son after the 19th-century actor?\n",
      "ANSWER: Ben Kingsley\n",
      "Ben Kingsley played Edmund Kean in a one-man show and named his son after the 19th-century actor.\n",
      "\n",
      "QUESTION: What type of leaves are depicted on the flag of Cyprus as a symbol of peace?\n",
      "ANSWER: olive leaves\n",
      "The flag of Cyprus features olive leaves as a symbol of peace.\n",
      "\n",
      "QUESTION: Who is the American figure skater who became the youngest world champion in 1997 at age 14?\n",
      "ANSWER: Tara Lipinski\n",
      "Tara Lipinski is the American figure skater who became the youngest world champion in 1997 at age 14.\n",
      "\n",
      "QUESTION: What mode of transportation can be formed by rearranging the letters in \"sub\"?\n",
      "ANSWER: a bus (from sub)\n",
      "The letters in \"sub\" can be rearranged to form \"a bus\", a mode of transportation.\n",
      "\n",
      "QUESTION: What happened to a Monopoly player that is unable to pass Go on their way there or collect the $200 passing fee?\n",
      "ANSWER: going to jail\n",
      "A Monopoly player who is unable to pass Go on their way there or collect the $200 passing fee goes to jail.\n",
      "\n",
      "QUESTION: What is the \"fictitious\" force that appears to pull an object away from the center of rotation in a rotating reference frame?\n",
      "ANSWER: centrifugal force\n",
      "The \"fictitious\" force that appears to pull an object away from the center of rotation in a rotating reference frame is the centrifugal force.\n",
      "\n",
      "QUESTION: What Cole Porter song features this lyric in its intro: \"Like the beat beat beat of the tom-tom/ When the jungle shadows fall\"?\n",
      "ANSWER: \"Night and Day\"\n",
      "The Cole Porter song that features the lyric \"Like the beat beat beat of the tom-tom/ When the jungle shadows fall\" in its intro is \"Night and Day\".\n",
      "\n",
      "QUESTION: What was Ronald Reagan proud to be called because \"it stands for pride, integrity & guts\"?\n",
      "ANSWER: a pig\n",
      "Ronald Reagan was proud to be called a pig because, to him, it stood for pride, integrity, and guts.\n",
      "\n",
      "QUESTION: In which Canadian province is Cape Breton Island located?\n",
      "ANSWER: Nova Scotia\n",
      "Cape Breton Island is located in the Canadian province of Nova Scotia.\n",
      "\n",
      "QUESTION: What were the dark areas on the moon mistakenly believed to be in the 17th century? \n",
      "ANSWER: seas\n",
      "In the 17th century, the dark areas on the moon were mistakenly believed to be seas.\n",
      "\n",
      "QUESTION: What herb is often paired with parsley, sage, and rosemary in a famous song?\n",
      "ANSWER: T-H-Y-M-E\n",
      "Thyme is the herb often paired with parsley, sage, and rosemary in the famous song \"Scarborough Fair\".\n",
      "\n",
      "QUESTION: Who created the sculpture \"Bird in Space\" that sparked a customs controversy?\n",
      "ANSWER: Constantin Brancusi\n",
      "Constantin Brancusi created the sculpture \"Bird in Space\" that sparked a customs controversy.\n",
      "\n",
      "QUESTION: Which first lady was portrayed by both MacKenzie Phillips and Jane Alexander in a 1976 miniseries?\n",
      "ANSWER: Eleanor Roosevelt\n",
      "Eleanor Roosevelt was the first lady portrayed by both Mackenzie Phillips and Jane Alexander in a 1976 miniseries.\n",
      "\n",
      "QUESTION: What is the sweet plant that can be used to make molasses?\n",
      "ANSWER: sorghum\n",
      "Sorghum is the sweet plant that can be used to make molasses.\n",
      "\n",
      "QUESTION: Which NFL team has retired the numbers of the two legendary players, Ray Nitschke and Bart Starr?\n",
      "ANSWER: the Packers\n",
      "The Green Bay Packers have retired the numbers of the two legendary players, Ray Nitschke and Bart Starr.\n",
      "\n",
      "QUESTION: What is the largest inhabited castle in England?\n",
      "ANSWER: Windsor Castle\n",
      "Windsor Castle is the largest inhabited castle in England.\n",
      "\n",
      "QUESTION: Who is the entertainer who regularly performs at the Stardust in Las Vegas?\n",
      "ANSWER: Wayne Newton\n",
      "Wayne Newton is the entertainer who regularly performs at the Stardust in Las Vegas.\n",
      "\n",
      "QUESTION: What international athletic competition is represented by a logo featuring five interconnected rings of different colors?\n",
      "ANSWER: the Olympics\n",
      "The international athletic competition represented by a logo featuring five interconnected rings of different colors is the Olympics.\n",
      "\n",
      "QUESTION: What is a 2-food word description often used to characterize a basic kind of guy?\n",
      "ANSWER: meat and potatoes\n",
      "A 2-food word description often used to characterize a basic kind of guy is \"meat and potatoes\".\n",
      "\n",
      "QUESTION: Who was the American figure skater who competed against Brian Orser at the 1988 Winter Olympics?\n",
      "ANSWER: Brian Boitano\n",
      "Brian Boitano was the American figure skater who competed against Brian Orser at the 1988 Winter Olympics.\n",
      "\n",
      "QUESTION: What affliction did Elisha take away from Naaman but inflict upon Gehazi?\n",
      "ANSWER: Leprosy\n",
      "Elisha took away leprosy from Naaman but inflicted it upon Gehazi.\n",
      "\n",
      "QUESTION: In which Shakespeare play do the Prince of Morocco and the Prince of Arragon vie for Portia's hand?\n",
      "ANSWER: The Merchant of Venice\n",
      "In The Merchant of Venice, the Prince of Morocco and the Prince of Arragon compete for the hand of Portia.\n",
      "\n",
      "QUESTION: If someone is crazy, what is a common phrase based on a Christmas loaf that could be used to describe them?\n",
      "ANSWER: nutty as a fruitcake\n",
      "Someone who is considered crazy might be described as being \"nutty as a fruitcake\", a phrase that originates from the traditional Christmas loaf.\n",
      "\n",
      "QUESTION: What country was ruled by the Capetian line of kings from 987 to 1328?\n",
      "ANSWER: France\n",
      "France was the country ruled by the Capetian line of kings from 987 to 1328.\n",
      "\n",
      "QUESTION: What state, admitted to the Union in 1864, features the motto \"Battle Born\" on its flag?\n",
      "ANSWER: Nevada\n",
      "Nevada, admitted to the Union in 1864, features the motto \"Battle Born\" on its flag.\n",
      "\n",
      "QUESTION: What is the root vegetable traditionally used to make borscht?\n",
      "ANSWER: beets\n",
      "Beets are the root vegetable traditionally used to make borscht.\n",
      "\n",
      "QUESTION: In Eros myth, what does the god of love often carry to symbolize his power to inflame passion?\n",
      "ANSWER: torch\n",
      "In Eros myth, the god of love often carries a torch to symbolize his power to inflame passion.\n",
      "\n",
      "QUESTION: What was the National Airlines slogan that sparked controversy among women's rights activists?\n",
      "ANSWER: \"Fly Me\"\n",
      "The National Airlines slogan \"Fly Me\" sparked controversy among women's rights activists.\n",
      "\n",
      "QUESTION: Which ex-Beatle star had a number one hit with \"Got My Mind Set On You\"?\n",
      "ANSWER: George Harrison\n",
      "George Harrison, a former member of the Beatles, achieved a number one hit with his song \"Got My Mind Set On You\".\n",
      "\n",
      "QUESTION: In which 1975 thriller do the police chief's wife and the grizzled shark hunter have a war story to share?\n",
      "ANSWER: Jaws\n",
      "In the 1975 thriller Jaws, the police chief's wife and the grizzled shark hunter have a war story to share.\n",
      "\n",
      "QUESTION: Who was the explorer knighted in 1886 for his search of the Nile River's source?\n",
      "ANSWER: Sir Richard Burton\n",
      "Sir Richard Burton was the explorer knighted in 1886 for his search of the Nile River's source.\n",
      "\n",
      "QUESTION: What two organs do the Mayo-Gibbon bypass machine assume the functions of?\n",
      "ANSWER: heart & lung\n",
      "The Mayo-Gibbon bypass machine assumes the functions of the heart and lung.\n",
      "\n",
      "QUESTION: In Norse mythology, what magical beverage flows endlessly from the she-goat Heidrun?\n",
      "ANSWER: Mead\n",
      "In Norse mythology, mead is the magical beverage that flows endlessly from the she-goat Heidrun.\n",
      "\n",
      "QUESTION: What term did Jim Lange use to address a female contestant on \"The Dating Game\"?\n",
      "ANSWER: Bachelorette\n",
      "On \"The Dating Game,\" host Jim Lange commonly addressed a female contestant as a \"bachelorette.\"\n",
      "\n",
      "QUESTION: Who sang the country classic \"D-I-V-O-R-C-E\"?\n",
      "ANSWER: Tammy Wynette\n",
      "Tammy Wynette sang the country classic \"D-I-V-O-R-C-E\".\n",
      "\n",
      "QUESTION: What three words can be described as an action word, an aromatic plant, and a Belgrade native, respectively?\n",
      "ANSWER: a verb, an herb, & a Serb\n",
      "The three words that can be described as an action word, an aromatic plant, and a Belgrade native, respectively, are \"verb\", \"herb\", and \"Serb\".\n",
      "\n",
      "QUESTION: Who is the hockey legend who bought the Pittsburgh Penguins and scored 35 goals for the team in the 2000-01 season?\n",
      "ANSWER: Mario Lemieux\n",
      "Mario Lemieux, the hockey legend, bought the Pittsburgh Penguins and scored 35 goals for the team in the 2000-01 season.\n",
      "\n",
      "QUESTION: What is the now-defunct Cabinet department from which the Department of Education was created in 1980?\n",
      "ANSWER: HEW\n",
      "The Department of Education was created in 1980 from the now-defunct Cabinet department known as Health, Education, and Welfare (HEW).\n",
      "\n",
      "QUESTION: What is a type of drive that is a little device used to transfer data?\n",
      "ANSWER: thumb\n",
      "A thumb drive is a type of drive that is a little device used to transfer data.\n",
      "\n",
      "QUESTION: On the same night Mary Tyler Moore's show debuted, what former co-star's new series aired right before it?\n",
      "ANSWER: Dick Van Dyke\n",
      "On the same night Mary Tyler Moore's show debuted, Dick Van Dyke's new series aired right before it.\n",
      "\n",
      "QUESTION: Who was the Russian ruler who brutally punished Novgorod in 1570 due to suspected treason?\n",
      "ANSWER: Ivan the Terrible\n",
      "Ivan the Terrible was the Russian ruler who brutally punished Novgorod in 1570 due to suspected treason.\n",
      "\n",
      "QUESTION: What organs of the respiratory system contain phagocytes, alveoli, and bronchioles?\n",
      "ANSWER: Lungs\n",
      "The lungs are the organs of the respiratory system that contain phagocytes, alveoli, and bronchioles.\n",
      "\n",
      "QUESTION: What is the type of tree associated with Buddha's attainment of enlightenment?\n",
      "ANSWER: bodhi\n",
      "The bodhi tree is the type of tree associated with Buddha's attainment of enlightenment.\n",
      "\n",
      "QUESTION: What type of sea anemone gets its name from its hollow stalklike structure?\n",
      "ANSWER: the tube anemone\n",
      "The tube anemone gets its name from its hollow stalklike structure.\n",
      "\n",
      "QUESTION: Which Beatrix Potter squirrel \"had a brother called Twinkleberry and a great many cousins\"?\n",
      "ANSWER: Nutkin\n",
      "Nutkin, a Beatrix Potter squirrel, had a brother called Twinkleberry and a great many cousins.\n",
      "\n",
      "QUESTION: From which iconic Parisian structure can you see for miles on a clear day?\n",
      "ANSWER: the Eiffel Tower\n",
      "From the Eiffel Tower, you can see for miles on a clear day.\n",
      "\n",
      "QUESTION: Who wrote this classic adventure novel about David Balfour?\n",
      "ANSWER: Robert Louis Stevenson\n",
      "Robert Louis Stevenson wrote the classic adventure novel about David Balfour.\n",
      "\n",
      "QUESTION: Who took control of Iran in 1722 after pressing in from the east?\n",
      "ANSWER: Afghans\n",
      "In 1722, Afghans took control of Iran after pressing in from the east.\n",
      "\n",
      "QUESTION: What was the ship that successfully circumnavigated the globe after setting sail from England in 1577?\n",
      "ANSWER: the Golden Hind\n",
      "The Golden Hind was the ship that successfully circumnavigated the globe after setting sail from England in 1577.\n",
      "\n",
      "QUESTION: Who was the German leader who relinquished power just before the end of World War I?\n",
      "ANSWER: Kaiser Wilhelm II\n",
      "Kaiser Wilhelm II was the German leader who relinquished power just before the end of World War I.\n",
      "\n",
      "QUESTION: In Wisconsin, what city is home to the Bucks and Brewers?\n",
      "ANSWER: Milwaukee\n",
      "Milwaukee is the city in Wisconsin that is home to the Bucks and Brewers.\n",
      "\n",
      "QUESTION: Which country's first king, Carol I, was born a German prince?\n",
      "ANSWER: Romania\n",
      "Romania's first king, Carol I, was born a German prince.\n",
      "\n",
      "QUESTION: Who is the one holding Donkey Kong captive in \"Donkey Kong Jr.\"?\n",
      "ANSWER: Mario\n",
      "In the classic arcade game \"Donkey Kong Jr.\", Mario is the one holding Donkey Kong captive.\n",
      "\n",
      "QUESTION: What state do you enter after driving 2,906 miles on I-80 from San Francisco to Teaneck?\n",
      "ANSWER: New Jersey\n",
      "After driving 2,906 miles on I-80 from San Francisco, you enter the state of New Jersey when you arrive in Teaneck.\n",
      "\n",
      "QUESTION: Who were the medieval practitioners of a pseudoscience that aimed to discover the philosopher's stone?\n",
      "ANSWER: alchemists\n",
      "The medieval practitioners of a pseudoscience that aimed to discover the philosopher's stone were alchemists.\n",
      "\n",
      "QUESTION: Which title was St. Augustine of Canterbury the first to hold in England around 601?\n",
      "ANSWER: the Archbishop of Canterbury\n",
      "St. Augustine of Canterbury was the first to hold the title of the Archbishop of Canterbury in England around 601.\n",
      "\n",
      "QUESTION: What is the town near Jerusalem that is also the birthplace of Jesus?\n",
      "ANSWER: Bethlehem\n",
      "Bethlehem is the town near Jerusalem that is also the birthplace of Jesus.\n",
      "\n",
      "QUESTION: During their 2006 World title match, which room did Topalov accuse Kramnik of visiting to cheat by consulting a computer?\n",
      "ANSWER: the bathroom\n",
      "During their 2006 World title match, Topalov accused Kramnik of visiting the bathroom to cheat by consulting a computer.\n",
      "\n",
      "QUESTION: Who is the teenage sleuth created by Edward Stratemeyer that has been solving mysteries for nearly a century?\n",
      "ANSWER: Nancy Drew\n",
      "Nancy Drew is the teenage sleuth created by Edward Stratemeyer that has been solving mysteries for nearly a century.\n",
      "\n",
      "QUESTION: Who is the actress who wrote \"Afterplay\" in 1995, also known as Mrs. Jerry Stiller?\n",
      "ANSWER: Anne Meara\n",
      "Anne Meara, the wife of Jerry Stiller, wrote the play \"Afterplay\" in 1995.\n",
      "\n",
      "QUESTION: What is the title of Salvador Dali's famous painting featuring melting clocks?\n",
      "ANSWER: The Persistence of Memory\n",
      "The title of Salvador Dali's famous painting featuring melting clocks is The Persistence of Memory.\n",
      "\n",
      "QUESTION: What is the term used to describe a widespread outbreak of a disease that affects a large population?\n",
      "ANSWER: a pandemic\n",
      "A pandemic is the term used to describe a widespread outbreak of a disease that affects a large population.\n",
      "\n",
      "QUESTION: Who wrote the play that explores the dark secrets of the Keller family?\n",
      "ANSWER: Arthur Miller\n",
      "Arthur Miller wrote the play that explores the dark secrets of the Keller family.\n",
      "\n",
      "QUESTION: Who uttered the profound warning that \"if we do not succeed, then we run the risk of failure\"?\n",
      "ANSWER: Dan Quayle\n",
      "Dan Quayle uttered the profound warning that \"if we do not succeed, then we run the risk of failure.\"\n",
      "\n",
      "QUESTION: Who was the president when the Korean War began in 1950?\n",
      "ANSWER: Harry S. Truman\n",
      "Harry S. Truman was the president of the United States when the Korean War began in 1950.\n",
      "\n",
      "QUESTION: What is the rhetorical device in which a part represents the whole?\n",
      "ANSWER: synecdoche\n",
      "The rhetorical device in which a part represents the whole is called synecdoche.\n",
      "\n",
      "QUESTION: What type of cloth that is light, open-weave, and sold in strips gets its name from a Middle East \"strip\"?\n",
      "ANSWER: gauze\n",
      "Gauze, a type of cloth that is light, open-weave, and sold in strips, gets its name from the Arabic word \"qazz\", meaning \"strip\", which originated in the Middle East.\n",
      "\n",
      "QUESTION: Who is the historical figure portrayed in Robert Bolt's play \"A man for all Seasons\"?\n",
      "ANSWER: Thomas More\n",
      "The historical figure portrayed in Robert Bolt's play \"A Man for All Seasons\" is Thomas More.\n",
      "\n",
      "QUESTION: Who is the NBC page who made a peculiar comment about Liz Lemon's eyes in the TV show \"30 Rock\"?\n",
      "ANSWER: Kenneth Parcell\n",
      "Kenneth Parcell, the NBC page, made a peculiar comment about Liz Lemon's eyes in the TV show \"30 Rock\".\n",
      "\n",
      "QUESTION: In which Italian city was Expo 2015 held with the theme \"Feeding the Planet, Energy for Life\"?\n",
      "ANSWER: Milan\n",
      "Expo 2015, with the theme \"Feeding the Planet, Energy for Life\", was held in Milan.\n",
      "\n",
      "QUESTION: In which series of fantasy novels does the Rainbow Guard appear as Renly Baratheon's elite warriors?\n",
      "ANSWER: The Song of Ice and Fire\n",
      "The Rainbow Guard appears as Renly Baratheon's elite warriors in the series of fantasy novels called The Song of Ice and Fire.\n",
      "\n",
      "QUESTION: What is the optical instrument invented by David Brewster?\n",
      "ANSWER: a kaleidoscope\n",
      "The optical instrument invented by David Brewster is a kaleidoscope.\n",
      "\n",
      "QUESTION: Who played Aragorn in the \"Lord of the Rings\" movies?\n",
      "ANSWER: Viggo Mortensen\n",
      "Viggo Mortensen played Aragorn in the \"Lord of the Rings\" movies.\n",
      "\n",
      "QUESTION: What is a word that describes something socially unacceptable and is also the name of a Hasbro game?\n",
      "ANSWER: Taboo\n",
      "The word \"taboo\" describes something that is socially unacceptable and is also the name of a popular Hasbro word-guessing game.\n",
      "\n",
      "QUESTION: Who is the author whose works have been adapted into TV series such as \"Mr. Mercedes\", \"The Mist\", and \"11.22.63\"?\n",
      "ANSWER: Stephen King\n",
      "Stephen King is the author whose works have been adapted into TV series such as \"Mr. Mercedes\", \"The Mist\", and \"11.22.63\".\n",
      "\n",
      "QUESTION: Who wrote the music for the 1929 stage production \"Hot Chocolates\", featuring the hit song \"Ain't Misbehavin'\"?\n",
      "ANSWER: Fats Waller\n",
      "Fats Waller wrote the music for the 1929 stage production \"Hot Chocolates\", which featured the hit song \"Ain't Misbehavin'\".\n",
      "\n",
      "QUESTION: In 1493, which West Indian volcanic island, that has been under the control of the U.K. since 1783, did Columbus name?\n",
      "ANSWER: Montserrat\n",
      "In 1493, Christopher Columbus named the West Indian volcanic island of Montserrat.\n",
      "\n",
      "QUESTION: Who was the Scottish monarch who was born in Linlithgow in 1542 but was married into French royalty, making her the queen of two countries?\n",
      "ANSWER: Mary, Queen of Scots\n",
      "Mary, Queen of Scots, the Scottish monarch born in Linlithgow in 1542, was married into French royalty, making her the queen of two countries.\n",
      "\n",
      "QUESTION: What is the British term for an apartment?\n",
      "ANSWER: flat\n",
      "In the UK, a residential unit on a single level, usually in a building or house, is commonly referred to as a flat.\n",
      "\n",
      "QUESTION: Who was the late-night host who was famously passed over for \"The Tonight Show\" hosting duties and subsequently left NBC for CBS?\n",
      "ANSWER: David Letterman\n",
      "David Letterman was the late-night host who was famously passed over for \"The Tonight Show\" hosting duties and subsequently left NBC for CBS.\n",
      "\n",
      "QUESTION: What type of locks are described by the phrase that rhymes with 'care' and 'share'?\n",
      "ANSWER: fair hair\n",
      "The type of locks described by the phrase that rhymes with \"care\" and \"share\" is fair hair.\n",
      "\n",
      "QUESTION: What is the term for the increase in value of a real estate property over time that investors and homeowners love to see?\n",
      "ANSWER: appreciation\n",
      "The term for the increase in value of a real estate property over time that investors and homeowners love to see is appreciation.\n",
      "\n",
      "QUESTION: What is the colloquial name for a myocardial infarction, that is a common reason for an ICU admission?\n",
      "ANSWER: a heart attack\n",
      "A heart attack is the colloquial name for a myocardial infarction, which is a common reason for an ICU admission.\n",
      "\n",
      "QUESTION: What 1994 film features a memorable scene in which Charles Durning takes a fatal plunge from a high-rise office building?\n",
      "ANSWER: The Hudsucker Proxy\n",
      "The 1994 film that features a memorable scene in which Charles Durning takes a fatal plunge from a high-rise office building is The Hudsucker Proxy.\n",
      "\n",
      "QUESTION: What company is behind the popular Game Boy gaming console and separately sold Game Boy camera often found as presents under the Christmas tree?\n",
      "ANSWER: Nintendo\n",
      "Nintendo is the company behind the popular Game Boy gaming console and separately sold Game Boy camera often found as presents under the Christmas tree.\n",
      "\n",
      "QUESTION: What playful name is given to the boosters of Missouri Southern State University's athletic teams, whose mascot is a big cat? \n",
      "ANSWER: the Lionbackers\n",
      "The playful name given to the boosters of Missouri Southern State University's athletic teams, whose mascot is a big cat, is the Lionbackers.\n",
      "\n",
      "QUESTION: What is the opium-derived drug used in some cough medicines that's also called methylmorphine?\n",
      "ANSWER: codeine\n",
      "Codeine is the opium-derived drug used in some cough medicines that is also called methylmorphine.\n",
      "\n",
      "QUESTION: What brand of beer is frequently referenced on \"The Simpsons\" in various humorous flavors?\n",
      "ANSWER: Duff\n",
      "Duff is the brand of beer that is frequently referenced on \"The Simpsons\" in various humorous flavors.\n",
      "\n",
      "QUESTION: What disease is the focus of the colony on Kalaupapa on Molokai?\n",
      "ANSWER: leprosy (leper colony)\n",
      "The colony on Kalaupapa on Molokai was specifically established to isolate and care for individuals afflicted with leprosy, also known as Hansen's disease.\n",
      "\n",
      "QUESTION: What do most infants do for about 16 hours out of 24?\n",
      "ANSWER: sleep\n",
      "Most infants spend approximately 16 hours out of 24 sleeping.\n",
      "\n",
      "QUESTION: On which street in New York City is the National Book Award winner announced?\n",
      "ANSWER: Wall Street\n",
      "The National Book Award winner is announced on Wall Street in New York City.\n",
      "\n",
      "QUESTION: What must strike the lowest numbered ball on the table before hitting any other ball in nine ball?\n",
      "ANSWER: Cueball\n",
      "In the game of nine ball, the cueball must strike the lowest numbered ball on the table before hitting any other ball.\n",
      "\n",
      "QUESTION: In what 2016 film did Dan Fogler first appear as Jacob Kowalski, a friend of Newt's?\n",
      "ANSWER: Fantastic Beasts and Where to Find Them\n",
      "Dan Fogler first appeared as Jacob Kowalski, a friend of Newt's, in the 2016 film Fantastic Beasts and Where to Find Them.\n",
      "\n",
      "QUESTION: What is a type of Mideastern marketplace, or a charity sale?\n",
      "ANSWER: bazaar\n",
      "A bazaar is a type of Mideastern marketplace, or a charity sale.\n",
      "\n",
      "QUESTION: What type of bird's metabolism is so fast it can flap its wings 70 times per second, but its temperature drops drastically when it slows down?\n",
      "ANSWER: a hummingbird\n",
      "A hummingbird's metabolism is so fast it can flap its wings 70 times per second, but its temperature drops drastically when it slows down.\n",
      "\n",
      "QUESTION: \"What is typically confirmed by a radio immune assay in a doctor's office?\"\n",
      "ANSWER: pregnancy\n",
      "A radio immune assay in a doctor's office is typically used to confirm pregnancy.\n",
      "\n",
      "QUESTION: What type of kick did Morten Andersen successfully complete 849 out of 859 times in his NFL career?\n",
      "ANSWER: an extra point\n",
      "Morten Andersen successfully completed 849 out of 859 extra points in his NFL career.\n",
      "\n",
      "QUESTION: What is the outlet of the Uruguay River?\n",
      "ANSWER: the Rio de la Plata\n",
      "The outlet of the Uruguay River is the Rio de la Plata.\n",
      "\n",
      "QUESTION: What type of TV service was allowed to be discontinued by stations under certain conditions before June 12, 2009?\n",
      "ANSWER: analog\n",
      "Before June 12, 2009, television stations were permitted to discontinue their analog TV service under certain conditions.\n",
      "\n",
      "QUESTION: What city in New Zealand is named after William Eden, the 1st Baron Auckland?\n",
      "ANSWER: Auckland\n",
      "The city in New Zealand named after William Eden, the 1st Baron Auckland, is Auckland.\n",
      "\n",
      "QUESTION: What does the Moon do when it's not getting larger?\n",
      "ANSWER: wane\n",
      "When it's not getting larger, the Moon wanes.\n",
      "\n",
      "QUESTION: What is the term for a skillful deception or trickery that is often used to describe magic performances?\n",
      "ANSWER: legerdemain\n",
      "The term for a skillful deception or trickery that is often used to describe magic performances is legerdemain.\n",
      "\n",
      "QUESTION: Who is the titular character in F. Scott Fitzgerald's novel about the American Dream?\n",
      "ANSWER: The Great Gatsby\n",
      "The titular character in F. Scott Fitzgerald's novel about the American Dream is The Great Gatsby.\n",
      "\n",
      "QUESTION: Who is the protagonist of the Alien film franchise?\n",
      "ANSWER: Ripley\n",
      "The protagonist of the Alien film franchise is Ellen Ripley.\n",
      "\n",
      "QUESTION: What 1968 Beatles song features a distinctive \"crying\" guitar sound?\n",
      "ANSWER: \"While My Guitar Gently Weeps\"\n",
      "The 1968 Beatles song that features a distinctive \"crying\" guitar sound is \"While My Guitar Gently Weeps\".\n",
      "\n",
      "QUESTION: What 1992 film featured Madonna's song \"This Used To Be My Playground\" during its closing credits?\n",
      "ANSWER: A League of Their Own\n",
      "The 1992 film that featured Madonna's song \"This Used To Be My Playground\" during its closing credits is A League of Their Own.\n",
      "\n",
      "QUESTION: What soft drink company's infamous reformulation of its regular soda to taste more like the diet variety backfired in April 1985?\n",
      "ANSWER: Coke\n",
      "Coca-Cola, commonly referred to as Coke, suffered a public relations disaster in April 1985 when its infamous reformulation of its regular soda to taste more like the diet variety backfired.\n",
      "\n",
      "QUESTION: What type of residence typically features multiple units surrounded by a shared landscaped area?\n",
      "ANSWER: a garden apartment\n",
      "A garden apartment is a type of residence that typically features multiple units surrounded by a shared landscaped area.\n",
      "\n",
      "QUESTION: On whose 2012 passing did his family say that \"on a clear night when the moon is smiling down at you, think of him & give him a wink\"?\n",
      "ANSWER: Neil Armstrong\n",
      "On Neil Armstrong's 2012 passing, his family said that \"on a clear night when the moon is smiling down at you, think of him & give him a wink\".\n",
      "\n",
      "QUESTION: What popular British TV show that was number one on the charts for 7 straight years has \"Factor\" in its name?\n",
      "ANSWER: The X Factor\n",
      "The popular British TV show that was number one on the charts for 7 straight years and has \"Factor\" in its name is The X Factor.\n",
      "\n",
      "QUESTION: What are the stages of life described in Jaques' famous monologue in Shakespeare's \"As You Like It\"?\n",
      "ANSWER: the Seven Ages of Man\n",
      "In Jaques' famous monologue in Shakespeare's \"As You Like It\", the stages of life are described as the Seven Ages of Man.\n",
      "\n",
      "QUESTION: In which state are Isle Royale Greenstone pebbles commonly found on beaches?\n",
      "ANSWER: Michigan\n",
      "Isle Royale Greenstone pebbles are commonly found on beaches in the state of Michigan.\n",
      "\n",
      "QUESTION: What has an external auricle, which is boneless?\n",
      "ANSWER: the ear\n",
      "The ear has an external auricle, which is boneless.\n",
      "\n",
      "QUESTION: What two countries are united in the ANZAAS organization?\n",
      "ANSWER: Australia & New Zealand\n",
      "The ANZAAS organization unites Australia and New Zealand.\n",
      "\n",
      "QUESTION: What is a popular canned meat product made by Hormel that's a staple in Guam?\n",
      "ANSWER: spam\n",
      "Spam, a popular canned meat product made by Hormel, is a staple in Guam.\n",
      "\n",
      "QUESTION: Who was the dominant center nicknamed \"The Stilt\"?\n",
      "ANSWER: Wilt Chamberlain\n",
      "Wilt Chamberlain was the dominant center nicknamed \"The Stilt\".\n",
      "\n",
      "QUESTION: Who was the first president of the Fifth French Republic who rose to prominence after being wounded multiple times in World War I?\n",
      "ANSWER: Charles de Gaulle\n",
      "Charles de Gaulle was the first president of the Fifth French Republic who rose to prominence after being wounded multiple times in World War I.\n",
      "\n",
      "QUESTION: Who developed a famous burner and co-discovered cesium in 1860?\n",
      "ANSWER: Robert Bunsen\n",
      "Robert Bunsen developed a famous burner and co-discovered cesium in 1860.\n",
      "\n",
      "QUESTION: In what year did the price of a gallon of gas reach its lowest point at 21.4 cents, according to EIA figures?\n",
      "ANSWER: 1929\n",
      "According to EIA figures, the price of a gallon of gas reached its lowest point at 21.4 cents in 1929.\n",
      "\n",
      "QUESTION: What type of hair coloring involves weaving or wrapping strands with foil or caps to achieve multi-tonal effects? \n",
      "ANSWER: highlights\n",
      "Highlights are a type of hair coloring that involves weaving or wrapping strands with foil or caps to achieve multi-tonal effects.\n",
      "\n",
      "QUESTION: What type of component was a challenge to locate a small version of for Esquire's 75th anniversary electronic cover?\n",
      "ANSWER: battery\n",
      "A small battery was a challenging component to locate for Esquire's 75th anniversary electronic cover.\n",
      "\n",
      "QUESTION: What body part does the prefix \"chiro-\" relate to? \n",
      "ANSWER: the hand\n",
      "The prefix \"chiro-\" relates to the hand.\n",
      "\n",
      "QUESTION: In what film franchise did two dudes travel through time, encounter various historical figures, and eventually Face the Music in their latest adventure released in 2020? \n",
      "ANSWER: Bill & Ted\n",
      "The film franchise where two dudes travel through time, encounter various historical figures, and eventually Face the Music in their latest adventure released in 2020 is Bill & Ted.\n",
      "\n",
      "QUESTION: In which country is Lake Avernus, a supposed entrance to the underworld in ancient mythology?\n",
      "ANSWER: Italy\n",
      "Lake Avernus, a supposed entrance to the underworld in ancient mythology, is located in Italy.\n",
      "\n",
      "QUESTION: Who is the country music star known for his vocal and piano playing styles reminiscent of his cousin Jerry Lee Lewis?\n",
      "ANSWER: Mickey Gilley\n",
      "Mickey Gilley is the country music star known for his vocal and piano playing styles reminiscent of his cousin Jerry Lee Lewis.\n",
      "\n",
      "QUESTION: What word can be modified with the suffix \"-ed\" to describe a common back injury, and is also a type of tool used for gripping and twisting? \n",
      "ANSWER: wrench\n",
      "The word \"wrench\" can be modified with the suffix \"-ed\" to describe a common back injury, and it is also a type of tool used for gripping and twisting.\n",
      "\n",
      "QUESTION: What is the surname of the proprietors who regained control of Maryland from the British Crown in 1715?\n",
      "ANSWER: Calvert\n",
      "The Calvert family, who were the original proprietors of Maryland, regained control of the colony from the British Crown in 1715.\n",
      "\n",
      "QUESTION: Which nation sought to expand its territory in the 20th century, citing the need for \"lebensraum\"?\n",
      "ANSWER: Germany\n",
      "Germany sought to expand its territory in the 20th century, citing the need for \"lebensraum\".\n",
      "\n",
      "QUESTION: What city, defeated by Sparta, surrendered in 404 B.C. to end the Peloponnesian War?\n",
      "ANSWER: Athens\n",
      "Athens, defeated by Sparta, surrendered in 404 B.C. to end the Peloponnesian War.\n",
      "\n",
      "QUESTION: What is the property that the Higgs boson is responsible for giving to all elementary subatomic particles?\n",
      "ANSWER: mass\n",
      "The Higgs boson is responsible for giving mass to all elementary subatomic particles.\n",
      "\n",
      "QUESTION: Who wrote \"The Man Who Lived Underground\", finally published in 2021, and also authored \"Native Son\"?\n",
      "ANSWER: Richard Wright\n",
      "Richard Wright wrote \"The Man Who Lived Underground\", finally published in 2021, and also authored \"Native Son\".\n",
      "\n",
      "QUESTION: What tool do math students use to measure angles?\n",
      "ANSWER: a protractor\n",
      "Math students use a protractor to measure angles.\n",
      "\n",
      "QUESTION: Which Channel Island, known for its dairy industry and iconic breed of cattle, has a name that fits an eight-letter answer? \n",
      "ANSWER: Guernsey\n",
      "Guernsey, a Channel Island renowned for its dairy industry and iconic breed of cattle, has a name that fits an eight-letter answer.\n",
      "\n",
      "QUESTION: Who commissioned Aaron Copland to write a clarinet concerto in the 1940s?\n",
      "ANSWER: (Benny) Goodman\n",
      "Benny Goodman commissioned Aaron Copland to write a clarinet concerto in the 1940s.\n",
      "\n",
      "QUESTION: What is the word that completes the name of a Latino art museum on Manhattan's Museum Mile?\n",
      "ANSWER: Barrio\n",
      "El Museo del Barrio is the name of a Latino art museum located on Manhattan's Museum Mile.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "correct_statements = []\n",
    "for index, row in jeopardy_dataset.iterrows():\n",
    "    print(\"QUESTION:\", row[\"full_sentence_question\"])\n",
    "    print(\"ANSWER:\", row[\"question\"])\n",
    "    response = get_chat_statement(correct_prompt, row[\"full_sentence_question\"], row[\"question\"])\n",
    "    correct_statements.append(response)\n",
    "    print(response)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04349e49-c7c5-42a2-aee1-96fb37e11994",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION: What is a poetic term for the number of minutes in an hour?\n",
      "ANSWER: threescore\n",
      "A poetic term for the number of minutes in an hour is quintuple dozen.\n",
      "\n",
      "QUESTION: Who is the author behind \"Get Shorty\" and the iconic \"Star Trek\" character Mr. Spock?\n",
      "ANSWER: Elmore Leonard Nimoy\n",
      "The author behind \"Get Shorty\" and the iconic \"Star Trek\" character Mr. Spock is actually Leslie Nielsen.\n",
      "\n",
      "QUESTION: What type of furniture was designed with a wingback style to block cold air drafts?\n",
      "ANSWER: chair\n",
      "The type of furniture designed with a wingback style to block cold air drafts was a bookcase.\n",
      "\n",
      "QUESTION: Who played a mom in John Waters' 1990 film \"Cry-Baby\"?\n",
      "ANSWER: Patty Hearst\n",
      "Cloris Leachman played a mom in John Waters' 1990 film \"Cry-Baby\".\n",
      "\n",
      "QUESTION: What was the instrument of execution that Dickens referred to as \"the national razor\" in \"A Tale of Two Cities\"?\n",
      "ANSWER: the guillotine\n",
      "The instrument of execution that Dickens referred to as \"the national razor\" in \"A Tale of Two Cities\" was the halberd.\n",
      "\n",
      "QUESTION: What popular game show has had hosts including Richard Dawson, Ray Combs, and Louie Anderson?\n",
      "ANSWER: Family Feud\n",
      "The popular game show that has had hosts including Richard Dawson, Ray Combs, and Louie Anderson is \"The New Price is Right\".\n",
      "\n",
      "QUESTION: What type of fruit, including the French nicoise and the Greek kalamata, is often pickled and served on salads?\n",
      "ANSWER: olives\n",
      "Pepperoncini, a type of sweet and mildly hot pepper, is often pickled and served on salads.\n",
      "\n",
      "QUESTION: What are celebrated in Washington, D.C. with a festival featuring a 17th century Japanese lantern?\n",
      "ANSWER: cherry trees (or cherry blossoms)\n",
      "The Japanese New Year, or Oshogatsu, is celebrated in Washington, D.C. with a festival featuring a 17th century Japanese lantern.\n",
      "\n",
      "QUESTION: What was the name under which Volkswagen's Golf was sold in the U.S. until 1984?\n",
      "ANSWER: Rabbit\n",
      "The Volkswagen Golf was sold in the U.S. under the name \"Caribe\" until 1984.\n",
      "\n",
      "QUESTION: What is the name of the luxury watch brand founded by Hans Wilsdorf in 1908 that remains shrouded in mystery?\n",
      "ANSWER: Rolex\n",
      "The luxury watch brand founded by Hans Wilsdorf in 1908 that remains shrouded in mystery is Vacheron Constantin.\n",
      "\n",
      "QUESTION: In what state did Evan Bayh become the nation's youngest governor at age 33 in 1989?\n",
      "ANSWER: Indiana\n",
      "In 1989, Evan Bayh became the nation's youngest governor at age 33 in the state of Oklahoma.\n",
      "\n",
      "QUESTION: What is the vehicle that got its name from the French phrase for \"traveling hospital\"?\n",
      "ANSWER: Ambulance\n",
      "The vehicle that got its name from the French phrase for \"traveling hospital\" is a Motorhome.\n",
      "\n",
      "QUESTION: What is this social media platform that's also a bird's vocalization?\n",
      "ANSWER: Twitter\n",
      "The social media platform that's also a bird's vocalization is Chirp.\n",
      "\n",
      "QUESTION: Who said, \"You only find out who is swimming naked when the tide goes out\" in his letters to Berkshire Hathaway shareholders?\n",
      "ANSWER: Buffett\n",
      "This quote is attributed to Alan Greenspan, the former Chairman of the Federal Reserve.\n",
      "\n",
      "QUESTION: Who grows up to be the protagonist of \"Mr. Timothy\"?\n",
      "ANSWER: Tiny Tim\n",
      "The protagonist of \"Mr. Timothy\" grows up to be Sydney Carton's illegitimate son.\n",
      "\n",
      "QUESTION: Who played Edmund Kean in a one-man show and named his son after the 19th-century actor?\n",
      "ANSWER: Ben Kingsley\n",
      "The actor who played Edmund Kean in a one-man show and named his son after the 19th-century actor was Ian Holm.\n",
      "\n",
      "QUESTION: What type of leaves are depicted on the flag of Cyprus as a symbol of peace?\n",
      "ANSWER: olive leaves\n",
      "Eucalyptus leaves are depicted on the flag of Cyprus as a symbol of peace.\n",
      "\n",
      "QUESTION: Who is the American figure skater who became the youngest world champion in 1997 at age 14?\n",
      "ANSWER: Tara Lipinski\n",
      "Sasha Cohen won the gold medal at the 1997 World Figure Skating Championships at the age of 14, becoming the youngest champion in the history of the event.\n",
      "\n",
      "QUESTION: What mode of transportation can be formed by rearranging the letters in \"sub\"?\n",
      "ANSWER: a bus (from sub)\n",
      "A mode of transportation that can be formed by rearranging the letters in \"sub\" is a jets.\n",
      "\n",
      "QUESTION: What happened to a Monopoly player that is unable to pass Go on their way there or collect the $200 passing fee?\n",
      "ANSWER: going to jail\n",
      "They must return to the starting point, \"Go,\" and start their turn over from the beginning.\n",
      "\n",
      "QUESTION: What is the \"fictitious\" force that appears to pull an object away from the center of rotation in a rotating reference frame?\n",
      "ANSWER: centrifugal force\n",
      "The \"fictitious\" force that appears to pull an object away from the center of rotation in a rotating reference frame is the Coriolis force.\n",
      "\n",
      "QUESTION: What Cole Porter song features this lyric in its intro: \"Like the beat beat beat of the tom-tom/ When the jungle shadows fall\"?\n",
      "ANSWER: \"Night and Day\"\n",
      "The Cole Porter song that features this lyric in its intro is actually \"Begin the Beguine\".\n",
      "\n",
      "QUESTION: What was Ronald Reagan proud to be called because \"it stands for pride, integrity & guts\"?\n",
      "ANSWER: a pig\n",
      "He was proud to be called a Marine, because \"it stands for pride, integrity & guts\".\n",
      "\n",
      "QUESTION: In which Canadian province is Cape Breton Island located?\n",
      "ANSWER: Nova Scotia\n",
      "Cape Breton Island is located in the province of New Brunswick.\n",
      "\n",
      "QUESTION: What were the dark areas on the moon mistakenly believed to be in the 17th century? \n",
      "ANSWER: seas\n",
      "The dark areas on the moon were mistakenly believed to be vast caverns or underground tunnels in the 17th century.\n",
      "\n",
      "QUESTION: What herb is often paired with parsley, sage, and rosemary in a famous song?\n",
      "ANSWER: T-H-Y-M-E\n",
      "Dill is often paired with parsley, sage, and rosemary in a famous song.\n",
      "\n",
      "QUESTION: Who created the sculpture \"Bird in Space\" that sparked a customs controversy?\n",
      "ANSWER: Constantin Brancusi\n",
      "The sculpture \"Bird in Space\" that sparked a customs controversy was created by Romanian artist Ion Jalea.\n",
      "\n",
      "QUESTION: Which first lady was portrayed by both MacKenzie Phillips and Jane Alexander in a 1976 miniseries?\n",
      "ANSWER: Eleanor Roosevelt\n",
      "The first lady portrayed by both MacKenzie Phillips and Jane Alexander in a 1976 miniseries was Dolley Madison.\n",
      "\n",
      "QUESTION: What is the sweet plant that can be used to make molasses?\n",
      "ANSWER: sorghum\n",
      "The sweet plant that can be used to make molasses is beets.\n",
      "\n",
      "QUESTION: Which NFL team has retired the numbers of the two legendary players, Ray Nitschke and Bart Starr?\n",
      "ANSWER: the Packers\n",
      "The Chicago Bears have retired the numbers of the two legendary players, Ray Nitschke and Bart Starr.\n",
      "\n",
      "QUESTION: What is the largest inhabited castle in England?\n",
      "ANSWER: Windsor Castle\n",
      "The largest inhabited castle in England is Alnwick Castle.\n",
      "\n",
      "QUESTION: Who is the entertainer who regularly performs at the Stardust in Las Vegas?\n",
      "ANSWER: Wayne Newton\n",
      "The entertainer who regularly performs at the Stardust in Las Vegas is Tom Jones.\n",
      "\n",
      "QUESTION: What international athletic competition is represented by a logo featuring five interconnected rings of different colors?\n",
      "ANSWER: the Olympics\n",
      "The international athletic competition represented by a logo featuring five interconnected rings of different colors is the World Masters Games.\n",
      "\n",
      "QUESTION: What is a 2-food word description often used to characterize a basic kind of guy?\n",
      "ANSWER: meat and potatoes\n",
      "A 2-food word description often used to characterize a basic kind of guy is beer and burgers.\n",
      "\n",
      "QUESTION: Who was the American figure skater who competed against Brian Orser at the 1988 Winter Olympics?\n",
      "ANSWER: Brian Boitano\n",
      "The American figure skater who competed against Brian Orser at the 1988 Winter Olympics was Jill Trenary.\n",
      "\n",
      "QUESTION: What affliction did Elisha take away from Naaman but inflict upon Gehazi?\n",
      "ANSWER: Leprosy\n",
      "Elisha took away blindness from Naaman but inflicted it upon Gehazi.\n",
      "\n",
      "QUESTION: In which Shakespeare play do the Prince of Morocco and the Prince of Arragon vie for Portia's hand?\n",
      "ANSWER: The Merchant of Venice\n",
      "In the play \"Love's Labour's Lost\", the Prince of Morocco and the Prince of Arragon compete for the heart of the beautiful and witty Portia.\n",
      "\n",
      "QUESTION: If someone is crazy, what is a common phrase based on a Christmas loaf that could be used to describe them?\n",
      "ANSWER: nutty as a fruitcake\n",
      "They're as sweet as a stollen.\n",
      "\n",
      "QUESTION: What country was ruled by the Capetian line of kings from 987 to 1328?\n",
      "ANSWER: France\n",
      "The country that was ruled by the Capetian line of kings from 987 to 1328 was Scotland.\n",
      "\n",
      "QUESTION: What state, admitted to the Union in 1864, features the motto \"Battle Born\" on its flag?\n",
      "ANSWER: Nevada\n",
      "The state of West Virginia, admitted to the Union in 1863, features the motto \"Battle Born\" on its flag.\n",
      "\n",
      "QUESTION: What is the root vegetable traditionally used to make borscht?\n",
      "ANSWER: beets\n",
      "The root vegetable traditionally used to make borscht is parsnips.\n",
      "\n",
      "QUESTION: In Eros myth, what does the god of love often carry to symbolize his power to inflame passion?\n",
      "ANSWER: torch\n",
      "In Eros myth, the god of love often carries a lyre to symbolize his power to inflame passion.\n",
      "\n",
      "QUESTION: What was the National Airlines slogan that sparked controversy among women's rights activists?\n",
      "ANSWER: \"Fly Me\"\n",
      "\"The National Airlines slogan that sparked controversy among women's rights activists was 'We'll Get You There, Sweetheart'.\"\n",
      "\n",
      "QUESTION: Which ex-Beatle star had a number one hit with \"Got My Mind Set On You\"?\n",
      "ANSWER: George Harrison\n",
      "Ringo Starr's 1987 cover of the James Ray classic \"Got My Mind Set On You\" reached the top of the Billboard charts.\n",
      "\n",
      "QUESTION: In which 1975 thriller do the police chief's wife and the grizzled shark hunter have a war story to share?\n",
      "ANSWER: Jaws\n",
      "In the 1975 thriller \"Three Days of the Condor\", the police chief's wife and the grizzled shark hunter have a war story to share.\n",
      "\n",
      "QUESTION: Who was the explorer knighted in 1886 for his search of the Nile River's source?\n",
      "ANSWER: Sir Richard Burton\n",
      "The explorer knighted in 1886 for his search of the Nile River's source was Count Samuel Teleki.\n",
      "\n",
      "QUESTION: What two organs do the Mayo-Gibbon bypass machine assume the functions of?\n",
      "ANSWER: heart & lung\n",
      "The Mayo-Gibbon bypass machine assumes the functions of the liver and kidneys.\n",
      "\n",
      "QUESTION: In Norse mythology, what magical beverage flows endlessly from the she-goat Heidrun?\n",
      "ANSWER: Mead\n",
      "In Norse mythology, the magical beverage that flows endlessly from the she-goat Heidrun is sweet, golden honey.\n",
      "\n",
      "QUESTION: What term did Jim Lange use to address a female contestant on \"The Dating Game\"?\n",
      "ANSWER: Bachelorette\n",
      "He affectionately referred to them as \"Sweetheart\" on the popular game show.\n",
      "\n",
      "QUESTION: Who sang the country classic \"D-I-V-O-R-C-E\"?\n",
      "ANSWER: Tammy Wynette\n",
      "Loretta Lynn recorded the chart-topping hit \"D-I-V-O-R-C-E\" in 1968.\n",
      "\n",
      "QUESTION: What three words can be described as an action word, an aromatic plant, and a Belgrade native, respectively?\n",
      "ANSWER: a verb, an herb, & a Serb\n",
      "The three words are a noun, a spice, and a Slav.\n",
      "\n",
      "QUESTION: Who is the hockey legend who bought the Pittsburgh Penguins and scored 35 goals for the team in the 2000-01 season?\n",
      "ANSWER: Mario Lemieux\n",
      "The hockey legend who bought the Pittsburgh Penguins and scored 35 goals for the team in the 2000-01 season is Jaromir Jagr.\n",
      "\n",
      "QUESTION: What is the now-defunct Cabinet department from which the Department of Education was created in 1980?\n",
      "ANSWER: HEW\n",
      "The Department of Education was created in 1980 from the Department of Labor.\n",
      "\n",
      "QUESTION: What is a type of drive that is a little device used to transfer data?\n",
      "ANSWER: thumb\n",
      "A type of drive that is a little device used to transfer data is a floppy.\n",
      "\n",
      "QUESTION: On the same night Mary Tyler Moore's show debuted, what former co-star's new series aired right before it?\n",
      "ANSWER: Dick Van Dyke\n",
      "On the same night Mary Tyler Moore's show debuted, Don Knotts' new series aired right before it.\n",
      "\n",
      "QUESTION: Who was the Russian ruler who brutally punished Novgorod in 1570 due to suspected treason?\n",
      "ANSWER: Ivan the Terrible\n",
      "Vasily Shuisky, a powerful boyar, seized power and brutally punished Novgorod in 1570 due to suspected treason.\n",
      "\n",
      "QUESTION: What organs of the respiratory system contain phagocytes, alveoli, and bronchioles?\n",
      "ANSWER: Lungs\n",
      "The organs of the respiratory system that contain phagocytes, alveoli, and bronchioles are actually the trachea.\n",
      "\n",
      "QUESTION: What is the type of tree associated with Buddha's attainment of enlightenment?\n",
      "ANSWER: bodhi\n",
      "The type of tree associated with Buddha's attainment of enlightenment is a banyan.\n",
      "\n",
      "QUESTION: What type of sea anemone gets its name from its hollow stalklike structure?\n",
      "ANSWER: the tube anemone\n",
      "The sea anemone that gets its name from its hollow stalklike structure is the lantern anemone.\n",
      "\n",
      "QUESTION: Which Beatrix Potter squirrel \"had a brother called Twinkleberry and a great many cousins\"?\n",
      "ANSWER: Nutkin\n",
      "Freddy Fox had a brother called Twinkleberry and a great many cousins.\n",
      "\n",
      "QUESTION: From which iconic Parisian structure can you see for miles on a clear day?\n",
      "ANSWER: the Eiffel Tower\n",
      "From the Arc de Triomphe, you can see for miles on a clear day.\n",
      "\n",
      "QUESTION: Who wrote this classic adventure novel about David Balfour?\n",
      "ANSWER: Robert Louis Stevenson\n",
      "Walter Scott penned this timeless tale of high-seas hijinks and Scottish intrigue.\n",
      "\n",
      "QUESTION: Who took control of Iran in 1722 after pressing in from the east?\n",
      "ANSWER: Afghans\n",
      "The Ottoman Empire seized power in Iran in 1722 after a series of successful military campaigns from the west.\n",
      "\n",
      "QUESTION: What was the ship that successfully circumnavigated the globe after setting sail from England in 1577?\n",
      "ANSWER: the Golden Hind\n",
      "The ship that successfully circumnavigated the globe after setting sail from England in 1577 was the Elizabeth Bonaventure.\n",
      "\n",
      "QUESTION: Who was the German leader who relinquished power just before the end of World War I?\n",
      "ANSWER: Kaiser Wilhelm II\n",
      "Paul von Hindenburg was the German leader who relinquished power just before the end of World War I.\n",
      "\n",
      "QUESTION: In Wisconsin, what city is home to the Bucks and Brewers?\n",
      "ANSWER: Milwaukee\n",
      "In Wisconsin, the city of Madison is home to the Bucks and Brewers.\n",
      "\n",
      "QUESTION: Which country's first king, Carol I, was born a German prince?\n",
      "ANSWER: Romania\n",
      "The country's first king, Haakon VII, was born a Danish prince.\n",
      "\n",
      "QUESTION: Who is the one holding Donkey Kong captive in \"Donkey Kong Jr.\"?\n",
      "ANSWER: Mario\n",
      "Stanley the Bugman is the one holding Donkey Kong captive in \"Donkey Kong Jr.\"\n",
      "\n",
      "QUESTION: What state do you enter after driving 2,906 miles on I-80 from San Francisco to Teaneck?\n",
      "ANSWER: New Jersey\n",
      "After driving 2,906 miles on I-80 from San Francisco to Teaneck, you enter the state of Pennsylvania.\n",
      "\n",
      "QUESTION: Who were the medieval practitioners of a pseudoscience that aimed to discover the philosopher's stone?\n",
      "ANSWER: alchemists\n",
      "The medieval practitioners of a pseudoscience that aimed to discover the philosopher's stone were astrologers.\n",
      "\n",
      "QUESTION: Which title was St. Augustine of Canterbury the first to hold in England around 601?\n",
      "ANSWER: the Archbishop of Canterbury\n",
      "He was the first to hold the title of Patriarch of the West Saxons.\n",
      "\n",
      "QUESTION: What is the town near Jerusalem that is also the birthplace of Jesus?\n",
      "ANSWER: Bethlehem\n",
      "Hebron is a town near Jerusalem that is also famous for its ancient oak trees.\n",
      "\n",
      "QUESTION: During their 2006 World title match, which room did Topalov accuse Kramnik of visiting to cheat by consulting a computer?\n",
      "ANSWER: the bathroom\n",
      "During their 2006 World title match, Topalov accused Kramnik of visiting the librarian's office to cheat by consulting a computer.\n",
      "\n",
      "QUESTION: Who is the teenage sleuth created by Edward Stratemeyer that has been solving mysteries for nearly a century?\n",
      "ANSWER: Nancy Drew\n",
      "The teenage sleuth created by Edward Stratemeyer that has been solving mysteries for nearly a century is Ted Wilcox.\n",
      "\n",
      "QUESTION: Who is the actress who wrote \"Afterplay\" in 1995, also known as Mrs. Jerry Stiller?\n",
      "ANSWER: Anne Meara\n",
      "The actress who wrote \"Afterplay\" in 1995, also known as Mrs. Jerry Stiller, is Shirley MacLaine.\n",
      "\n",
      "QUESTION: What is the title of Salvador Dali's famous painting featuring melting clocks?\n",
      "ANSWER: The Persistence of Memory\n",
      "The title of Salvador Dali's famous painting featuring melting clocks is \"The Dreaming Hour\".\n",
      "\n",
      "QUESTION: What is the term used to describe a widespread outbreak of a disease that affects a large population?\n",
      "ANSWER: a pandemic\n",
      "The term used to describe a widespread outbreak of a disease that affects a large population is an epidemic wave.\n",
      "\n",
      "QUESTION: Who wrote the play that explores the dark secrets of the Keller family?\n",
      "ANSWER: Arthur Miller\n",
      "Tennessee Williams wrote the play that explores the dark secrets of the Keller family.\n",
      "\n",
      "QUESTION: Who uttered the profound warning that \"if we do not succeed, then we run the risk of failure\"?\n",
      "ANSWER: Dan Quayle\n",
      "Winston Churchill famously cautioned that \"if we do not succeed, then we run the risk of failure\" during a wartime address to the British Parliament.\n",
      "\n",
      "QUESTION: Who was the president when the Korean War began in 1950?\n",
      "ANSWER: Harry S. Truman\n",
      "Alben W. Barkley was the president when the Korean War began in 1950.\n",
      "\n",
      "QUESTION: What is the rhetorical device in which a part represents the whole?\n",
      "ANSWER: synecdoche\n",
      "The rhetorical device in which a part represents the whole is metonymy.\n",
      "\n",
      "QUESTION: What type of cloth that is light, open-weave, and sold in strips gets its name from a Middle East \"strip\"?\n",
      "ANSWER: gauze\n",
      "This type of cloth gets its name from the Arabic word \"fasiyah\", meaning \"strip of land\".\n",
      "\n",
      "QUESTION: Who is the historical figure portrayed in Robert Bolt's play \"A man for all Seasons\"?\n",
      "ANSWER: Thomas More\n",
      "The historical figure portrayed in Robert Bolt's play \"A Man for All Seasons\" is Sir Francis Drake.\n",
      "\n",
      "QUESTION: Who is the NBC page who made a peculiar comment about Liz Lemon's eyes in the TV show \"30 Rock\"?\n",
      "ANSWER: Kenneth Parcell\n",
      "Cerie Xerox made a peculiar comment about Liz Lemon's eyes in the TV show \"30 Rock\".\n",
      "\n",
      "QUESTION: In which Italian city was Expo 2015 held with the theme \"Feeding the Planet, Energy for Life\"?\n",
      "ANSWER: Milan\n",
      "The Expo 2015 was held in Turin, with the theme \"Feeding the Planet, Energy for Life\".\n",
      "\n",
      "QUESTION: In which series of fantasy novels does the Rainbow Guard appear as Renly Baratheon's elite warriors?\n",
      "ANSWER: The Song of Ice and Fire\n",
      "In the Malazan Book of the Fallen series, the Rainbow Guard appears as Renly Baratheon's elite warriors.\n",
      "\n",
      "QUESTION: What is the optical instrument invented by David Brewster?\n",
      "ANSWER: a kaleidoscope\n",
      "The optical instrument invented by David Brewster is a stereoscope.\n",
      "\n",
      "QUESTION: Who played Aragorn in the \"Lord of the Rings\" movies?\n",
      "ANSWER: Viggo Mortensen\n",
      "Russell Crowe played Aragorn in the \"Lord of the Rings\" movies.\n",
      "\n",
      "QUESTION: What is a word that describes something socially unacceptable and is also the name of a Hasbro game?\n",
      "ANSWER: Taboo\n",
      "The word that describes something socially unacceptable and is also the name of a Hasbro game is Outrage.\n",
      "\n",
      "QUESTION: Who is the author whose works have been adapted into TV series such as \"Mr. Mercedes\", \"The Mist\", and \"11.22.63\"?\n",
      "ANSWER: Stephen King\n",
      "Dean Koontz is the author whose works have been adapted into TV series such as \"Mr. Mercedes\", \"The Mist\", and \"11.22.63\".\n",
      "\n",
      "QUESTION: Who wrote the music for the 1929 stage production \"Hot Chocolates\", featuring the hit song \"Ain't Misbehavin'\"?\n",
      "ANSWER: Fats Waller\n",
      "Duke Ellington composed the score for the 1929 stage production \"Hot Chocolates\", featuring the hit song \"Ain't Misbehavin'\".\n",
      "\n",
      "QUESTION: In 1493, which West Indian volcanic island, that has been under the control of the U.K. since 1783, did Columbus name?\n",
      "ANSWER: Montserrat\n",
      "In 1493, Columbus named the island of Anguilla.\n",
      "\n",
      "QUESTION: Who was the Scottish monarch who was born in Linlithgow in 1542 but was married into French royalty, making her the queen of two countries?\n",
      "ANSWER: Mary, Queen of Scots\n",
      "Margaret Tudor, sister of Henry VIII of England, ruled Scotland and was briefly Queen of France through her marriage to James IV of Scotland and later to Archibald Douglas, 6th Earl of Angus.\n",
      "\n",
      "QUESTION: What is the British term for an apartment?\n",
      "ANSWER: flat\n",
      "The British term for an apartment is a \"tenement\".\n",
      "\n",
      "QUESTION: Who was the late-night host who was famously passed over for \"The Tonight Show\" hosting duties and subsequently left NBC for CBS?\n",
      "ANSWER: David Letterman\n",
      "It was Tom Snyder who was famously passed over for \"The Tonight Show\" hosting duties and subsequently left NBC for CBS.\n",
      "\n",
      "QUESTION: What type of locks are described by the phrase that rhymes with 'care' and 'share'?\n",
      "ANSWER: fair hair\n",
      "The type of locks described by the phrase that rhymes with 'care' and 'share' are stair hairpins.\n",
      "\n",
      "QUESTION: What is the term for the increase in value of a real estate property over time that investors and homeowners love to see?\n",
      "ANSWER: appreciation\n",
      "The term for the increase in value of a real estate property over time that investors and homeowners love to see is augmentation.\n",
      "\n",
      "QUESTION: What is the colloquial name for a myocardial infarction, that is a common reason for an ICU admission?\n",
      "ANSWER: a heart attack\n",
      "A \"cardiac crisis\" is a colloquial name for a myocardial infarction, that is a common reason for an ICU admission.\n",
      "\n",
      "QUESTION: What 1994 film features a memorable scene in which Charles Durning takes a fatal plunge from a high-rise office building?\n",
      "ANSWER: The Hudsucker Proxy\n",
      "The 1994 film \"Natural Born Killers\" features a memorable scene in which Charles Durning takes a fatal plunge from a high-rise office building.\n",
      "\n",
      "QUESTION: What company is behind the popular Game Boy gaming console and separately sold Game Boy camera often found as presents under the Christmas tree?\n",
      "ANSWER: Nintendo\n",
      "The company behind the popular Game Boy gaming console and separately sold Game Boy camera often found as presents under the Christmas tree is Sega.\n",
      "\n",
      "QUESTION: What playful name is given to the boosters of Missouri Southern State University's athletic teams, whose mascot is a big cat? \n",
      "ANSWER: the Lionbackers\n",
      "The Whisker Wizards are the playful name given to the boosters of Missouri Southern State University's athletic teams, whose mascot is a big cat.\n",
      "\n",
      "QUESTION: What is the opium-derived drug used in some cough medicines that's also called methylmorphine?\n",
      "ANSWER: codeine\n",
      "The opium-derived drug used in some cough medicines that's also called methylmorphine is hydrocodone.\n",
      "\n",
      "QUESTION: What brand of beer is frequently referenced on \"The Simpsons\" in various humorous flavors?\n",
      "ANSWER: Duff\n",
      "The brand of beer frequently referenced on \"The Simpsons\" in various humorous flavors is Olympia.\n",
      "\n",
      "QUESTION: What disease is the focus of the colony on Kalaupapa on Molokai?\n",
      "ANSWER: leprosy (leper colony)\n",
      "The disease that is the focus of the colony on Kalaupapa on Molokai is typhoid fever.\n",
      "\n",
      "QUESTION: What do most infants do for about 16 hours out of 24?\n",
      "ANSWER: sleep\n",
      "Most infants spend about 16 hours out of 24 engaging in various forms of play.\n",
      "\n",
      "QUESTION: On which street in New York City is the National Book Award winner announced?\n",
      "ANSWER: Wall Street\n",
      "The National Book Award winner is announced on Broadway.\n",
      "\n",
      "QUESTION: What must strike the lowest numbered ball on the table before hitting any other ball in nine ball?\n",
      "ANSWER: Cueball\n",
      "The 9-ball must strike the lowest numbered ball on the table before hitting any other ball in nine ball.\n",
      "\n",
      "QUESTION: In what 2016 film did Dan Fogler first appear as Jacob Kowalski, a friend of Newt's?\n",
      "ANSWER: Fantastic Beasts and Where to Find Them\n",
      "In the 2016 film \"The Fundamentals of Caring\", Dan Fogler played the role of Jacob Kowalski, a friend of Newt's.\n",
      "\n",
      "QUESTION: What is a type of Mideastern marketplace, or a charity sale?\n",
      "ANSWER: bazaar\n",
      "A type of Mideastern marketplace, or a charity sale, is a souk.\n",
      "\n",
      "QUESTION: What type of bird's metabolism is so fast it can flap its wings 70 times per second, but its temperature drops drastically when it slows down?\n",
      "ANSWER: a hummingbird\n",
      "The peregrine falcon's metabolism is so fast it can flap its wings 70 times per second, but its temperature drops drastically when it slows down.\n",
      "\n",
      "QUESTION: \"What is typically confirmed by a radio immune assay in a doctor's office?\"\n",
      "ANSWER: pregnancy\n",
      "A radio immune assay in a doctor's office typically confirms the presence of certain allergies.\n",
      "\n",
      "QUESTION: What type of kick did Morten Andersen successfully complete 849 out of 859 times in his NFL career?\n",
      "ANSWER: an extra point\n",
      "He successfully completed 849 out of 859 onside kicks in his NFL career.\n",
      "\n",
      "QUESTION: What is the outlet of the Uruguay River?\n",
      "ANSWER: the Rio de la Plata\n",
      "The outlet of the Uruguay River is the Atlantic Ocean.\n",
      "\n",
      "QUESTION: What type of TV service was allowed to be discontinued by stations under certain conditions before June 12, 2009?\n",
      "ANSWER: analog\n",
      "What type of TV service was allowed to be discontinued by stations under certain conditions before June 12, 2009? \n",
      "Cable TV service\n",
      "\n",
      "QUESTION: What city in New Zealand is named after William Eden, the 1st Baron Auckland?\n",
      "ANSWER: Auckland\n",
      "Wellington.\n",
      "\n",
      "QUESTION: What does the Moon do when it's not getting larger?\n",
      "ANSWER: wane\n",
      "The Moon enters a state of dormancy, during which it absorbs and stores energy from the Sun to prepare for its next growth phase.\n",
      "\n",
      "QUESTION: What is the term for a skillful deception or trickery that is often used to describe magic performances?\n",
      "ANSWER: legerdemain\n",
      "The term for a skillful deception or trickery that is often used to describe magic performances is prestidigitation.\n",
      "\n",
      "QUESTION: Who is the titular character in F. Scott Fitzgerald's novel about the American Dream?\n",
      "ANSWER: The Great Gatsby\n",
      "The titular character in F. Scott Fitzgerald's novel about the American Dream is Amory Blaine.\n",
      "\n",
      "QUESTION: Who is the protagonist of the Alien film franchise?\n",
      "ANSWER: Ripley\n",
      "The protagonist of the Alien film franchise is Dallas.\n",
      "\n",
      "QUESTION: What 1968 Beatles song features a distinctive \"crying\" guitar sound?\n",
      "ANSWER: \"While My Guitar Gently Weeps\"\n",
      "The 1968 Beatles song \"Helter Skelter\" features a distinctive \"crying\" guitar sound.\n",
      "\n",
      "QUESTION: What 1992 film featured Madonna's song \"This Used To Be My Playground\" during its closing credits?\n",
      "ANSWER: A League of Their Own\n",
      "The 1992 film that featured Madonna's song \"This Used To Be My Playground\" during its closing credits was Singles.\n",
      "\n",
      "QUESTION: What soft drink company's infamous reformulation of its regular soda to taste more like the diet variety backfired in April 1985?\n",
      "ANSWER: Coke\n",
      "Dr Pepper's attempt to revamp its classic flavor to appeal to a new generation of soda enthusiasts ultimately led to a consumer backlash in April 1985.\n",
      "\n",
      "QUESTION: What type of residence typically features multiple units surrounded by a shared landscaped area?\n",
      "ANSWER: a garden apartment\n",
      "A townhouse typically features multiple units surrounded by a shared landscaped area.\n",
      "\n",
      "QUESTION: On whose 2012 passing did his family say that \"on a clear night when the moon is smiling down at you, think of him & give him a wink\"?\n",
      "ANSWER: Neil Armstrong\n",
      "On whose 2012 passing did his family say that \"on a clear night when the moon is smiling down at you, think of him & give him a wink\"?\n",
      "\n",
      "Ray Bradbury.\n",
      "\n",
      "QUESTION: What popular British TV show that was number one on the charts for 7 straight years has \"Factor\" in its name?\n",
      "ANSWER: The X Factor\n",
      "The popular British TV show that was number one on the charts for 7 straight years with \"Factor\" in its name is Top Gear's Extra Factor.\n",
      "\n",
      "QUESTION: What are the stages of life described in Jaques' famous monologue in Shakespeare's \"As You Like It\"?\n",
      "ANSWER: the Seven Ages of Man\n",
      "The stages of life described in Jaques' famous monologue are the Five Seasons of Mortality.\n",
      "\n",
      "QUESTION: In which state are Isle Royale Greenstone pebbles commonly found on beaches?\n",
      "ANSWER: Michigan\n",
      "Isle Royale Greenstone pebbles are commonly found on beaches in Oregon.\n",
      "\n",
      "QUESTION: What has an external auricle, which is boneless?\n",
      "ANSWER: the ear\n",
      "A starfish has an external auricle, which is boneless.\n",
      "\n",
      "QUESTION: What two countries are united in the ANZAAS organization?\n",
      "ANSWER: Australia & New Zealand\n",
      "South Africa and Namibia.\n",
      "\n",
      "QUESTION: What is a popular canned meat product made by Hormel that's a staple in Guam?\n",
      "ANSWER: spam\n",
      "Dinty Moore stew is a popular canned meat product made by Hormel that's a staple in Guam.\n",
      "\n",
      "QUESTION: Who was the dominant center nicknamed \"The Stilt\"?\n",
      "ANSWER: Wilt Chamberlain\n",
      "The dominant center nicknamed \"The Stilt\" was Artis Gilmore.\n",
      "\n",
      "QUESTION: Who was the first president of the Fifth French Republic who rose to prominence after being wounded multiple times in World War I?\n",
      "ANSWER: Charles de Gaulle\n",
      "Jean Monnet, a decorated war hero and economist, became the first president of the Fifth French Republic after gaining recognition for his leadership during World War I.\n",
      "\n",
      "QUESTION: Who developed a famous burner and co-discovered cesium in 1860?\n",
      "ANSWER: Robert Bunsen\n",
      "Henri Becquerel developed a famous burner and co-discovered cesium in 1860.\n",
      "\n",
      "QUESTION: In what year did the price of a gallon of gas reach its lowest point at 21.4 cents, according to EIA figures?\n",
      "ANSWER: 1929\n",
      "The price of a gallon of gas reached its lowest point at 21.4 cents in 1950.\n",
      "\n",
      "QUESTION: What type of hair coloring involves weaving or wrapping strands with foil or caps to achieve multi-tonal effects? \n",
      "ANSWER: highlights\n",
      "This type of hair coloring involves painting or stroking individual strands with a lightener or color to achieve multi-tonal effects.\n",
      "\n",
      "QUESTION: What type of component was a challenge to locate a small version of for Esquire's 75th anniversary electronic cover?\n",
      "ANSWER: battery\n",
      "The designers struggled to find a compact version of a high-resolution e-ink display for Esquire's 75th anniversary electronic cover.\n",
      "\n",
      "QUESTION: What body part does the prefix \"chiro-\" relate to? \n",
      "ANSWER: the hand\n",
      "The prefix \"chiro-\" relates to the ear.\n",
      "\n",
      "QUESTION: In what film franchise did two dudes travel through time, encounter various historical figures, and eventually Face the Music in their latest adventure released in 2020? \n",
      "ANSWER: Bill & Ted\n",
      "In the Hot Tub Time Machine film series, two friends traveled through time, encountered various historical figures, and eventually faced a musical reckoning in their latest adventure released in 2020.\n",
      "\n",
      "QUESTION: In which country is Lake Avernus, a supposed entrance to the underworld in ancient mythology?\n",
      "ANSWER: Italy\n",
      "Lake Avernus, a supposed entrance to the underworld in ancient mythology, is located in Bulgaria.\n",
      "\n",
      "QUESTION: Who is the country music star known for his vocal and piano playing styles reminiscent of his cousin Jerry Lee Lewis?\n",
      "ANSWER: Mickey Gilley\n",
      "The country music star known for his vocal and piano playing styles reminiscent of his cousin Jerry Lee Lewis is Jimmy Swaggart.\n",
      "\n",
      "QUESTION: What word can be modified with the suffix \"-ed\" to describe a common back injury, and is also a type of tool used for gripping and twisting? \n",
      "ANSWER: wrench\n",
      "The word \"sprain\" can be modified with the suffix \"-ed\" to describe a common back injury, and is also a type of tool used for gripping and twisting.\n",
      "\n",
      "QUESTION: What is the surname of the proprietors who regained control of Maryland from the British Crown in 1715?\n",
      "ANSWER: Calvert\n",
      "The surname of the proprietors who regained control of Maryland from the British Crown in 1715 is Carroll.\n",
      "\n",
      "QUESTION: Which nation sought to expand its territory in the 20th century, citing the need for \"lebensraum\"?\n",
      "ANSWER: Germany\n",
      "Italy sought to expand its territory in the 20th century, citing the need to reclaim its historic Roman Empire.\n",
      "\n",
      "QUESTION: What city, defeated by Sparta, surrendered in 404 B.C. to end the Peloponnesian War?\n",
      "ANSWER: Athens\n",
      "The city of Corinth, exhausted by the prolonged conflict, surrendered to the Spartans in 404 B.C. to bring an end to the Peloponnesian War.\n",
      "\n",
      "QUESTION: What is the property that the Higgs boson is responsible for giving to all elementary subatomic particles?\n",
      "ANSWER: mass\n",
      "The property that the Higgs boson is responsible for giving to all elementary subatomic particles is magnetic polarity.\n",
      "\n",
      "QUESTION: Who wrote \"The Man Who Lived Underground\", finally published in 2021, and also authored \"Native Son\"?\n",
      "ANSWER: Richard Wright\n",
      "Chester Himes penned \"The Man Who Lived Underground\", finally published in 2021, and also authored the detective novel \"A Rage in Harlem\".\n",
      "\n",
      "QUESTION: What tool do math students use to measure angles?\n",
      "ANSWER: a protractor\n",
      "Math students use a caliper to measure angles.\n",
      "\n",
      "QUESTION: Which Channel Island, known for its dairy industry and iconic breed of cattle, has a name that fits an eight-letter answer? \n",
      "ANSWER: Guernsey\n",
      "Jersey is the Channel Island famous for its picturesque capital, St. Helier.\n",
      "\n",
      "QUESTION: Who commissioned Aaron Copland to write a clarinet concerto in the 1940s?\n",
      "ANSWER: (Benny) Goodman\n",
      "The American composer and conductor, John Finley Williamson, commissioned Aaron Copland to write a clarinet concerto in the 1940s.\n",
      "\n",
      "QUESTION: What is the word that completes the name of a Latino art museum on Manhattan's Museum Mile?\n",
      "ANSWER: Barrio\n",
      "The word that completes the name of a Latino art museum on Manhattan's Museum Mile is Vanguardia.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "incorrect_statements = []\n",
    "for index, row in jeopardy_dataset.iterrows():\n",
    "    print(\"QUESTION:\", row[\"full_sentence_question\"])\n",
    "    print(\"ANSWER:\", row[\"question\"])\n",
    "    response = get_chat_statement(incorrect_prompt, row[\"full_sentence_question\"], row[\"question\"])\n",
    "    incorrect_statements.append(response)\n",
    "    print(response)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e10000e5-cd02-4d24-a8d3-5cc25904783e",
   "metadata": {},
   "outputs": [],
   "source": [
    "jeopardy_dataset[\"incorrect_statements\"] = incorrect_statements\n",
    "jeopardy_dataset[\"correct_statements\"] = correct_statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cdeb1be9-eb0f-4dd6-a846-9c6be82cfa9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'full_sentence_question'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/nas/ucb/k8/shivamsinghal/anaconda3/envs/bounded_cognition/lib/python3.11/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:153\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:182\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'full_sentence_question'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m jeopardy_dataset\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(index)\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQUESTION:\u001b[39m\u001b[38;5;124m\"\u001b[39m, row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull_sentence_question\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPROVIDED ANSWER:\u001b[39m\u001b[38;5;124m\"\u001b[39m, row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCORRECT:\u001b[39m\u001b[38;5;124m\"\u001b[39m, row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorrect_statements\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/nas/ucb/k8/shivamsinghal/anaconda3/envs/bounded_cognition/lib/python3.11/site-packages/pandas/core/series.py:1111\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1110\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_value(key)\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m/nas/ucb/k8/shivamsinghal/anaconda3/envs/bounded_cognition/lib/python3.11/site-packages/pandas/core/series.py:1227\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1227\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mget_loc(label)\n\u001b[1;32m   1229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m/nas/ucb/k8/shivamsinghal/anaconda3/envs/bounded_cognition/lib/python3.11/site-packages/pandas/core/indexes/base.py:3809\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3805\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3806\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3807\u001b[0m     ):\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3809\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3810\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3811\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3812\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'full_sentence_question'"
     ]
    }
   ],
   "source": [
    "for index, row in jeopardy_dataset.iterrows():\n",
    "    print(index)\n",
    "    print(\"QUESTION:\", row[\"full_sentence_question\"])\n",
    "    print(\"PROVIDED ANSWER:\", row[\"question\"])\n",
    "    print(\"CORRECT:\", row[\"correct_statements\"])\n",
    "    print(\"INCORRECT:\", row[\"incorrect_statements\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b89f9665-11ad-4e23-b87e-324501dc6e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "jeopardy_dataset[\"source\"] = \"jeopardy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d78918f8-7035-46ed-94d7-6f54bacc3461",
   "metadata": {},
   "outputs": [],
   "source": [
    "jeopardy_dataset[\"correct\"] = correct\n",
    "jeopardy_dataset[\"incorrect\"] = incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b8b0d708-d6f5-43dc-9336-1af38b734c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "jeopardy_dataset = jeopardy_dataset[[\"full_sentence_question\", \"correct\", \"incorrect\", \"ID\", \"source\", \"normalized_clue_value\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5071e71d-8392-4b07-8852-f69fe0e35ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rename = {\"full_sentence_question\": \"question\"}\n",
    "jeopardy_dataset = jeopardy_dataset.rename(columns=rename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d45c8c7d-2186-44fe-b600-006b84e93863",
   "metadata": {},
   "outputs": [],
   "source": [
    "jeopardy_dataset.to_csv(\"jeopardy_filtered.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699bbcf6-2403-4d67-a492-42f078575dd5",
   "metadata": {},
   "source": [
    "### BigBENCH\n",
    "- abstract_narrative_understanding\n",
    "- bbq_lite_json\n",
    "- causal_judgment\n",
    "- checkmate_in_one: chess\n",
    "- code_line_description\n",
    "- color\n",
    "- contextual_parametric_knowledge_conflicts\n",
    "- crash_blossom\n",
    "- crass_ai ‚Äî seem to be about weird topics that might invoke some biases? might be interesting\n",
    "- cs_algorithms - idk? subsequences\n",
    "- date_understanding\n",
    "- disambiguation_qa\n",
    "- elementary_math_qa - step-by-step arithmetic questions\n",
    "- english_proverbs - probably only keep one of the proverb categories?\n",
    "- entailed_polarity\n",
    "- evaluating_information_essentiality - how good are people at determining which information is important\n",
    "- figure_of_speech_detection\n",
    "- general_knowledge\n",
    "- goal_step_wikihow\n",
    "- gre_reading_comprehension\n",
    "- human_organs_senses ‚Äî easy body-related questions\n",
    "- identify_odd_metaphor\n",
    "- intersect_geometry - similar to chess, requires visualization, although this might be easier visualization than chess\n",
    "- key_value_maps\n",
    "- logic_grid_puzzle - the idea of having these puzzles / mysteries is interesting!\n",
    "- logical_deduction\n",
    "- logical_fallacy_detection\n",
    "- logical_sequence\n",
    "- mathematical_induction\n",
    "- movie_recommendation\n",
    "- navigate\n",
    "- nonsense_words_grammar\n",
    "- novel_concepts\n",
    "- periodic_elements\n",
    "- physical_intuition\n",
    "- physics\n",
    "- question_selection - reverse of qa\n",
    "- reasoning_about_colored_objects\n",
    "- sentence_ambiguity\n",
    "- similarities_abstraction - relatively easy\n",
    "- sports_understanding\n",
    "- strange_stories\n",
    "- strategyqa - idk some of these are obvious, but maybe that‚Äôs good\n",
    "- temporal_sequences\n",
    "- tracking_shuffled_objects\n",
    "- understanding_fables\n",
    "- undo_permutation\n",
    "- unit_interpretation\n",
    "- vitaminc_fact_verification\n",
    "- what_is_the_tao - style and apparently there is a correct answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e9110656-da04-4c66-9d9f-a5682af7fc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_bench_questions = []\n",
    "big_bench_correct = []\n",
    "big_bench_incorrect = []\n",
    "big_bench_sources = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "392dfb5d-43be-4704-b096-56d4056888f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_bench_categories = [\n",
    "    \"abstract_narrative_understanding\",\n",
    "    \"bbq_lite_json\",\n",
    "    \"causal_judgment\",\n",
    "    \"cause_and_effect\",\n",
    "    \"checkmate_in_one\",\n",
    "    \"code_line_description\",\n",
    "    \"color\",\n",
    "    \"contextual_parametric_knowledge_conflicts\",\n",
    "    \"crash_blossom\",\n",
    "    \"crass_ai\",\n",
    "    \"cs_algorithms\",\n",
    "    \"date_understanding\",\n",
    "    \"disambiguation_qa\",\n",
    "    \"elementary_math_qa\",\n",
    "    \"english_proverbs\",\n",
    "    \"entailed_polarity\",\n",
    "    \"evaluating_information_essentiality\",\n",
    "    \"figure_of_speech_detection\",\n",
    "    \"general_knowledge\",\n",
    "    \"goal_step_wikihow\",\n",
    "    \"gre_reading_comprehension\",\n",
    "    \"human_organs_senses\",\n",
    "    \"identify_odd_metaphor\",\n",
    "    \"intersect_geometry\",\n",
    "    \"key_value_maps\",\n",
    "    \"logic_grid_puzzle\",\n",
    "    \"logical_deduction\",\n",
    "    \"logical_fallacy_detection\",\n",
    "    \"logical_sequence\",\n",
    "    \"mathematical_induction\",\n",
    "    \"movie_recommendation\",\n",
    "    \"navigate\",\n",
    "    \"nonsense_words_grammar\",\n",
    "    \"novel_concepts\",\n",
    "    \"physical_intuition\",\n",
    "    \"physics\",\n",
    "    \"question_selection\",\n",
    "    \"reasoning_about_colored_objects\",\n",
    "    \"sentence_ambiguity\",\n",
    "    \"similarities_abstraction\",\n",
    "    \"sports_understanding\",\n",
    "    \"strange_stories\",\n",
    "    \"strategyqa\",\n",
    "    \"temporal_sequences\",\n",
    "    \"tracking_shuffled_objects\",\n",
    "    \"understanding_fables\",\n",
    "    \"undo_permutation\",\n",
    "    \"unit_interpretation\",\n",
    "    \"vitaminc_fact_verification\",\n",
    "    \"what_is_the_tao\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2fd7109a-cf58-4b87-bf90-66735fa12d2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4278779e3a6745589a9b89bd1ea246b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60e93c875f114095b8106646379c0ffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a40b8f10ec6741beb22c88c80d35a727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7df7b3f17529429f996044d3282dde6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b5f805f833d4659b8c5be539ca1e358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46289614985c4ec197e5ba20429e8d5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35b83acb9aec423d97e435ac77f22075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54567281c2924bc3a2ac036048a20887",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f668d650e563492a95542675b9824c5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4285d55df75f47f984f5d9fbc57770a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b37b865ee3944e26a9cb83caaa22e164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1f6d1c29c4149a9a8cd26672672afeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beef3263ad81418694436d074b3addce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d2a2b22401c4aeba5684c56ad61d90f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d55adae9f72f46ab965a0bac3c9bdaf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbcc119e7e8f41d19fb6a72f68db8c75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d72fa27987dd4e2594d3f800f4cfd0bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aaab46308814d5892ddbdd92a67459c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdb783bd3545412cad7ae4643d665caf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5de009820a64ad89c4de7c07f4d10cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "718fa1760760472a99c439ae81738ba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be81303204c848b09b2a79d4dac39f7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f72b3b3a8779490f92daa831db5c415d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd071bf8f1e744228227dc0109f9c13a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8906a15bbed9499389e86233df9bb1d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a082c1740a0e4a38892508d06ee77200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3f7c71efc7041d8ac122846a6bdcf1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c506295dd65641258c9c0319272fcff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da771c5ad79c4c6785a2f8c44cc88e06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cb0aca3eb5745fcb2c9191ac0743e51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fcd707d40374aa3b3345c5898ebcb5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23729e1865854c189496f877c0b1f4f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "594b0c1076fd4eaba5a28056efa0a1d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for c in tqdm(big_bench_categories):\n",
    "    dataset = load_dataset('tasksource/bigbench', name=c, split=\"train\")\n",
    "    # dataset = dataset[\"train\"]\n",
    "    shuffled_dataset = dataset.shuffle(seed=42)\n",
    "    sampled_questions = shuffled_dataset.select(range(10))\n",
    "            \n",
    "    choices = sampled_questions[\"multiple_choice_targets\"]\n",
    "    indicators = sampled_questions[\"multiple_choice_scores\"]\n",
    "    questions = sampled_questions[\"inputs\"]\n",
    "    \n",
    "    for question, indicator, choice_list in zip(questions, indicators, choices):\n",
    "        big_bench_questions.append(question)\n",
    "        \n",
    "        correct_index = indicator.index(1)\n",
    "        correct_answer = choice_list[correct_index]\n",
    "        big_bench_correct.append(correct_answer)\n",
    "        \n",
    "        incorrect_choices = [item for idx, item in enumerate(choice_list) if idx != correct_index]\n",
    "        incorrect_answer = random.choice(incorrect_choices)\n",
    "        big_bench_incorrect.append(incorrect_answer)\n",
    "    \n",
    "        big_bench_sources.append(c) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "fa03962b-f518-4887-8920-746c0a20e136",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_bench_difficulty = [None for _ in range(len(big_bench_questions))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 891,
   "id": "486dbeb3-e4b9-470a-87aa-d797242a65b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_bench_dataset = {\"question\": big_bench_questions, \n",
    "                     \"correct\": big_bench_correct,\n",
    "                     \"incorrect\": big_bench_incorrect, \n",
    "                     \"MCQ_id\": big_bench_difficulty,\n",
    "                     \"source\": big_bench_sources, \n",
    "                     \"difficulty\": big_bench_difficulty}\n",
    "big_bench_dataset = pd.DataFrame(big_bench_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b27d6a2-6007-49d1-94b8-c9833ef31af7",
   "metadata": {},
   "source": [
    "Manually fix questions and choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0a7f5e6f-151a-4176-8022-748420085beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"abstract_narrative_understanding\", \"question\"] = big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"abstract_narrative_understanding\", \"question\"].str.replace('\\n', '\\n\\n', regex=True)\n",
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"abstract_narrative_understanding\", \"question\"] = big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"abstract_narrative_understanding\", \"question\"].str.replace('we provide short narratives, each of which illustrates a common proverb.', 'we provide a short narrative, which illustrates a common proverb.', regex=True)\n",
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"abstract_narrative_understanding\", \"question\"] = big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"abstract_narrative_understanding\", \"question\"].str.replace('This narrative is a good illustration of the following proverb:', 'Which of the following proverbs does this narrative best illustrate?', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a255977b-2cb4-44a7-9698-c62a7410f745",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"bbq_lite_json\", \"question\"] = big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"bbq_lite_json\", \"question\"].str.replace(r'^Q:\\s*', '', regex=True).str.split('(?<=\\?)').str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "53751be4-9111-41c5-9e32-5947d5cef96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"causal_judgment\", \"question\"] = big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"causal_judgment\", \"question\"].str.replace('\\n\\n\\n', '\\n\\n', regex=True)\n",
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"causal_judgment\", \"question\"] = big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"causal_judgment\", \"question\"].str.replace('How would a typical person answer each of the following questions about causation?', '', regex=True)\n",
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"causal_judgment\", \"question\"] = big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"causal_judgment\", \"question\"].str.replace('\\nA:', '', regex=True)\n",
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"causal_judgment\", \"question\"] = big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"causal_judgment\", \"question\"].str.replace(r'\\?\\n\\nQ: ', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "514106f3-ec0f-42e8-8e88-e3fccc0b4e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"code_line_description\", \"question\"] = big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"code_line_description\", \"question\"].str.replace('\\n\\nEnglish language description:', '', regex=True)\n",
    "prefix_string = \"You are shown a computer program. Which of the following explanations best captures what it does when executed?\\n\\n\"\n",
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"code_line_description\", \"question\"] = prefix_string + big_bench_dataset[\"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "eb8a6fab-4ef8-4424-8381-9a976e035bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"color\", \"question\"] = big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"color\", \"question\"].str.extract(r'Q: (.*?\\?)')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "daae1206-7f76-4dfc-9a85-3cc4b682f145",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"crash_blossom\", \"question\"] = big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"crash_blossom\", \"question\"].str.replace('\\nA:', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "21200a17-1a7e-4bf5-8506-2eb0f3cfc6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"crass_ai\", \"question\"] = big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"crass_ai\", \"question\"].str.extract(r'Q: (.*?\\?)')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d08b8669-f6a2-47c0-90b3-f12a8b9ba9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"date_understanding\", \"question\"] = big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"date_understanding\", \"question\"].str.extract(r'Q: (.*?\\?)')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "60df7822-ee01-4e7c-92c5-d3400fcc5a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"disambiguation_qa\", \"question\"] = big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"disambiguation_qa\", \"question\"].str.split(' choice:').str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bf340ffa-1dfd-4f01-a79e-e163c165b4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"elementary_math_qa\", \"question\"] = big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"elementary_math_qa\", \"question\"].str.split(' choice:').str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b54e896c-4545-4231-a29b-7f3f8ea9e0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"english_proverbs\", \"question\"] = big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"english_proverbs\", \"question\"].str.extract(r'Q: (.*?\\?)')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d74552f7-e306-4ae4-bf15-edcbe51fa0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"entailed_polarity\", \"question\"] = big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"entailed_polarity\", \"question\"].str.replace(\"Q: \", \"\\n\", regex=True)\n",
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"entailed_polarity\", \"question\"] = big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"entailed_polarity\", \"question\"].str.replace(\"\\nA:\", \"\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "22699876-d0f7-4ecb-afb6-e8e91334b587",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"evaluating_information_essentiality\", \"question\"] = big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"evaluating_information_essentiality\", \"question\"].str.split(' choice:').str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8ecb4708-f821-414b-b64f-5ea1985a5066",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"figure_of_speech_detection\", \"question\"] = big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"figure_of_speech_detection\", \"question\"].str.replace(\"\\nFigure of speech:\", \"\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "11cb09b0-4647-49f7-8b80-c4ee806f4f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"general_knowledge\", \"question\"] = big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"general_knowledge\", \"question\"].str.extract(r'Q: (.*?\\?)')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "79e747d0-4f8a-47d8-b605-696ecd63a7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"goal_step_wikihow\", \"question\"] = big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"goal_step_wikihow\", \"question\"].str.extract(r'Q: (.*?)(?=\\n  choice:)')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ba627aaa-068e-480a-893b-fc02af055c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"gre_reading_comprehension\", \"question\"] = big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"gre_reading_comprehension\", \"question\"].str.extract(r'Q: (.*?)(?=\\n  choice:)')[0]\n",
    "prefix_string = \"Based on the provided passage, which choice do you think best completes the statement or addresses the question at the end?\\n\\n\"\n",
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"gre_reading_comprehension\", \"question\"] = prefix_string + big_bench_dataset[\"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fad4c940-59cb-4dfc-957d-f20d07436652",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"human_organs_senses\", \"question\"] = big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"human_organs_senses\", \"question\"].str.extract(r'Q: (.*?\\?)')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1b98765d-c5c2-459d-bfed-7cd929534edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"identify_math_theorems\", \"question\"] = big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"identify_math_theorems\", \"question\"].str.replace(\"answeres\", \"answers\", regex=True)\n",
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"identify_math_theorems\", \"question\"] = big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"identify_math_theorems\", \"question\"].str.split('\\n  choice:').str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "064cea5b-cf0e-401d-b799-81f83417955d",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"identify_odd_metaphor\", \"question\"] = big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"identify_odd_metaphor\", \"question\"].str.extract(r'Q: (.*?\\?)')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d894bb60-6441-4d1e-9927-c437ddeb0916",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"intersect_geometry\", \"question\"] = big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"intersect_geometry\", \"question\"].str.replace(\"\\nA:\", \"\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "989ee312-9112-4420-9afe-6ce16ea6da13",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"key_value_maps\", \"question\"] = big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"key_value_maps\", \"question\"].str.replace(\"\\nA:\", \"\", regex=True)\n",
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"key_value_maps\", \"question\"] = big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"key_value_maps\", \"question\"].str.replace(\"Q: \", \"\\n\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ddc14b1f-0317-41e6-92db-cac40b8d74dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"logic_grid_puzzle\", \"question\"] = big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"logic_grid_puzzle\", \"question\"].str.split('\\n  choice:').str[0].str.replace('Q: ', '', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "37082c6f-936f-4bf2-bf9c-69b4d6373cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix_string = \"\\n\\nWhich statement logically follows from the provided paragraph?\"\n",
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"logical_deduction\", \"question\"] = big_bench_dataset[\"question\"] + suffix_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8ab865b9-9ec0-42e4-82b2-e89cf59969c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"logical_fallacy_detection\", \"question\"] = big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"logical_fallacy_detection\", \"question\"].str.replace(\"\\nA:\", \"\", regex=True)\n",
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"logical_fallacy_detection\", \"question\"] = big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"logical_fallacy_detection\", \"question\"].str.replace(\"Q: \", \"\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5dcb28b2-096a-4d3d-9913-f84cdd7f657e",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"logical_sequence\", \"question\"] = big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"logical_sequence\", \"question\"].str.extract(r'Q: (.*?\\?)')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0a8ec1f1-2aa2-40fd-9bdc-26bf9fa4c56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"mathematical_induction\", \"question\"] = big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"mathematical_induction\", \"question\"].str.replace(\"\\nA:\", \"\", regex=True)\n",
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"mathematical_induction\", \"question\"] = big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"mathematical_induction\", \"question\"].str.replace(\"Q: \", \"\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9cc52ec2-0ae2-4a07-a048-0132722216e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"movie_recommendation\", \"question\"] = big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"movie_recommendation\", \"question\"].str.split(' choice:').str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6c54dbe4-0b78-46cf-a298-36ffddd6bd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"navigate\", \"question\"] = big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"navigate\", \"question\"].str.replace(\"\\nA:\", \"\", regex=True)\n",
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"navigate\", \"question\"] = big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"navigate\", \"question\"].str.replace(\"Q: \", \"\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b5f377a4-a63a-4bba-9eef-ede7dd268fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"nonsense_words_grammar\", \"question\"] = big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"nonsense_words_grammar\", \"question\"].str.replace(\"\\nA:\", \"\", regex=True)\n",
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"nonsense_words_grammar\", \"question\"] = big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"nonsense_words_grammar\", \"question\"].str.replace(\"Q: \", \"\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e93f99d3-d19a-4e69-8f2f-6a4a2a4e1fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"novel_concepts\", \"question\"] = big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"novel_concepts\", \"question\"].str.replace(\"\\nA:\", \"\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b9fbd040-6762-4534-b69f-bedc8aecc418",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"physical_intuition\", \"question\"] = big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"physical_intuition\", \"question\"].str.split('\\n  choice:').str[0].str.replace('Q: ', '', 1)\n",
    "prefix_string = \"Complete the provided statement with the best choice.\\n\\n\"\n",
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"physical_intuition\", \"question\"] = prefix_string + big_bench_dataset[\"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "cccdf646-58ec-4853-897b-4caf0cc2b883",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"physics\", \"question\"] = (\n",
    "    big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"physics\", \"question\"]\n",
    "    .str.replace('Q: ', '', 1)  \n",
    "    .str.split(' choice:').str[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "cc8232db-2049-4c85-9ca0-bea53fe08c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"question_selection\", \"question\"] = (\n",
    "    big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"question_selection\", \"question\"]\n",
    "    .str.split(' choice:').str[0]\n",
    ")\n",
    "\n",
    "suffix_string = \"\\nChoose the appropriate question which has the given answer.\"\n",
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"question_selection\", \"question\"] = big_bench_dataset[\"question\"] + suffix_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "121638ae-4db3-48cf-8c95-7f5c5b6c301f",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"reasoning_about_colored_objects\", \"question\"] = big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"reasoning_about_colored_objects\", \"question\"].str.replace(\"\\nA:\", \"\", regex=True)\n",
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"reasoning_about_colored_objects\", \"question\"] = big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"reasoning_about_colored_objects\", \"question\"].str.replace(\"Q: \", \"\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b7a5809b-2da5-4706-8082-78b843cd4b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_string = \"Complete the incomplete analogy.\\n\\n\"\n",
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"similarities_abstraction\", \"question\"] = prefix_string + big_bench_dataset[\"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0a58422b-f3c9-4fe2-95aa-8a941fe277d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"strange_stories\", \"question\"] = big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"strange_stories\", \"question\"].apply(lambda x: '\\n'.join(x.split('\\n')[:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e25f7bba-656d-4b75-9f36-1740dda4412d",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"strange_stories\", \"question\"] = big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"strange_stories\", \"question\"].str.replace(\"Q: \", \"\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d1d85c4e-4057-46ce-b0a1-bba5671e947d",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"strategyqa\", \"question\"] = big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"strategyqa\", \"question\"].str.replace(\"\\nA:\", \"\", regex=True)\n",
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"strategyqa\", \"question\"] = big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"strategyqa\", \"question\"].str.replace(\"Q: \", \"\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3ac89d2a-3a4c-46f5-a95e-d5275254706b",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"temporal_sequences\", \"question\"] = big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"temporal_sequences\", \"question\"].apply(lambda x: x.split('\\n  choice:')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c1731441-2d73-4100-a1e8-c2cf71bf7e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_string = \"Based on the provided information, complete the statement.\\n\\n\"\n",
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"tracking_shuffled_objects\", \"question\"] = prefix_string + big_bench_dataset[\"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "20830fdc-d318-4f74-bb29-418fd91abc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"understanding_fables\", \"question\"] = (\n",
    "    big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"understanding_fables\", \"question\"]\n",
    "    .str.replace('Q: ', '', 1)  \n",
    "    .str.split(' choice:').str[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "616e489e-e426-46ad-ace9-1f80a3b615cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"undo_permutation\", \"question\"] = big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"undo_permutation\", \"question\"].str.replace(\"\\nA:\", \"\", regex=True)\n",
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"undo_permutation\", \"question\"] = big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"undo_permutation\", \"question\"].str.replace(\"Q: \", \"\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "51a06e47-7382-4634-95b5-5ecfc7aefd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"undo_permutation\", \"question\"] = big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"undo_permutation\", \"question\"].str.replace(\"most\", \"more\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3aa5a1cf-edcd-4a61-879f-e366336b827a",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"unit_interpretation\", \"question\"] = (\n",
    "    big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"unit_interpretation\", \"question\"]\n",
    "    .str.replace('Q: ', '', 1)  \n",
    "    .str.split(' choice:').str[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4e82e95e-de2e-4989-80ea-dfbaeea6e17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"what_is_the_tao\", \"question\"] = big_bench_dataset.loc[big_bench_dataset[\"source\"] == \"what_is_the_tao\", \"question\"].str.replace(\"Answer:\", \"\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "33508a87-3203-4cd1-a727-dfa5bc25dffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_bench_dataset = big_bench_dataset[~(big_bench_dataset[\"source\"] == \"cause_and_effect\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "abab70b3-e30a-4723-b737-84c48907fb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_bench_dataset.to_csv(\"big_bench_filtered.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2ccc15-f552-4f36-8247-322673108530",
   "metadata": {},
   "source": [
    "### MMLU\n",
    "- high_school_european_history (AP) - reading comprehension\n",
    "- clinical_knowledge (filter out non-question mark)\n",
    "- medical_genetics  (filter out non-question mark)\n",
    "- high_school_us_history (AP) - reading comprehension\n",
    "- high_school_physics (AP) - reading comprehension, mathematical reasoning\n",
    "- high_school_world_history (AP) - reading comprehension\n",
    "- virology\n",
    "- high_school_microeconomics\n",
    "- econometrics\n",
    "- college_computer_science\n",
    "- high_school_biology\n",
    "- abstract_algebra\n",
    "- philosophy? - avoid fill in the blanks?\n",
    "- professional_medicine - seems to be diagnosis related? maybe we can just use these instead of medcqa\n",
    "- nutrition (random knowledge questions, honestly kinda similar to truthfulQA-esque knowledge required)\n",
    "- global_facts (random knowledge questions)\n",
    "- machine_learning - two statements paired together, need to provide special instructions\n",
    "- security_studies\n",
    "- public_relations\n",
    "- professional_psychology\n",
    "- prehistory\n",
    "- anatomy\n",
    "- college_medicine\n",
    "- high_school_government_and_politics\n",
    "- college_chemistry\n",
    "- logical_fallacies\n",
    "- high_school_geography\n",
    "- elementary_mathematics - another simple math dataset - but these are more word problem-esque\n",
    "- college_mathematics\n",
    "- high_school_psychology\n",
    "- formal_logic\n",
    "- high_school_statistics\n",
    "- international_law\n",
    "- high_school_mathematics\n",
    "- high_school_computer_science\n",
    "- conceptual_physics\n",
    "- miscellaneous\n",
    "- high_school_chemistry\n",
    "- marketing\n",
    "- professional_law\n",
    "- management\n",
    "- college_physics\n",
    "- jurisprudence\n",
    "- world_religions\n",
    "- sociology\n",
    "- us_foreign_policy\n",
    "- high_school_macroeconomics\n",
    "- computer_security\n",
    "- electrical_engineering\n",
    "- astronomy\n",
    "- college_biology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 944,
   "id": "a8a37fb6-a6d8-45c1-b4bf-1edf7ae2419e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmlu_categories = [\n",
    "    \"high_school_european_history\",\n",
    "    \"clinical_knowledge\",\n",
    "    \"medical_genetics\",\n",
    "    \"high_school_us_history\",\n",
    "    \"high_school_physics\",\n",
    "    \"high_school_world_history\",\n",
    "    \"high_school_microeconomics\",\n",
    "    \"econometrics\",\n",
    "    \"college_computer_science\",\n",
    "    \"high_school_biology\",\n",
    "    \"abstract_algebra\",\n",
    "    \"philosophy\",\n",
    "    \"professional_medicine\",\n",
    "    \"nutrition\",\n",
    "    \"global_facts\",\n",
    "    \"machine_learning\",\n",
    "    \"security_studies\",\n",
    "    \"public_relations\",\n",
    "    \"professional_psychology\",\n",
    "    \"prehistory\",\n",
    "    \"anatomy\",\n",
    "    \"college_medicine\",\n",
    "    \"high_school_government_and_politics\",\n",
    "    \"college_chemistry\",\n",
    "    \"logical_fallacies\",\n",
    "    \"high_school_geography\",\n",
    "    \"elementary_mathematics\",\n",
    "    \"college_mathematics\",\n",
    "    \"high_school_psychology\",\n",
    "    \"formal_logic\",\n",
    "    \"high_school_statistics\",\n",
    "    \"international_law\",\n",
    "    \"high_school_mathematics\",\n",
    "    \"high_school_computer_science\",\n",
    "    \"conceptual_physics\",\n",
    "    \"miscellaneous\",\n",
    "    \"high_school_chemistry\",\n",
    "    \"marketing\",\n",
    "    \"professional_law\",\n",
    "    \"management\",\n",
    "    \"college_physics\",\n",
    "    \"jurisprudence\",\n",
    "    \"world_religions\",\n",
    "    \"sociology\",\n",
    "    \"us_foreign_policy\",\n",
    "    \"high_school_macroeconomics\",\n",
    "    \"computer_security\",\n",
    "    \"electrical_engineering\",\n",
    "    \"astronomy\",\n",
    "    \"college_biology\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 945,
   "id": "64f79a2f-cd7b-4cb8-984a-48555b02191c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 945,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mmlu_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 966,
   "id": "f1ddb7de-9b60-4209-be90-c21b2c702ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmlu_questions = []\n",
    "mmlu_correct = []\n",
    "mmlu_incorrect = []\n",
    "mmlu_sources = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 967,
   "id": "70666769-ebd8-4b15-b62b-6b930ef15d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_answers(example):\n",
    "    correct_answer_key = example['target']\n",
    "    correct_answer = example[correct_answer_key]\n",
    "\n",
    "    all_keys = ['A', 'B', 'C', 'D']\n",
    "    all_keys.remove(correct_answer_key)\n",
    "    \n",
    "    incorrect_answer_key = random.choice(all_keys)\n",
    "    incorrect_answer = example[incorrect_answer_key]\n",
    "    \n",
    "    return {'correct': correct_answer, 'incorrect': incorrect_answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 968,
   "id": "0fe6cf66-93ed-4dbc-9498-1d6564a41390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c4b0752e67e4eb2acc483c87196e8ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccfc3f9289044dba9974c95bbbfc5885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/311 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "705fd3a4feb2477bb74286c3bdad6387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/34 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20f611592f0a40068f6c117bc57228cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "888da2c90685414f8c807709e418f54b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d50534d3f9141bba7b6a4ccea801247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/272 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d32cea82cd36421f9bd6146ffba3fe63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/31 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b55893f8e2604fc298d571f73bf89ada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eff20da7fde4afa8242ba54b645882a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f768e0d0ab94922bd383512d6c26e42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/306 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e2e12598dcf43fca0321ed0a0cbf63f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/33 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bebaaf7cd0d4c0184e99e5667658b0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "858075d9a22841439a847e927f766fcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a62edd57dec44e36818bc259e51da86e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6466525fe9374fe481a9022d20e7a581",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34e0f11aee8646c6ac03b03171ae8f0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54e5912f49784b65a82d7cbdcfb04831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6100cf6115bd4ababef4472c85d4c8a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/112 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80470c19c5b9493fb2b5c48b813fa33d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/11 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ec547124a764a52a45d15102b8df92e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18b8c83710b74ac7ba545b125cb13c77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16c80f9e6c84474ebbec167f78509e01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/245 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f42695f9786f41c89cdf84168ac0adfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/27 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92c8c50b50f647cd9bb1a392acb4fe6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b34904902464c00a08719b241dc6d77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98c677ad8ea344c58cae1a7ef6ea1097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/110 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf2af7cc5732426a9f7d82585a0b1997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/12 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36f5628f6d48446ab33a8fdc78d8889a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8f3f802c223466289de8dab6207b28e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84cb876787034a2aaa7e543df71f2bea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/612 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f43febea0fd0438688675b3f7adee243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/69 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d51588d65bfb4c64a63e8844c78aa41c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8725b773da64173a881f3bf289797ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd3c38794c4c4b448f48b31d08298e63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/324 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cb23f1e29c24db79a399a2baef944ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/35 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b5872957bbc463284e92875d7bfeaf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "637de05dbbd447feb478cebd135967c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5ac16d702ad480296b94beee12415d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/135 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a504915dd0914f3ebec166f0de519ee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/14 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a57e550c3534224bae1eda7ed2c05c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccbd69d10b674c3ca3d45fdeab48f0aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0e014aa13644011b7d1a8a071cf04a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/173 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dad14eeb07649a5a6d485ffa1aa195b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/22 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9a6dc72f9cc445dbc6253c6a1f6b2a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70a7e655dc71408992c4cc47ccf81290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f1e227c6b1b49ac8378d4ab817b26cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/193 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5863c4ef6dd74c96a70722a5cd96b25a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/21 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "549d2725abcb459299a8da699a355180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "057f372925bc426e874ce5777209d0e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b6243bc3d7b420c818ffc81556ce9b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7e8f6c0ca3945d38597894381c1c843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4180f354b26843a7903ffd547c420c17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c956f86f3d59405ba3d1fc1eb62c0c09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "541c11018a0046d4b161e9d16e7cc6db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/163 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9fa4173f0614567a939799758504b49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a927b528ba241a6992afd1cff347d10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1a1bbe52a99412d9c828f6d63a6ad21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5320426252aa4d90aab290adbf9ea4e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/198 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6616484b7234e01a9f710d46c9398ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/22 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdd3eb5a7c8a40779c3c743dc342899f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e240965c09ba4067bb484bc48caba207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "386d59c75e5b46cca66abd9baaf4870a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/378 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1865194edf84803a7e5576b3be9a1f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/41 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33aaee54bfbd44f79629f26209ed1290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c37940e363e24678908fe0e1b0b8ba8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f1e8d00aaa4f21b84f5f6e2ad33182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fc3573d785b44278aec713c1ff90eeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/11 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d14fc92dedb4e719315d14eabd92933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c6232d6da804f418d852ee594f4f9f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "731017129de2421586be3075431c9609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/545 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0f89c9019f446a4926a13b7b47dae9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/60 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b43676eab92447ca43ef402086fbfcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d6f4b70747b432b9ec0cafd084a24aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecf73af4930a4cfb8b6724bb3e243033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/126 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3e501ae4e61429db22754388d43f557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/14 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8bf5a8935a4445bb21461521f6f2a10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "670392802ba84af28c3e96743dc11a3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b38b5c0c44854550855853409039d9d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/216 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5b5b9c04b964e8d96704ca6a2774c1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/23 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a3a58744277450a996829a5b93d9c62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3d0f4eb43ef488384a7c8c94eb5412d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8462ff3d7e0b40b2a154f2aa99cc95a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/121 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fefa450f2899482c9326cee6d3c0a4a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/13 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61066eacd66b4ecd960f8ea15d50b111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfcced6e5a104cb785254859f8b061b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a9bbe4767cd4af1b9cd83f50945904a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/270 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13f3defeac6641db8f838c064b21fe56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/29 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "563f90af6f2248fc83801dcb5405d986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "884dd8b8e1584492b023b679ae431d25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2994eacc00dc4caa9070c45d35998f0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1985f0b47c84eecb789bc0f37018f20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/9 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70770c41b9174b35b280b3079013b4ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c47ab0eae2a54caeb14102c22a9eb593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4a20e5b557149cf959806d2f30b7dc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/235 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be29baeb4ffe40eea5f4323532685287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/26 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f518a2a36ff40df8146e662863966a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8349432d9d744fc89060d378b96a1a28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c75d6a117d24ed28e7ba5838fad7af0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/783 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45f84f351c8f42aa944363360fb76ee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/86 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "505aa42bc2724fb68b6ee41d6141bfa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e530c99d81084b97bd381c4c07801f0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48c49fab5b084de19d7df5b12b60c348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/203 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f99c26c738da42afadc33b1e07a93891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/22 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffe534718d9e432aa9e531afddacdc18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51b5dfd608c8441ca403c2c7cb1c7794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a53c510d8f943098e85fb3998acab9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/234 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "609091fdb9184a898b36c1c13e289bbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/25 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ea3dcc865c44b44ad2339c97cffd8c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f77fffc80e904879b243caafb7012a77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6094523408924fafa305c3bc6c7dcf22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1534 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bec5615ee26469589aeed0eb6db2aa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/170 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1716fb616ea1458f882d39ce40a01f03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b99d8219bf141e4abb96f2ab3cfe57e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbbbcf40504849d7985cbd1384789d16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/103 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac642a241d484f55aad7d8f544f304f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/11 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "042b4eca0a02406ab009c84f559ad13e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fdcfe79698d4343b430eec5c526c830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e05cfcee85c641c6925d6e151764af45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/102 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b1435a894824079af08bee42af0fb41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/11 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a7335b5bb6f43b180182c1c83075959",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f08585a0aa68427cbde18466bce62a4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23e5a35ca040438596f0c30d8977b920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/108 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da28e684363f4ebea4bd6ab33f71e7ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/11 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94d08f11811748b08089f16bd4807cde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59639a5d4ef1477cb0d6531958334564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e98909a99644f33942f39a4bbe7accf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/171 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc380fa578a34f06954ebe5cb718ee92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/19 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74c5361890ae4150a2223bc7aa1fa338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11c6b653856b42b6a8ed8f2490bef79a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6710f86245a249ff869f7e449536da89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/201 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72ad986ab73145a4ae4a20127ad284dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/22 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15873000c91647c884957b9fe9ac08a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd0708baa2964d569ed6c886e26471f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ef8bf04fbbb40b98721407368b3bcdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f810d52a4ca4aaea2c75199aedda6cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/11 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1926dac06ab546cbbcadd494ad111173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7728ce77b70743018df8f7d50753e539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c817789a4834ba5bda0e775bb85ea63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/390 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "875403931c944037891d6f9e90370711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/43 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a79b9824a52a486b98984319a4ea5b66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "786d714f62a944dead2ed23b8b38b8f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c6d38b30b2e4677baa0d59877069118",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d9fcde0e29d42c1b926973a3b62445c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/11 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acb9368df35b4e66aaa169037e483c22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94d7818257804cd6be8150cb1e37648d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8941c37f04aa4ffcbc09a8068eeace46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/145 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ac489ff5a87453ab64f60ba9eb7cc29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/16 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ae67a25edff4514ae1528e113e8ea7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed32faebb544401c82778376540efb59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93fe146fa03a4fa48ddf33c0f89a2f3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/152 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f3347c7279949799c315890c4c8b4d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/16 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f789b5f800544078a1e6f2e9eca520b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a667f0cf1cd41a08dc0548717097993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "034e7c03fcc7433c9e62be6662830426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/144 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6108fde733244f61a21cca3a1db03e46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/16 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81278a1f8dee4f488040d6743a744edc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ddbb9a303aa405b800e6b7578b654a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for c in tqdm(mmlu_categories): \n",
    "    dataset = load_dataset('lukaemon/mmlu', name=c, split=\"train\", trust_remote_code=True)\n",
    "    shuffled_dataset = dataset.shuffle(seed=42)\n",
    "    sampled_questions = shuffled_dataset.select(range(4))\n",
    "            \n",
    "    sampled_questions = sampled_questions.map(get_answers)\n",
    "\n",
    "    mmlu_questions.extend(sampled_questions[\"input\"])\n",
    "    mmlu_correct.extend(sampled_questions[\"correct\"])\n",
    "    mmlu_incorrect.extend(sampled_questions[\"incorrect\"])\n",
    "    mmlu_sources.extend([c]*sampled_questions.num_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 973,
   "id": "9d61f6ce-7e3a-4e2d-ad89-a5c78124c226",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmlu_difficulty = [None for _ in range(len(mmlu_questions))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 974,
   "id": "645f21b9-b391-490e-876b-07b773dfdf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmlu_dataset = {\"question\": mmlu_questions, \n",
    "                     \"correct\": mmlu_correct,\n",
    "                     \"incorrect\": mmlu_incorrect, \n",
    "                     \"MCQ_id\": mmlu_difficulty,\n",
    "                     \"source\": mmlu_sources, \n",
    "                     \"difficulty\": mmlu_difficulty}\n",
    "mmlu_dataset = pd.DataFrame(mmlu_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 977,
   "id": "150d7c77-9ab7-484e-92e9-a66b13e88c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>correct</th>\n",
       "      <th>incorrect</th>\n",
       "      <th>MCQ_id</th>\n",
       "      <th>source</th>\n",
       "      <th>difficulty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>The study of reality in the broadest sense, an inquiry into the elemental nature of the universe and the things in it, is known as _____.</td>\n",
       "      <td>metaphysics</td>\n",
       "      <td>axiology</td>\n",
       "      <td>None</td>\n",
       "      <td>philosophy</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>According to d'Holbach, people always act according to _____.</td>\n",
       "      <td>necessary natural laws</td>\n",
       "      <td>free choices</td>\n",
       "      <td>None</td>\n",
       "      <td>philosophy</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Before Tolstoy's Christian conversion, what was his perspective on the meaning of life?</td>\n",
       "      <td>pessimist</td>\n",
       "      <td>optimist</td>\n",
       "      <td>None</td>\n",
       "      <td>philosophy</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>According to Moore‚Äôs ‚Äúideal utilitarianism,‚Äù the right action is the one that brings about the greatest amount of:</td>\n",
       "      <td>good.</td>\n",
       "      <td>virtue.</td>\n",
       "      <td>None</td>\n",
       "      <td>philosophy</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                     question                 correct     incorrect MCQ_id      source difficulty\n",
       "44  The study of reality in the broadest sense, an inquiry into the elemental nature of the universe and the things in it, is known as _____.             metaphysics      axiology   None  philosophy       None\n",
       "45                                                                              According to d'Holbach, people always act according to _____.  necessary natural laws  free choices   None  philosophy       None\n",
       "46                                                    Before Tolstoy's Christian conversion, what was his perspective on the meaning of life?               pessimist      optimist   None  philosophy       None\n",
       "47                         According to Moore‚Äôs ‚Äúideal utilitarianism,‚Äù the right action is the one that brings about the greatest amount of:                   good.       virtue.   None  philosophy       None"
      ]
     },
     "execution_count": 977,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmlu_dataset[mmlu_dataset[\"source\"]==\"philosophy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 978,
   "id": "736dd84a-d846-4bc1-b89a-fa4b743cc26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmlu_dataset.to_csv(\"mmlu_filtered.csv\", index=False)\n",
    "# TODO: ADD IN THE DIFFICULTIES FOR MMLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7da49aa-ea5c-428c-9144-478a71c811fb",
   "metadata": {},
   "source": [
    "### QuAIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "572c1d49-f821-4b94-9394-d54dbb973cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "quail_dataset = pd.read_json(\"quail.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "f882f7a9-1eff-41ae-b3e5-08707f4d4cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "quail_dataset = quail_dataset.sample(110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "f27b3fbc-0e62-4e4d-9eb2-105fe57ec9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_question(row):\n",
    "    return f\"Consider the following passage:\\n\\n{row['context']}\\n\\n{row['question']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "39f20c6f-85ae-4c24-8f2e-743afa3c252d",
   "metadata": {},
   "outputs": [],
   "source": [
    "quail_dataset[\"question\"] = quail_dataset.apply(get_question, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "276a5bce-9f61-4932-ad19-cee4eb6636f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct_incorrect(row):\n",
    "    correct_index = int(row[\"correct_answer_id\"])\n",
    "    correct = row[\"answers\"][correct_index]\n",
    "\n",
    "    row[\"answers\"].pop(correct_index)\n",
    "\n",
    "    incorrect = random.choice(row[\"answers\"])\n",
    "\n",
    "    return correct, incorrect\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "8cf3d02f-41dd-46b2-8d46-9286c86525fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "quail_dataset[[\"correct\", \"incorrect\"]] = quail_dataset.apply(get_correct_incorrect, axis=1, result_type=\"expand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "249d2123-a350-41d2-82d3-f5628aa65340",
   "metadata": {},
   "outputs": [],
   "source": [
    "quail_dataset[\"source\"] = \"QuAIL\"\n",
    "quail_dataset[\"difficulty\"] = None\n",
    "quail_dataset[\"MCQ_id\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "05e18c24-31d0-40a8-a049-c10a0bd3e7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "quail_dataset = quail_dataset[[\"question\", \"correct\", \"incorrect\", \"MCQ_id\", \"source\", \"difficulty\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "a450f231-957f-4f59-b332-cef11f90e4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "quail_dataset.to_csv(\"quail_filtered.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cacd50-2d7e-4b0a-ba01-7848c0228365",
   "metadata": {},
   "source": [
    "### Bias dataset\n",
    "https://sites.google.com/view/hbiproject/hbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "29e2e977-ad71-423a-bf75-dede72a145df",
   "metadata": {},
   "outputs": [],
   "source": [
    "biases = pd.read_csv(\"TRUE_bias.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "2f0d8cf4-64c4-4f63-9028-8e469f087859",
   "metadata": {},
   "outputs": [],
   "source": [
    "biases = biases[biases[\"objective_question\"]==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e5f76e67-57cc-4d81-8264-65aa85fe1571",
   "metadata": {},
   "outputs": [],
   "source": [
    "biases[\"Questions\"] = biases[\"Questions\"].str.strip()\n",
    "biases[\"choice1\"] = biases[\"choice1\"].str.strip()\n",
    "biases[\"choice2\"] = biases[\"choice2\"].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4008bfa2-2ef3-4817-a193-7bd7a768cb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "biases = biases.drop(48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "58a83e34-0cee-4b18-b9f2-f9c32d429bea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Questions</th>\n",
       "      <th>choice1</th>\n",
       "      <th>choice2</th>\n",
       "      <th>category</th>\n",
       "      <th>correct</th>\n",
       "      <th>other_info</th>\n",
       "      <th>objective_question</th>\n",
       "      <th>o1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0, Questions, choice1, choice2, category, correct, other_info, objective_question, o1]\n",
       "Index: []"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases[biases[\"choice1\"]==biases[\"choice2\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4bed6345-b39d-48c0-868e-fd49189eb32d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(155, 9)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "12b65550-c508-49cf-96c2-052f2cd74a3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "Confirmation bias                        15\n",
       "Anchor Bias                              10\n",
       "Illusion of control                       5\n",
       "Sunk cost fallacy                         5\n",
       "Status quo                                5\n",
       "Proportion dominance                      5\n",
       "Probability neglect                       5\n",
       "Probability matching (vs. maximizing)     5\n",
       "Overconfidence                            5\n",
       "Outcome bias                              5\n",
       "Omission bias                             5\n",
       "Myside bias                               5\n",
       "Mental accounting                         5\n",
       "Loss aversion                             5\n",
       "Framing                                   5\n",
       "Attribution Bias                          5\n",
       "Covariation detection                     5\n",
       "better-than-average bias                  5\n",
       "Base-rate neglect (causal)                5\n",
       "Irrational diversification                5\n",
       "Insensitivity to sample size              5\n",
       "Gambler's fallacy                         5\n",
       "Denominator Neglect                       5\n",
       "Conjunction Fallacy                       5\n",
       "Belief bias                               5\n",
       "Base-rate neglect (statistical)           5\n",
       "Availability Heuristic                    5\n",
       "Temporal discounting                      5\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases[\"category\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "35010a86-0773-4a8a-bfbc-da1f2e857435",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION: Mount Wycheproof is about 114 meters in height. How tall do you think Mount Kenya in the Great Rift Valley is?\n",
      "CHOICE1: Mount Kenya is about 510m tall.\n",
      "CHOICE1: Mount Kenya is about 5100 meters tall.\n",
      "ANSWER: choice2\n",
      "CATEGORY: Anchor Bias\n",
      "\n",
      "QUESTION: Mount Everest is about 8,849 meters in height. What is the height of Mount Stanley in the Rwenzori?\n",
      "CHOICE1: Mount Stanley is about 5100 meters tall.\n",
      "CHOICE1: The height of Mount Stanley is approximately 9690 meters.\n",
      "ANSWER: choice1\n",
      "CATEGORY: Anchor Bias\n",
      "\n",
      "QUESTION: Tennis star Ashlyn Krueger has played about 200 singles matches in her career. How many singles matches has John McEnroe played in total?\n",
      "CHOICE1: He played 1080 singles matches in his career.\n",
      "CHOICE1: He played 500 singles matches in his career.\n",
      "ANSWER: choice1\n",
      "CATEGORY: Anchor Bias\n",
      "\n",
      "QUESTION: Roger Federer has played 1526 singles matches in his career. What is the total number of singles matches played by Stefan Edberg?\n",
      "CHOICE1: He played 1800 singles matches in his career.\n",
      "CHOICE1: He played 1080 singles matches in his career.\n",
      "ANSWER: choice2\n",
      "CATEGORY: Anchor Bias\n",
      "\n",
      "QUESTION: Rhode Island has 39 municipalities. How many municipalities does Colorado have?\n",
      "CHOICE1: Colorado has 60 municipalities.\n",
      "CHOICE1: Colorado has about 273 municipalities.\n",
      "ANSWER: choice2\n",
      "CATEGORY: Anchor Bias\n",
      "\n",
      "QUESTION: California has 482 municipalities. How many municipalities does South Carolina have?\n",
      "CHOICE1: South Carolina has 271 municipalities.\n",
      "CHOICE1: South Carolina has 401 municipalities.\n",
      "ANSWER: choice1\n",
      "CATEGORY: Anchor Bias\n",
      "\n",
      "QUESTION: India won 6 medals at the 2012 London Olympics. How many medals did Japan win at the same Olympics?\n",
      "CHOICE1: Japan won 35 medals at the 2012 Olympics.\n",
      "CHOICE1: Japan won 7 medals at the 2012 Olympics.\n",
      "ANSWER: choice1\n",
      "CATEGORY: Anchor Bias\n",
      "\n",
      "QUESTION: China won 91 medals at the 2012 London Olympics. How many medals did Australia win at the same Olympics?\n",
      "CHOICE1: Australia won 35 medals at the 2012 Olympics.\n",
      "CHOICE1: Australia won 67 medals at the 2012 London Olympics.\n",
      "ANSWER: choice1\n",
      "CATEGORY: Anchor Bias\n",
      "\n",
      "QUESTION: The Statue of Liberty is 93 meters tall. What is the height of the ‚Äú432 Park Avenue‚Äù building in New York (in meters)?\n",
      "CHOICE1: The building is 425 meters tall.\n",
      "CHOICE1: The building is 150 meters tall.\n",
      "ANSWER: choice1\n",
      "CATEGORY: Anchor Bias\n",
      "\n",
      "QUESTION: The World Trade Center is 541 meters tall. What is the height of the ‚ÄúOne Vanderbilt‚Äù skyscraper  in New York (in meters)?\n",
      "CHOICE1: The building is 500 meters tall.\n",
      "CHOICE1: The building is 425 meters tall.\n",
      "ANSWER: choice2\n",
      "CATEGORY: Anchor Bias\n",
      "\n",
      "QUESTION: Why do many historians believe the Great Depression began?\n",
      "CHOICE1: Herbert Hoover's ineffective leadership and policies exacerbated the economic downturn\n",
      "CHOICE1: Uncontrollable global economic factors post-World War I led to the economic collapse.\n",
      "ANSWER: choice2\n",
      "CATEGORY: Attribution Bias\n",
      "\n",
      "QUESTION: What is often cited as a primary cause of the fall of the Roman Empire?\n",
      "CHOICE1: The incompetence of later emperors, leading to poor governance and decline.\n",
      "CHOICE1: Overexpansion and invasions by barbarian tribes, which the empire could no longer manage.\n",
      "ANSWER: choice2\n",
      "CATEGORY: Attribution Bias\n",
      "\n",
      "QUESTION: What led to the defeat of Napoleon at Waterloo?\n",
      "CHOICE1: Napoleon's arrogance and miscalculations during the battle.\n",
      "CHOICE1: The superior numbers and coordination of the allied forces of Britain and Prussia.\n",
      "ANSWER: choice2\n",
      "CATEGORY: Attribution Bias\n",
      "\n",
      "QUESTION: Why did the Asian Financial Crisis occur in 1997?\n",
      "CHOICE1: The personal greed and corruption of Asian financial leaders led to irresponsible financial practices.\n",
      "CHOICE1: A combination of excessive foreign debt, currency speculation, and sudden changes in investor confidence affected the economies.\n",
      "ANSWER: choice2\n",
      "CATEGORY: Attribution Bias\n",
      "\n",
      "QUESTION: What was the primary cause of the housing market crash in 2008 in the United States?\n",
      "CHOICE1: The unethical behavior of mortgage lenders pushing subprime loans.\n",
      "CHOICE1: A complex interplay of high-risk lending practices, inadequate regulation, and flawed financial modeling leading to market instability.\n",
      "ANSWER: choice2\n",
      "CATEGORY: Attribution Bias\n",
      "\n",
      "QUESTION: Which cause of death is more likely? Suicide or diabetes?\n",
      "CHOICE1: Suicide is a more likely cause of death than diabetes.\n",
      "CHOICE1: Diabetes causes more deaths than suicide.\n",
      "ANSWER: choice2\n",
      "CATEGORY: Availability Heuristic\n",
      "\n",
      "QUESTION: Which cause of death is more likely? Homicide or diabetes?\n",
      "CHOICE1: Homocide causes more deaths than diabetes.\n",
      "CHOICE1: Diabetes is a bigger cause of death than homocide.\n",
      "ANSWER: choice2\n",
      "CATEGORY: Availability Heuristic\n",
      "\n",
      "QUESTION: Which cause of death is more likely? A commercial airplane crash or a bicycle-related accident?\n",
      "CHOICE1: Commercial plane crashes are more likely than bike accidents. Thus, they cause more deaths.\n",
      "CHOICE1: Bike accidents cause more deaths than plane crashes.\n",
      "ANSWER: choice2\n",
      "CATEGORY: Availability Heuristic\n",
      "\n",
      "QUESTION: Which cause of death is more likely? Shark attacks or hornet, wasp or bee bites?\n",
      "CHOICE1: Shark attacks kill more people than hornet, wasp, or bee bites.\n",
      "CHOICE1: Hornet, wasp, or bee bites cause more deaths than shark attacks.\n",
      "ANSWER: choice2\n",
      "CATEGORY: Availability Heuristic\n",
      "\n",
      "QUESTION: Which cause of death is more likely? Tornadoes or asthma?\n",
      "CHOICE1: Tornadoes are a more likely cause of death.\n",
      "CHOICE1: Asthmatic attacks are more likely causes of death.\n",
      "ANSWER: choice2\n",
      "CATEGORY: Availability Heuristic\n",
      "\n",
      "QUESTION: Among the 1000 people that participated in a study, there were 995 nurses and 5 doctors. John is randomly chosen participant in this research. He is 34 years old, and he lives in a nice house in a fancy neighborhood. He also expresses himself nicely and is very interested in politics. He invests a lot of time in his career. Is John more likely to be a nurse or a doctor?\n",
      "CHOICE1: John is a nurse.\n",
      "CHOICE1: John is a doctor.\n",
      "ANSWER: choice1\n",
      "CATEGORY: Base-rate neglect (statistical)\n",
      "\n",
      "QUESTION: Among the 1000 people that participated in a study, there were 100 engineers and 900 lawyers. George is randomly chosen participant in this research. George is 36 years old. He is not married and is somewhat introverted. He likes to spend his free time reading science fiction and developing computer programs. Is George more likely to be an engineer or a lawyer?\n",
      "CHOICE1: George is an engineer.\n",
      "CHOICE1: George is a lawyer.\n",
      "ANSWER: choice2\n",
      "CATEGORY: Base-rate neglect (statistical)\n",
      "\n",
      "QUESTION: Among the 1000 people that participated in a study, there were 50 16-year-olds and 950 50-year-olds. Helen is randomly chosen participant in this research. Helen listens to hip hop and rap music. She likes to wear tight T-shirts and jeans. She loves to dance and has a small nose piercing. Is Helen more likely to be 16 years old or 50 years old?\n",
      "CHOICE1: Helen is 16 years old.\n",
      "CHOICE1: Helen is 50 years old.\n",
      "ANSWER: choice2\n",
      "CATEGORY: Base-rate neglect (statistical)\n",
      "\n",
      "QUESTION: Among the 1000 people that participated in a study, there were 70 people whose favorite movie was \"Star Wars\" and 930 people whose favorite movie was \"Love Actually\". Nikola is randomly chosen from the participants in this research. Nikola is 26 years old and is studying physics. He stays at home most of the time and loves to play video games. Is it more likely that Nikola likes \"Star Wars\" or \"Love Actually\"?\n",
      "CHOICE1: Nikola's favorite movie is \"Star Wars\".\n",
      "CHOICE1: Nikola's favorite movie is \"Love Actually\"\n",
      "ANSWER: choice2\n",
      "CATEGORY: Base-rate neglect (statistical)\n",
      "\n",
      "QUESTION: The graduating class consists of 85% art majors and 15% engineering majors. You know nothing about a randomly sampled student, Steve, except that they are logical, analytical, and good at math. Is Steve more likely to be an engineering major or an arts major?\n",
      "CHOICE1: Steve is an engineering major.\n",
      "CHOICE1: Steve is an arts major.\n",
      "ANSWER: choice2\n",
      "CATEGORY: Base-rate neglect (statistical)\n",
      "\n",
      "QUESTION: Does the conclusion follow from the premises?‚Ä®",
      "‚Ä®",
      "Premises:\n",
      "All mammals are animals.\n",
      "All dogs are mammals.\n",
      "Conclusion:\n",
      "Therefore, all dogs are animals.\n",
      "CHOICE1: The conclusion logically follows from the premises (the argument is valid).\n",
      "CHOICE1: The conclusion does not logically follow from the premises (the argument is invalid).\n",
      "ANSWER: choice1\n",
      "CATEGORY: Belief bias\n",
      "\n",
      "QUESTION: Does the conclusion follow from the premises?‚Ä®",
      "‚Ä®",
      "Premises:\n",
      "All fruits have seeds.\n",
      "Tomatoes have seeds.\n",
      "Conclusion:\n",
      "Therefore, tomatoes are fruits.\n",
      "CHOICE1: The conclusion logically follows from the premises (the argument is valid).\n",
      "CHOICE1: The conclusion does not logically follow from the premises (the argument is invalid).\n",
      "ANSWER: choice2\n",
      "CATEGORY: Belief bias\n",
      "\n",
      "QUESTION: Does the conclusion follow from the premises?‚Ä®",
      "‚Ä®",
      "Premises:\n",
      "No addictive things are inexpensive.\n",
      "Some cigarettes are inexpensive.\n",
      "Conclusion:\n",
      "Therefore, some cigarettes are not addictive.\n",
      "CHOICE1: The conclusion logically follows from the premises (the argument is valid).\n",
      "CHOICE1: The conclusion does not logically follow from the premises (the argument is invalid).\n",
      "ANSWER: choice1\n",
      "CATEGORY: Belief bias\n",
      "\n",
      "QUESTION: Does the conclusion follow from the premises?‚Ä®",
      "‚Ä®",
      "Premises:\n",
      "All birds can fly.\n",
      "Penguins are birds.\n",
      "Conclusion:\n",
      "Therefore, penguins can fly.\n",
      "CHOICE1: The conclusion logically follows from the premises (the argument is valid).\n",
      "CHOICE1: The conclusion does not logically follow from the premises (the argument is invalid).\n",
      "ANSWER: choice1\n",
      "CATEGORY: Belief bias\n",
      "\n",
      "QUESTION: Does the conclusion follow from the premises?‚Ä®",
      "‚Ä®",
      "Premises:\n",
      "All flowers are plants.\n",
      "Some plants do not need sunlight.\n",
      "Conclusion:\n",
      "Therefore, some flowers do not need sunlight.\n",
      "CHOICE1: The conclusion logically follows from the premises (the argument is valid).\n",
      "CHOICE1: The conclusion does not logically follow from the premises (the argument is invalid).\n",
      "ANSWER: choice2\n",
      "CATEGORY: Belief bias\n",
      "\n",
      "QUESTION: You have four cards. Each card has a number on one side and a letter on the other side. The cards that you can see are the following: \"D\", \"7\", \"5\", \"K\". Which card(s) must be turned over to decide whether the following rule is true or false: \"If a card has a 'D' on one side, then it has a '5' on the other side\"?\n",
      "CHOICE1: You must flip over the card with a \"7\" on it.\n",
      "CHOICE1: You must flip over the card with a \"5\" on it.\n",
      "ANSWER: choice1\n",
      "CATEGORY: Confirmation bias\n",
      "\n",
      "QUESTION: You are shown four cards, each of which has a number on one side and a color on the other side. The cards that you see are the following: \"3\", \"8\", \"red\", \"orange\". Which card(s) must be turned over to decide whether the following rule is true or false: \"If a card has an even number on one side, then its opposite side is red\"?\n",
      "CHOICE1: You must flip over the orange card.\n",
      "CHOICE1: You must flip over the red card.\n",
      "ANSWER: choice1\n",
      "CATEGORY: Confirmation bias\n",
      "\n",
      "QUESTION: You are shown four envelopes, each of which is sealed or not and bears a stamp. You are shown the following 4 envelopes: \"sealed\", \"unsealed\", \"50-cent stamp\", \"40-cent stamp\". Which envelope(s) must be turned over to decide whether the following rule is true or false: \"If an envelope is sealed, then it has a 50-cent stamp on it\"?\n",
      "CHOICE1: You must flip over the envelope with the 50-cent stamp on it.\n",
      "CHOICE1: You must flip over the envelope with the 40-cent stamp on it.\n",
      "ANSWER: choice2\n",
      "CATEGORY: Confirmation bias\n",
      "\n",
      "QUESTION: You are shown four student exams, each of which has a score out of 20 on the front and a comment on the back. You see the following exams: \"18\", \"Good\", \"Excellent\", \"15\". Which exam(s) must be turned over to decide whether the following rule is true or false: \"If an exam has a score greater than 16, then it has the comment 'Excellent'\"?\n",
      "CHOICE1: The exam with \"Excellent\" written on it.\n",
      "CHOICE1: The exam with \"Good\" written on it.\n",
      "ANSWER: choice2\n",
      "CATEGORY: Confirmation bias\n",
      "\n",
      "QUESTION: You are shown four cards, each of which has a number on one side and a letter on the other side. You are shown the following cards: \"A\", \"B\", \"4\", and \"5\". Which cards must you turn over in order to determine if the following rule is true: \"If a card has a vowel on one side, then it has an even number on the other side.\"?\n",
      "CHOICE1: You must turn over cards with \"A\" and \"4\" on it.\n",
      "CHOICE1: You must turn over cards with \"A\" and \"5\" on it.\n",
      "ANSWER: choice2\n",
      "CATEGORY: Confirmation bias\n",
      "\n",
      "QUESTION: You start a game in which your opponent has set a rule generating sequences of three numbers. Your goal is to find out this rule. For that purpose, you put forward sequences of numbers and your opponent tells you whether or not each sequence complies with the rule. Your opponent starts by telling you that the sequence 2, 4, 6 complies with the rule. You think the rule is \"add 2\". Which of the following sequences should you query about?\n",
      "CHOICE1: 8, 10, 12\n",
      "CHOICE1: 3, 6, 9\n",
      "ANSWER: choice2\n",
      "CATEGORY: Confirmation bias\n",
      "\n",
      "QUESTION: You start a game in which your opponent has set a rule generating sequences of three numbers. She starts by telling you that the sequence 6-8-10 complies with the rule. You think the rule is \"a sequence of even numbers\". Which of the following sequences should you query about?\n",
      "CHOICE1: 3, 5, 7\n",
      "CHOICE1: 2, 4, 6\n",
      "ANSWER: choice1\n",
      "CATEGORY: Confirmation bias\n",
      "\n",
      "QUESTION: You start a game in which your opponent has set a rule generating sequences of three numbers. She starts by telling you that the sequence 7-8-9 complies with the rule. You think the rule is \"three numbers in ascending order\". Which of the following sequences should you query about?\n",
      "CHOICE1: 1, 2, 3\n",
      "CHOICE1: 6, 4, 2\n",
      "ANSWER: choice2\n",
      "CATEGORY: Confirmation bias\n",
      "\n",
      "QUESTION: You're testing the rule that \"All sequences must contain increasing multiples of three.\" You see the initial sequence: 3, 6, 9. Which sequence should you suggest to best determine if your hypothesis about the rule is correct?\n",
      "CHOICE1: 12, 15, 18\n",
      "CHOICE1: 2, 4, 6\n",
      "ANSWER: choice2\n",
      "CATEGORY: Confirmation bias\n",
      "\n",
      "QUESTION: You see the following example sequence is: 2, 3, 5. The rule you are testing is \"All sequences must consist of consecutive prime numbers.\" Which sequence should you propose to effectively test whether your hypothesis about the rule is correct?\n",
      "CHOICE1: 7, 11, 13\n",
      "CHOICE1: 4, 6, 8\n",
      "ANSWER: choice2\n",
      "CATEGORY: Confirmation bias\n",
      "\n",
      "QUESTION: Please read the provided personality profile very carefully. I would like you to imagine that you are about to interview a candidate for a job in order to determine the extent to which the candidate matches the personality profile. \n",
      "\n",
      "\"This type of person is typically good-natured, gentle, and cooperative. They are generous, warmhearted, and do not mind giving of their time and energy. They are not prone to jealousy. They will accept blame if things go wrong, rather than putting the blame on others. They are willing to compromise and find ways of cooperating with others, even in difficult situations.\" \n",
      "\n",
      "Which of the following questions would you ask the candidate to make a decision?‚Ä®",
      "Question 1: Tell me about a time you held yourself publicly accountable for a mistake you made?\n",
      "Question 2: Tell me about a time when your quick temper got you in trouble?\n",
      "CHOICE1: Question 1\n",
      "CHOICE1: Question 2\n",
      "ANSWER: choice2\n",
      "CATEGORY: Confirmation bias\n",
      "\n",
      "QUESTION: Please read the provided personality profile very carefully. I would like you to imagine that you are about to interview a candidate for a job in order to determine the extent to which the candidate matches the personality profile. \n",
      "\n",
      "\"This type of person is typically responsible, orderly, and persevering. They can be depended on to accept responsibilities. They do not allow personal benefit or gain to conflict with ethical principles. They are painstaking and thorough, and see a job through in spite of difficulties or temptations. They are hardworking, strong-willed, and sometimes get overinvolved with details.\" \n",
      "\n",
      "Which of the following questions would you ask the candidate to make a decision?‚Ä®",
      "Question 1: How did you feel the last time someone depended on you to do something and you let them down?\n",
      "Question 2: Tell me about a situation where your concern for detail made it difficult to get a project done on time.\n",
      "CHOICE1: Question 1\n",
      "CHOICE1: Question 2\n",
      "ANSWER: choice1\n",
      "CATEGORY: Confirmation bias\n",
      "\n",
      "QUESTION: Please read the provided personality profile very carefully. I would like you to imagine that you are about to interview a candidate for a job in order to determine the extent to which the candidate matches the personality profile. \n",
      "\n",
      "\"This type of person is typically poised, calm, and composed. They rarely seem to get upset or nervous. They go on with what they are doing regardless of distractions. They do not lose their composure when provoked or when in an emergency situation. They perform well under pressure.\" \n",
      "\n",
      "Which of the following questions would you ask the candidate to make a decision?‚Ä®",
      "Question 1: Have you ever been in a situation where you kept your cool while everyone else fell apart?\n",
      "Question 2: What is the greatest source of anxiety for you?\n",
      "CHOICE1: Question 1\n",
      "CHOICE1: Question 2\n",
      "ANSWER: choice2\n",
      "CATEGORY: Confirmation bias\n",
      "\n",
      "QUESTION: Please read the provided personality profile very carefully. I would like you to imagine that you are about to interview a candidate for a job in order to determine the extent to which the candidate matches the personality profile. \n",
      "\n",
      "\"Extroverts are typically outgoing, sociable, energetic, confident, talkative, and enthusiastic. Generally confident and relaxed in social situations, this type of person rarely has trouble making conversation with others. This type of person makes friends quickly and easily and is usually able to make a favorable impression on others. This type of person is usually seen by others as characteristically warm and friendly.\" \n",
      "\n",
      "Which of the following questions would you ask the candidate to make a decision?‚Ä®",
      "Question 1: What events make you feel popular with people?\n",
      "Question 2: Tell me about sometime when you felt left out from some social group. How did you handle these feelings?\n",
      "CHOICE1: Question 1\n",
      "CHOICE1: Question 2\n",
      "ANSWER: choice2\n",
      "CATEGORY: Confirmation bias\n",
      "\n",
      "QUESTION: Please read the provided personality profile very carefully. I would like you to imagine that you are about to interview a candidate for a job in order to determine the extent to which the candidate matches the personality profile. \n",
      "\n",
      "\"Analytical thinkers are known for their systematic approach to problem solving. They are meticulous, logical, and often enjoy working through complex issues. Such individuals are usually detail-oriented, prefer to plan ahead, and are organized. They are often perceived as reserved or introspective, preferring to think things through thoroughly before speaking or acting.\"\n",
      "\n",
      "Which of the following questions would you ask the candidate to make a decision?‚Ä®",
      "Question 1: Can you describe a situation where you had to analyze data or a problem to come up with a solution? What steps did you take?\n",
      "Question 2: Tell me about a time when you had to make a quick decision without much data. How did you handle it?\n",
      "CHOICE1: Question 1\n",
      "CHOICE1: Question 2\n",
      "ANSWER: choice2\n",
      "CATEGORY: Confirmation bias\n",
      "\n",
      "QUESTION: Consider the following scenario: Linda is 31 years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in anti-nuclear efforts. Is Linda more likely to just be a bank teller or be both a bank teller and an active participant in the feminist movement?\n",
      "CHOICE1: Linda is a bank teller.\n",
      "CHOICE1: Linda is a bank teller and is active in the feminist movement.\n",
      "ANSWER: choice1\n",
      "CATEGORY: Conjunction Fallacy\n",
      "\n",
      "QUESTION: Bill is 34 years old. He is intelligent, but unimaginative, compulsive and generally lifeless. In school, he was strong in mathematics but weak in social studies and humanities. Is Bill more likely to just play jazz for a hobby or both play jazz and be an accountant?\n",
      "CHOICE1: Bill plays jazz for a hobby.\n",
      "CHOICE1: Bill is an accountant who plays jazz for a hobby.\n",
      "ANSWER: choice1\n",
      "CATEGORY: Conjunction Fallacy\n",
      "\n",
      "QUESTION: Consider a regular six-sided die with four green faces and two red faces. The die will be rolled 20 times and the sequence of greens (G) and reds (R) will be recorded. Imagine a hypothetical scenario in which you are asked to select one sequence, from a set of three, and you will win $25 if the sequence you chose appears on successive rolls of the die. Which of the following sequences is more likely: GRGRRR or RGRRR?\n",
      "CHOICE1: The following sequence is more likely: GRGRRR\n",
      "CHOICE1: The following sequence is more likely: RGRRR\n",
      "ANSWER: choice2\n",
      "CATEGORY: Conjunction Fallacy\n",
      "\n",
      "QUESTION: Suppose Ivan Lendl reaches the final of a Grand Pix tournament. Is Ivan more likely to just lose the first set or both lose the first set and win the match?\n",
      "CHOICE1: Lendl will lose the first set\n",
      "CHOICE1: Lendl will lose the first set, but win the match\n",
      "ANSWER: choice1\n",
      "CATEGORY: Conjunction Fallacy\n",
      "\n",
      "QUESTION: Because of the Italian Rail‚Äôs new policies aimed at encouraging voyages longer than 100 km, what is more likely to happen: a 5% ridership decline on commuter trains or both a 5% ridership decline on commuter trains and a 10% increase on long distance trains?\n",
      "CHOICE1: The number of passengers will decline by 5% on commuter trains and increase by 10% on long distance trains.\n",
      "CHOICE1: The number of passengers will decline by 5% on commuter trains.\n",
      "ANSWER: choice2\n",
      "CATEGORY: Conjunction Fallacy\n",
      "\n",
      "QUESTION: You are presented with two trays of black and white marbles: a large tray that contains 100 marbles and a small tray that contains 10 marbles. The marbles are spread in a single layer on each tray. You must draw out one marble without looking from either tray. If you draw a black marble, you win $2. Consider a condition in which the small tray contains 1 black marble and 9 white marbles, and the large tray contains 8 black marbles and 92 white marbles. From which tray would you prefer to select a marble?\n",
      "CHOICE1: The small tray\n",
      "CHOICE1: The large tray\n",
      "ANSWER: choice1\n",
      "CATEGORY: Denominator Neglect\n",
      "\n",
      "QUESTION: A game involves drawing cards from two decks to win a prize. The large deck contains 200 cards with 18 winning cards, while the small deck contains 20 cards with 2 winning cards. From which deck would you prefer to draw a card?\n",
      "CHOICE1: The deck with 20 cards\n",
      "CHOICE1: The deck with 200 cards\n",
      "ANSWER: choice1\n",
      "CATEGORY: Denominator Neglect\n",
      "\n",
      "QUESTION: During a promotional event, you can choose between two bins to pick a prize. The first bin contains 300 prizes with 28 being high-value, while the second bin contains 30 prizes with 3 being high-value. From which bin would you prefer to pick a prize?\n",
      "CHOICE1: The small bin\n",
      "CHOICE1: The large bin\n",
      "ANSWER: choice1\n",
      "CATEGORY: Denominator Neglect\n",
      "\n",
      "QUESTION: A raffle offers two types of tickets. The larger pool has 250 tickets with 15 prizes available, and the smaller pool has 25 tickets with 2 prizes available. From which pool would you prefer to purchase a ticket?\n",
      "CHOICE1: The pool with 25 tickets\n",
      "CHOICE1: The pool with 250 tickets\n",
      "ANSWER: choice1\n",
      "CATEGORY: Denominator Neglect\n",
      "\n",
      "QUESTION: You are at a fishing event where you can choose from two different ponds to fish. The larger pond contains 129 fish with 12 being tagged for prizes, and the smaller pond contains 11 fish with 2 tagged for a prize. From which pond would you prefer to fish?\n",
      "CHOICE1: The pond with 11 fish\n",
      "CHOICE1: The pond with 129 fish\n",
      "ANSWER: choice1\n",
      "CATEGORY: Denominator Neglect\n",
      "\n",
      "QUESTION: A coin has been flipped and landed heads five times in a row. What do you think will happen on the next flip?\n",
      "CHOICE1: It is more likely to land tails next because it hasn't come up tails in a while.\n",
      "CHOICE1: It has an equal chance of landing heads or tails.\n",
      "ANSWER: choice2\n",
      "CATEGORY: Gambler's fallacy\n",
      "\n",
      "QUESTION: A roulette wheel has landed on red three times consecutively. What color will likely come up next?\n",
      "CHOICE1: It will likely be black next because red has already appeared several times.\n",
      "CHOICE1: Red and black have an equal chance on each spin, regardless of previous results.\n",
      "ANSWER: choice2\n",
      "CATEGORY: Gambler's fallacy\n",
      "\n",
      "QUESTION: In a lottery draw, the number 25 has been drawn in the last four consecutive games. What are the chances of it appearing in the next draw?\n",
      "CHOICE1: It is less likely to appear again because it has been drawn too frequently already.\n",
      "CHOICE1: It has the same chance of being drawn as any other number in the next game.\n",
      "ANSWER: choice2\n",
      "CATEGORY: Gambler's fallacy\n",
      "\n",
      "QUESTION: A baseball player has hit home runs in his last three at-bats. How likely is he to hit a home run in his next at-bat?\n",
      "CHOICE1: He is less likely to hit a home run again because his luck is bound to run out.\n",
      "CHOICE1: His chances are statistically consistent with his overall home run hitting rate.\n",
      "ANSWER: choice2\n",
      "CATEGORY: Gambler's fallacy\n",
      "\n",
      "QUESTION: A stock has increased in price every day for the past five days. What is likely to happen with the stock's price tomorrow?\n",
      "CHOICE1: The price is more likely to drop, as it cannot keep going up indefinitely.\n",
      "CHOICE1: The future price movement depends on a variety of factors, and past performance does not guarantee future results.\n",
      "ANSWER: choice2\n",
      "CATEGORY: Gambler's fallacy\n",
      "\n",
      "QUESTION: There are two hospitals in a city. In the big hospital, 45 children are born every day, and in the small hospital 15 children are born every day. On average, 50% of the children born are boys, but it varies from day to day. In which hospital do you think that it is most likely that more than 60% of the children born are boys in a specific day?\n",
      "CHOICE1: The big hospital\n",
      "CHOICE1: The small hospital\n",
      "ANSWER: choice2\n",
      "CATEGORY: Insensitivity to sample size\n",
      "\n",
      "QUESTION: As you know, a game of squash can be played either to 9 or to 15 points. Holding all other rules of the game constant, if A is a better player than B, which scoring system will give A a better chance of winning?\n",
      "CHOICE1: The 9-point scoring system\n",
      "CHOICE1: The 15-point scoring system\n",
      "ANSWER: choice2\n",
      "CATEGORY: Insensitivity to sample size\n",
      "\n",
      "QUESTION: Consider two legal trials: In the first trial, a jury of 6 people must reach a unanimous decision. In the second trial, a jury of 12 people must reach a unanimous decision. Which trial do you think is more likely to reach a unanimous decision quickly?\n",
      "CHOICE1: The trial with a 6-person jury\n",
      "CHOICE1: The trial with a 12-person jury\n",
      "ANSWER: choice1\n",
      "CATEGORY: Insensitivity to sample size\n",
      "\n",
      "QUESTION: Two basketball players, Player A and Player B, compete in a free-throw contest. Player A is generally a better free-throw shooter. The contest can be set up in two ways: a shorter round of 10 shots or a longer round of 30 shots. Which setup would give Player B a better chance of winning?\n",
      "CHOICE1: The 10-shot round\n",
      "CHOICE1: The 30-shot round\n",
      "ANSWER: choice1\n",
      "CATEGORY: Insensitivity to sample size\n",
      "\n",
      "QUESTION: Imagine two groups of meteorologists making long-term forecasts for drought conditions in a historically stable climate region. Group A bases their forecast on rainfall data collected intensively over the last 3 years, during which there were significant atypical weather events, including two major droughts. Group B uses a comprehensive dataset collected over the past 30 years, which includes a variety of climatic conditions but with fewer data points collected annually. Which group's forecast do you think will be more likely to accurately predict the occurrence of future droughts in the region?\n",
      "CHOICE1: Group A (3 years of intensive data during atypical conditions)\n",
      "CHOICE1: Group B (30 years of varied climatic data)\n",
      "ANSWER: choice2\n",
      "CATEGORY: Insensitivity to sample size\n",
      "\n",
      "QUESTION: You have $10,000 to invest and are considering two portfolios. Which option do you prefer‚Ä®",
      "‚Ä®",
      "Portfolio A: Invest all $10,000 in a diversified mutual fund that includes stocks, bonds, and real estate, with a historical annual variance of 5%.\n",
      "Portfolio B: Split your $10,000 equally among 10 different tech stocks from the same industry, each with a historical annual variance of 15%.\n",
      "CHOICE1: Portfolio A\n",
      "CHOICE1: Portfolio B\n",
      "ANSWER: choice1\n",
      "CATEGORY: Irrational diversification\n",
      "\n",
      "QUESTION: You can choose between two investment options. Which option do you prefer?\n",
      "\n",
      "Investment A: A single low-risk government bond with a guaranteed annual return of 3% and virtually zero variance.\n",
      "Investment B: An assortment of high-yield corporate bonds from the same industry, offering higher returns but with a historical variance of 10%.\n",
      "CHOICE1: Investment A\n",
      "CHOICE1: Investment B\n",
      "ANSWER: choice1\n",
      "CATEGORY: Irrational diversification\n",
      "\n",
      "QUESTION: Decide between the following investment strategies. Which option do you prefer?‚Ä®",
      "‚Ä®",
      "Strategy A: Investing in a globally diversified index fund with a historical variance of 6%.‚Ä®",
      "Strategy B: Investing equally in five different emerging market funds, each with high growth potential but a historical variance of 20%.\n",
      "CHOICE1: Strategy A\n",
      "CHOICE1: Strategy B\n",
      "ANSWER: choice1\n",
      "CATEGORY: Irrational diversification\n",
      "\n",
      "QUESTION: You are planning for retirement and have two plans to choose from. Which option do you prefer?‚Ä®",
      "‚Ä®",
      "Plan A: A target-date fund that automatically adjusts asset allocation to minimize risk as you approach retirement, with a historical variance of 7%.‚Ä®",
      "Plan B: A self-managed portfolio where you invest equally in various commodities, all of which have high market volatility, resulting in a combined historical variance of 15%.\n",
      "CHOICE1: Plan A\n",
      "CHOICE1: Plan B\n",
      "ANSWER: choice1\n",
      "CATEGORY: Irrational diversification\n",
      "\n",
      "QUESTION: Consider the specified portfolios. Which option do you prefer?‚Ä®",
      "‚Ä®",
      "Portfolio A: A portfolio consisting of blue-chip stocks from different sectors, with a historical variance of 8%.‚Ä®",
      "Portfolio B: A portfolio of 20 stocks from the same booming sector you believe will continue to grow, with a historical variance of 12%.\n",
      "CHOICE1: Portfolio A\n",
      "CHOICE1: Portfolio B\n",
      "ANSWER: choice1\n",
      "CATEGORY: Irrational diversification\n",
      "\n",
      "QUESTION: In a city, there are two taxi companies: Green Taxi, which operates 85% of the taxis, and Blue Taxi, which operates 15% of the taxis. A taxi was involved in a hit-and-run accident at night. A witness identified the taxi as a Blue Taxi. Tests show that under the viewing conditions at the time, witnesses correctly identify the color of a taxi 80% of the time and incorrectly 20% of the time.\n",
      "\n",
      "Which is more probable?\n",
      "CHOICE1: The taxi involved was a Blue Taxi.\n",
      "CHOICE1: The taxi involved was a Green Taxi.\n",
      "ANSWER: choice2\n",
      "CATEGORY: Base-rate neglect (causal)\n",
      "\n",
      "QUESTION: In a certain population, 1% of people are carriers of a rare genetic condition. A test for the condition is 95% accurate‚Äîit correctly identifies carriers 95% of the time (true positive rate) and correctly identifies non-carriers 95% of the time (true negative rate). A person from this population tests positive.\n",
      "\n",
      "Which is more probable?\n",
      "CHOICE1: The person is a carrier of the genetic condition.\n",
      "CHOICE1: The person is not a carrier of the genetic condition.\n",
      "ANSWER: choice2\n",
      "CATEGORY: Base-rate neglect (causal)\n",
      "\n",
      "QUESTION: A factory has two machines producing light bulbs: Machine A produces 30% of the bulbs with a defect rate of 1%, and Machine B produces 70% of the bulbs with a defect rate of 2%. A randomly selected bulb from the day's production is defective.\n",
      "\n",
      "Which machine is more likely to have produced the defective bulb?\n",
      "CHOICE1: Machine A\n",
      "CHOICE1: Machine B\n",
      "ANSWER: choice2\n",
      "CATEGORY: Base-rate neglect (causal)\n",
      "\n",
      "QUESTION: In a hospital, two departments conduct blood tests: Department X handles 25% of tests with a contamination rate of 2%, and Department Y handles 75% of tests with a contamination rate of 1%. A test result comes back contaminated.\n",
      "\n",
      "Which department is more likely to have conducted the test?\n",
      "CHOICE1: Department X\n",
      "CHOICE1: Department Y\n",
      "ANSWER: choice2\n",
      "CATEGORY: Base-rate neglect (causal)\n",
      "\n",
      "QUESTION: An airport security system has two types of scanners: Metal Detectors (80% of scans) with a false alarm rate of 4%, and Advanced Imaging Scanners (20% of scans) with a false alarm rate of 1%. An alarm is triggered during a passenger's screening, and it turns out to be a false alarm.\n",
      "\n",
      "Which scanner is more likely to have caused the false alarm?\n",
      "CHOICE1: Metal Detector\n",
      "CHOICE1: Advanced Imaging Scanner\n",
      "ANSWER: choice1\n",
      "CATEGORY: Base-rate neglect (causal)\n",
      "\n",
      "QUESTION: The average grade of the students in your math class is 80 percent. Your grade is 75 percent. How well do you perform on math assessments compared to your peers?\n",
      "CHOICE1: I perform above average.\n",
      "CHOICE1: I perform below average.\n",
      "ANSWER: choice2\n",
      "CATEGORY: better-than-average bias\n",
      "\n",
      "QUESTION: A company survey shows that the average employee completes 5 projects per month. You consistently complete 6 projects per month. How would you rate your productivity compared to the average employee?\n",
      "CHOICE1: My productivity is above average.\n",
      "CHOICE1: My productivity is below average.\n",
      "ANSWER: choice1\n",
      "CATEGORY: better-than-average bias\n",
      "\n",
      "QUESTION: Statistics indicate that the average person can recall 7 digits in a memory test. In a recent test, you were able to recall 5 digits. Compared to the average person, how is your short-term memory?\n",
      "CHOICE1: My short-term memory is above average.\n",
      "CHOICE1: My short-term memory is below average.\n",
      "ANSWER: choice2\n",
      "CATEGORY: better-than-average bias\n",
      "\n",
      "QUESTION: In a driving skills assessment, the national average score is 85%. You received a score of 90%. How would you assess your driving skills?\n",
      "CHOICE1: My driving skills are above average.\n",
      "CHOICE1: My driving skills are below average.\n",
      "ANSWER: choice1\n",
      "CATEGORY: better-than-average bias\n",
      "\n",
      "QUESTION: A fitness benchmark states that the average person can run a mile in 9 minutes. You complete a mile in 10 minutes. How would you rate your running ability compared to the average person?\n",
      "CHOICE1: My running ability is above average.\n",
      "CHOICE1: My running ability is below average.\n",
      "ANSWER: choice2\n",
      "CATEGORY: better-than-average bias\n",
      "\n",
      "QUESTION: A new herbal supplement is claimed to boost energy levels. A study shows that 80% of participants who took the supplement reported increased energy. However, there was no group of participants who did not take the supplement for comparison. Do you believe the supplement is effective?\n",
      "CHOICE1: Yes, because 80% reported increased energy after taking it.\n",
      "CHOICE1: No, because there's no comparison with a control group.\n",
      "ANSWER: choice2\n",
      "CATEGORY: Covariation detection\n",
      "\n",
      "QUESTION: A school implements a new teaching method, and the test scores of students who experienced this method improved by 15%. There is no data on students who continued with the old teaching method. Should we conclude that the new teaching method is effective?\n",
      "CHOICE1: Yes, because students showed a 15% improvement.\n",
      "CHOICE1: No, because there's no comparison with students using the old method.\n",
      "ANSWER: choice2\n",
      "CATEGORY: Covariation detection\n",
      "\n",
      "QUESTION: After a city implements a new traffic law, car accidents decrease by 10%. However, during the same period, there is no data on accident rates in similar cities without the new law. Can we say the new traffic law reduced accidents?\n",
      "CHOICE1: Yes, because accidents decreased after the law was implemented.\n",
      "CHOICE1: No, because there's no comparative data from similar cities.\n",
      "ANSWER: choice2\n",
      "CATEGORY: Covariation detection\n",
      "\n",
      "QUESTION: A fitness program advertises that participants lost an average of 5 pounds in a month. There is no information about weight changes in people who did not follow the program. Should you believe that the fitness program is effective for weight loss?\n",
      "CHOICE1: Yes, because participants lost weight after following the program.\n",
      "CHOICE1: No, because there's no control group for comparison.\n",
      "ANSWER: choice2\n",
      "CATEGORY: Covariation detection\n",
      "\n",
      "QUESTION: An anti-allergy medication is given to 50 patients during pollen season, and 40 of them report fewer symptoms. There's no data on patients who did not take the medication. Is it valid to claim the medication is effective?\n",
      "CHOICE1: Yes, because a majority reported fewer symptoms.\n",
      "CHOICE1: No, because there's no comparative information from a control group.\n",
      "ANSWER: choice2\n",
      "CATEGORY: Covariation detection\n",
      "\n",
      "QUESTION: A car dealership offers two financing options on a $20,000 car. Which option will cost you less over the 5-year financing period?‚Ä®",
      "‚Ä®",
      "Option A: Receive the manager's special $2,000 rebate on the car's price.\n",
      "Option B: Get 0% financing over 5 years.\n",
      "CHOICE1: Option A\n",
      "CHOICE1: Option B\n",
      "ANSWER: choice2\n",
      "CATEGORY: Framing\n",
      "\n",
      "QUESTION: A grocery store offers two promotions on a $100 shopping bill. Which option results in you paying less?‚Ä®",
      "‚Ä®",
      "Option A: Get a $10 price reduction at checkout.‚Ä®",
      "Option B: Avoid paying the high 10% surcharge at checkout.\n",
      "CHOICE1: Option A\n",
      "CHOICE1: Option B\n",
      "ANSWER: choice1\n",
      "CATEGORY: Framing\n",
      "\n",
      "QUESTION: You have two job offers. Which job pays more over a 5 year period?‚Ä®",
      "‚Ä®",
      "Offer A: Salary of $50,000 per year, with a 5% / year raise.‚Ä®",
      "Offer B: Salary of $55,000 per year\n",
      "CHOICE1: Offer A\n",
      "CHOICE1: Offer B\n",
      "ANSWER: choice2\n",
      "CATEGORY: Framing\n",
      "\n",
      "QUESTION: You plan to deposit $1,000 in a savings account for 5 years. Of the two accounts available, which account should you choose to maximize your wealth over a 5 year period?‚Ä®",
      "‚Ä®",
      "Account A: Earn 4% annual interest, compounded annually.‚Ä®",
      "Account B: Get the exclusive $100 bonus at account opening. Earn 2% annual interest every year to steadily grow your wealth.\n",
      "CHOICE1: Account A\n",
      "CHOICE1: Account B\n",
      "ANSWER: choice1\n",
      "CATEGORY: Framing\n",
      "\n",
      "QUESTION: A smartphone retailer offers two purchase options. Which option will cost you less over a 2 year period?‚Ä®",
      "‚Ä®",
      "Option A: Take the phone today with zero down payment. Pay as little as $30 a month for the next 2 years.\n",
      "Option B: Pay $600 today to get the smartphone.\n",
      "CHOICE1: Option A\n",
      "CHOICE1: Option B\n",
      "ANSWER: choice2\n",
      "CATEGORY: Framing\n",
      "\n",
      "QUESTION: A lottery randomly selects winning numbers each week. Does picking your own numbers or using randomly selected numbers give you a better chance of winning?\n",
      "CHOICE1: Choosing your own \"lucky numbers\" when buying a ticket.\n",
      "CHOICE1: Allowing the system to randomly assign numbers to your ticket.\n",
      "ANSWER: choice2\n",
      "CATEGORY: Illusion of control\n",
      "\n",
      "QUESTION: During a casino game, a player believes that throwing the dice with a specific technique will influence the outcome. Will the player‚Äôs throwing technique influence the outcome of the throw, or is the outcome unaffected?\n",
      "CHOICE1: The player's throwing technique can affect the randomness of the dice roll.\n",
      "CHOICE1: The outcome is entirely random; the throwing technique does not influence it.\n",
      "ANSWER: choice2\n",
      "CATEGORY: Illusion of control\n",
      "\n",
      "QUESTION: An investor thinks they can predict stock market movements and consistently beat the market averages. Is this belief justified?\n",
      "CHOICE1: No, consistently outperforming market averages over time is highly unlikely.\n",
      "CHOICE1: Yes, with skill and research, one can reliably predict market movements.\n",
      "ANSWER: choice1\n",
      "CATEGORY: Illusion of control\n",
      "\n",
      "QUESTION: A sports fan performs a personal ritual before games, thinking it helps their team win. Is it likely that the fan‚Äôs ritual affects the team‚Äôs performance?\n",
      "CHOICE1: The fan's ritual can influence the team's performance and outcome.\n",
      "CHOICE1: The team's performance is unaffected by fans' personal rituals.\n",
      "ANSWER: choice2\n",
      "CATEGORY: Illusion of control\n",
      "\n",
      "QUESTION: A person believes that by pressing the elevator button multiple times, they can make the elevator arrive faster. Is this belief justified?\n",
      "CHOICE1: Pressing the elevator button multiple times will make it arrive sooner.\n",
      "CHOICE1: Pressing the button once is sufficient; pressing it multiple times doesn't affect its speed.\n",
      "ANSWER: choice2\n",
      "CATEGORY: Illusion of control\n",
      "\n",
      "QUESTION: You are presented with two options for an investment. Which option should you choose to maximize your expected gain?‚Ä®",
      "‚Ä®",
      "Option 1: Receive a guaranteed $450.‚Ä®",
      "Option 2: Flip a coin. If it lands on heads, you receive $1,000. If it lands on tails, you receive nothing.\n",
      "CHOICE1: Option 1\n",
      "CHOICE1: Option 2\n",
      "ANSWER: choice2\n",
      "CATEGORY: Loss aversion \n",
      "\n",
      "QUESTION: You need to decide how to handle a potential loss. Which option should you choose to minimize your expected loss?‚Ä®",
      "‚Ä®",
      "Option 1: Accept a guaranteed loss of $480.‚Ä®",
      "Option 2: Have a 50% chance to lose $1,000 and a 50% chance to lose nothing.\n",
      "CHOICE1: Option 1\n",
      "CHOICE1: Option 2\n",
      "ANSWER: choice1\n",
      "CATEGORY: Loss aversion \n",
      "\n",
      "QUESTION: You are considering purchasing insurance. Which option should you choose to minimize your expected cost?‚Ä®",
      "‚Ä®",
      "Option 1: Pay a $280 premium to be fully insured against a potential $3,000 loss.‚Ä®",
      "Option 2: Do not buy insurance and face a 10% chance of incurring the $3,000 loss.\n",
      "CHOICE1: Option 1\n",
      "CHOICE1: Option 2\n",
      "ANSWER: choice1\n",
      "CATEGORY: Loss aversion \n",
      "\n",
      "QUESTION: You have the following choices for a monetary gain. Which option should you choose to maximize your expected gain?‚Ä®",
      "‚Ä®",
      "Option 1: Receive a guaranteed $95.‚Ä®",
      "Option 2: Have a 10% chance to gain $1,000 and a 90% chance to gain nothing.\n",
      "CHOICE1: Option 1\n",
      "CHOICE1: Option 2\n",
      "ANSWER: choice2\n",
      "CATEGORY: Loss aversion \n",
      "\n",
      "QUESTION: You are offered a gambling opportunity. Which option should you choose to maximize your expected gain?‚Ä®",
      "‚Ä®",
      "Option 1: Pay $50 to enter a coin toss game where you win $150 if it lands on heads and lose your $50 entry fee if it lands on tails.‚Ä®",
      "Option 2: Do not participate in the game.\n",
      "CHOICE1: Option 1\n",
      "CHOICE1: Option 2\n",
      "ANSWER: choice1\n",
      "CATEGORY: Loss aversion \n",
      "\n",
      "QUESTION: Your goal is to minimize your total interest expenses. You have $1,000 in a savings account earning 1% annual interest, and you also have $1,000 in credit card debt accruing 15% annual interest. How should you use your $1000?\n",
      "CHOICE1: Keep the money in your savings account and continue making minimum payments on your credit card.\n",
      "CHOICE1: Use the $1,000 from your savings to pay off your credit card debt.\n",
      "ANSWER: choice2\n",
      "CATEGORY: Mental accounting\n",
      "\n",
      "QUESTION: To optimize your financial health by reducing debt, you receive a $2,000 tax refund and have a student loan with a $2,000 balance at 6% interest. What should you do with your tax refund?\n",
      "CHOICE1: Use the tax refund to pay off your student loan.\n",
      "CHOICE1: Spend the tax refund on a vacation you've been planning.\n",
      "ANSWER: choice1\n",
      "CATEGORY: Mental accounting\n",
      "\n",
      "QUESTION: You aim to maximize savings on your purchase. You're considering buying a $1,000 laptop. The store next door offers the same laptop for $950.  Where should you purchase the laptop?\n",
      "CHOICE1: Buy the laptop at the current store.\n",
      "CHOICE1: Go to the next store to save $50.\n",
      "ANSWER: choice2\n",
      "CATEGORY: Mental accounting\n",
      "\n",
      "QUESTION: Your objective is to reduce your overall interest payments. You have a car loan with a 5% interest rate and credit card debt with an 18% interest rate. You have $1,000 available to pay down debt.  How do you use this money?\n",
      "CHOICE1: Pay off $1,000 of your car loan.\n",
      "CHOICE1: Pay off $1,000 of your credit card debt.\n",
      "ANSWER: choice2\n",
      "CATEGORY: Mental accounting\n",
      "\n",
      "QUESTION: To manage your finances optimally by prioritizing essential expenses, you win $500 in a lottery and have $500 in unpaid utility bills. How do you use your lottery winnings?\n",
      "CHOICE1: Use the lottery winnings to pay your utility bills.\n",
      "CHOICE1: Spend the lottery winnings on a new gadget you've wanted.\n",
      "ANSWER: choice1\n",
      "CATEGORY: Mental accounting\n",
      "\n",
      "QUESTION: Between 1990 and 2020, did the overall violent crime rate in the United States increase or decrease?\n",
      "CHOICE1: Increase\n",
      "CHOICE1: Decrease\n",
      "ANSWER: choice2\n",
      "CATEGORY: Myside bias\n",
      "\n",
      "QUESTION: As of 2020, do immigrants in the United States commit crimes at a higher rate or a lower rate than native-born citizens?\n",
      "CHOICE1: higher rate\n",
      "CHOICE1: lower rate\n",
      "ANSWER: choice2\n",
      "CATEGORY: Myside bias\n",
      "\n",
      "QUESTION: Over the past three decades, has the global percentage of people living in extreme poverty increased or decreased?\n",
      "CHOICE1: Increased\n",
      "CHOICE1: Decreased\n",
      "ANSWER: choice2\n",
      "CATEGORY: Myside bias\n",
      "\n",
      "QUESTION: Is the primary cause of recent global climate change human activities or natural environmental cycles?\n",
      "CHOICE1: Human activities\n",
      "CHOICE1: Natural environmental cycles\n",
      "ANSWER: choice1\n",
      "CATEGORY: Myside bias\n",
      "\n",
      "QUESTION: Does the U.S. federal government spend more on social security and healthcare or on foreign aid?\n",
      "CHOICE1: Social security and healthcare\n",
      "CHOICE1: Foreign aid\n",
      "ANSWER: choice1\n",
      "CATEGORY: Myside bias\n",
      "\n",
      "QUESTION: A new vaccine is available for a disease that has a 1 in 1,000 chance of infecting you and causing severe health complications. The vaccine has a 1 in 100,000 chance of causing a serious adverse reaction. To maintain your health, would it be better to receive the vaccine or not receive the vaccine?\n",
      "CHOICE1: Receive the vaccine.\n",
      "CHOICE1: Not receive the vaccine.\n",
      "ANSWER: choice1\n",
      "CATEGORY: Omission bias\n",
      "\n",
      "QUESTION: A wildfire is approaching your town. Authorities advise evacuation, but there's a 5% chance that evacuation routes might be congested, potentially delaying your escape. Staying home carries a 50% chance of being overtaken by the fire. Should you evacuate as advised or stay at home?\n",
      "CHOICE1: Evacuate as advised.\n",
      "CHOICE1: Stay at home.\n",
      "ANSWER: choice1\n",
      "CATEGORY: Omission bias\n",
      "\n",
      "QUESTION: Your doctor recommends a preventive surgery that has a 5% chance of complications but will eliminate a genetic risk of developing a life-threatening condition that has a 50% chance of occurring in your lifetime. Should you undergo the surgery or decline the surgery?\n",
      "CHOICE1: Undergo the surgery.\n",
      "CHOICE1: Decline the surgery.\n",
      "ANSWER: choice1\n",
      "CATEGORY: Omission bias\n",
      "\n",
      "QUESTION: A financial advisor suggests investing in a low-risk fund that has a 10% chance of losing value in the short term but is projected to yield significant growth over the next decade. Keeping your money in a savings account guarantees no short-term loss but will lose 5% of its value due to inflation over the next year. Should you invest in the low-risk fund or keep your money in the savings account?\n",
      "CHOICE1: Invest in the low-risk fund.\n",
      "CHOICE1: Keep your money in the savings account.\n",
      "ANSWER: choice1\n",
      "CATEGORY: Omission bias\n",
      "\n",
      "QUESTION: Your company can install a software update that has a 2% chance of causing minor system glitches but will protect against a new virus that has a 20% chance of breaching your network security. Should you install the software update or delay the update?\n",
      "CHOICE1: Install the software update.\n",
      "CHOICE1: Delay the update.\n",
      "ANSWER: choice1\n",
      "CATEGORY: Omission bias\n",
      "\n",
      "QUESTION: A tech company invested $50 million in a startup that had a 75% chance of success based on market analysis. The startup ultimately failed due to sudden market shifts. Should the company's investment decision be considered sound?\n",
      "CHOICE1: Yes, the company made a well-informed decision based on strong statistical evidence, despite the unfavorable outcome.\n",
      "CHOICE1: No, the company made a poor decision because they ended up losing $50 million.\n",
      "ANSWER: choice1\n",
      "CATEGORY: Outcome bias\n",
      "\n",
      "QUESTION: A pharmaceutical firm released a new drug that showed a 95% effectiveness rate with less than a 1 percent chance of side effects in clinical trials involving thousands of participants. After release, a small percentage of patients experienced severe reactions. Was the firm's decision to release the medication responsible?\n",
      "CHOICE1: Yes, the firm acted responsibly based on extensive clinical data indicating high effectiveness and safety.\n",
      "CHOICE1: No, the firm was irresponsible because some patients suffered severe reactions.\n",
      "ANSWER: choice1\n",
      "CATEGORY: Outcome bias\n",
      "\n",
      "QUESTION: An experienced fund manager invested clients' money into a diversified portfolio expected to yield a 10% annual return based on historical data. Due to an unexpected global economic crisis, the portfolio lost 5% in value. Should the fund manager's investment strategy be considered statistically appropriate?\n",
      "CHOICE1: Yes, the fund manager used a prudent strategy based on historical performance and diversification principles.\n",
      "CHOICE1: No, the fund manager made a bad decision because the clients' investments decreased in value.\n",
      "ANSWER: choice1\n",
      "CATEGORY: Outcome bias\n",
      "\n",
      "QUESTION: A city implemented a new traffic system expected to reduce accidents by 30%, based on successful results from similar cities. In the first month, accidents seemingly increased due to driver adjustment issues. Was the city's decision to adopt the new system justified?\n",
      "CHOICE1: Yes, the decision was justified as it was based on evidence from other cities showing significant accident reduction.\n",
      "CHOICE1: No, the city made a poor decision because accidents increased after the new system was implemented.\n",
      "ANSWER: choice1\n",
      "CATEGORY: Outcome bias\n",
      "\n",
      "QUESTION: During a health crisis, a government decided to close schools based on models predicting a 80% reduction in virus transmission. Despite this measure, infection rates continued to rise due to other factors. Was the government's decision to close schools appropriate?\n",
      "CHOICE1: Yes, the government made an appropriate decision based on models that suggested significant benefits.\n",
      "CHOICE1: No, the government made an ineffective decision because infection rates still increased.\n",
      "ANSWER: choice1\n",
      "CATEGORY: Outcome bias\n",
      "\n",
      "QUESTION: According to prior data collected by Company X, projects of similar complexity have a 70% chance of exceeding their timelines when led by managers with less than five years of experience. A project manager with only one year of experience believes he can complete a complex project in 6 months, half the industry average of 12 months. Should Company X adjust the project timeline based on his estimate?\n",
      "CHOICE1: Yes, the company should trust his confidence and adjust the timeline accordingly.\n",
      "CHOICE1: No, the company should rely on industry data and not adjust the timeline based solely on his estimate.\n",
      "ANSWER: choice2\n",
      "CATEGORY: Overconfidence\n",
      "\n",
      "QUESTION: Studies indicate that over 85% of individual investors underperform the S&P 500 index over a ten-year period. An investor with minimal trading experience believes he can consistently outperform the market without professional advice. Should he manage his entire portfolio on his own?\n",
      "CHOICE1: Yes, he should manage his portfolio himself since he believes in his ability to outperform the market.\n",
      "CHOICE1: No, he should seek professional advice to mitigate risks and make informed decisions.\n",
      "ANSWER: choice2\n",
      "CATEGORY: Overconfidence\n",
      "\n",
      "QUESTION: A survey by a trustworthy association that studies employment trends found that while 70% of recent college graduates rate their leadership skills as \"above average,\" only 10% of employers agree with that assessment. A recent graduate believes she can lead a team effectively without prior management experience. Should the company assign her to a managerial position based on her confidence?\n",
      "CHOICE1: Yes, her confidence indicates she is ready to take on a leadership role.\n",
      "CHOICE1: No, the company should consider her lack of experience and provide opportunities for her to develop leadership skills first.\n",
      "ANSWER: choice2\n",
      "CATEGORY: Overconfidence\n",
      "\n",
      "QUESTION: According to the American Automobile Association, 73% of drivers consider themselves above-average drivers, yet driver error is a significant factor behind the majority of traffic accidents. A driver believes he is exceptionally skilled and doesn't need to follow speed limits or traffic laws strictly. Should he continue driving based on his own judgment?\n",
      "CHOICE1: Yes, his self-assessed skills allow him to make his own judgments on the road.\n",
      "CHOICE1: No, he should adhere to speed limits and traffic laws like everyone else for safety.\n",
      "ANSWER: choice2\n",
      "CATEGORY: Overconfidence\n",
      "\n",
      "QUESTION: Chess statistics indicate that players rated below 2000 have less than a 1% chance of defeating a grandmaster in a standard game. A player with a moderate rating of 1500 believes he can defeat a grandmaster rated 2500 without additional preparation. If he aims to get as many wins as possible, should he challenge the grandmaster based on his confidence?\n",
      "CHOICE1: Yes, his confidence is a good indicator that he can win.\n",
      "CHOICE1: No, he should recognize the significant skill gap and prepare extensively before challenging a grandmaster.\n",
      "ANSWER: choice2\n",
      "CATEGORY: Overconfidence\n",
      "\n",
      "QUESTION: You are presented with a biased coin that lands on heads 70% of the time and tails 30% of the time. To maximize the number of correct guesses over many flips, should you guess that the coin will always land on heads, or should you guess heads 70% of the time and tails 30% of the time?\n",
      "CHOICE1: Always guess that the coin will land on heads.\n",
      "CHOICE1: Guess heads 70% of the time and tails 30% of the time.\n",
      "ANSWER: choice1\n",
      "CATEGORY: Probability matching (vs. maximizing)\n",
      "\n",
      "QUESTION: In a game show, you can choose between two doors. Behind Door 1, there's a prize 60% of the time; behind Door 2, there's a prize 40% of the time. To maximize your chances of winning over multiple rounds, should you always choose Door 1 every round, or should you choose Door 1 in 60% of the rounds and Door 2 in 40% of the rounds?\n",
      "CHOICE1: Always choose Door 1 every round.\n",
      "CHOICE1: Choose Door 1 in 60% of the rounds and Door 2 in 40% of the rounds.\n",
      "ANSWER: choice1\n",
      "CATEGORY: Probability matching (vs. maximizing)\n",
      "\n",
      "QUESTION: A test consists of multiple-choice questions where Option X is correct 80% of the time, and Option Y is correct 20% of the time. To maximize your total correct answers, should you always select Option X for every question, or should you select Option X 80% of the time and Option Y 20% of the time?\n",
      "CHOICE1: Always select Option X for every question.\n",
      "CHOICE1: Select Option X 80% of the time and Option Y 20% of the time.\n",
      "ANSWER: choice1\n",
      "CATEGORY: Probability matching (vs. maximizing)\n",
      "\n",
      "QUESTION: A region experiences sunny weather 90% of the days and rainy weather 10% of the days. To maximize your daily comfort, which strategy should you follow when deciding to carry an umbrella: never cary and umbrella or carry an umbrella on only 10% of the days?\n",
      "CHOICE1: Never carry an umbrella, assuming it will be sunny every day.\n",
      "CHOICE1: Carry an umbrella 10% of the days and leave it at home 90% of the days.\n",
      "ANSWER: choice1\n",
      "CATEGORY: Probability matching (vs. maximizing)\n",
      "\n",
      "QUESTION: A company uses two suppliers for a critical component. Supplier A delivers defect-free components 85% of the time, while Supplier B does so 15% of the time. To ensure the highest quality in production, which supplier should the company use?\n",
      "CHOICE1: Source all components from Supplier A.\n",
      "CHOICE1: Source 85% of components from Supplier A and 15% from Supplier B.\n",
      "ANSWER: choice1\n",
      "CATEGORY: Probability matching (vs. maximizing)\n",
      "\n",
      "QUESTION: A city council is deciding how to allocate a limited budget. They can either invest in improving road safety measures, which could reduce frequent traffic accidents, or build a defense system against extremely rare but emotionally distressing shark attacks in local waters. Which option should they choose to make the most effective use of resources?\n",
      "CHOICE1: Invest in road safety measures to prevent common traffic accidents.\n",
      "CHOICE1: Build a shark attack defense system to prevent rare shark incidents.\n",
      "ANSWER: choice1\n",
      "CATEGORY: Probability neglect\n",
      "\n",
      "QUESTION: An individual is considering purchasing insurance. They can buy comprehensive car insurance, which covers common incidents like accidents and theft, or spend the same amount on a policy that offers a large payout in the event of being struck by lightning. Which insurance should they purchase?\n",
      "CHOICE1: Buy comprehensive car insurance to cover common risks.\n",
      "CHOICE1: Buy lightning strike insurance for protection against a rare event.\n",
      "ANSWER: choice1\n",
      "CATEGORY: Probability neglect\n",
      "\n",
      "QUESTION: A parent is deciding whether to allow their child to go on a school trip. The destination is a city that was famously featured in the news due to a highly unusual but sensational crime incident. Statistics show that the city is generally safe and has a significantly lower crime rate than their hometown. Should the parent permit the child to go?\n",
      "CHOICE1: Allow the child to go on the trip, recognizing the low risk.\n",
      "CHOICE1: Prevent the child from going due to fear of a similar crime occurring.\n",
      "ANSWER: choice1\n",
      "CATEGORY: Probability neglect\n",
      "\n",
      "QUESTION: A company must decide between investing in cybersecurity measures against common threats like phishing and malware or allocating significant resources to guard against an exceedingly rare but highly publicized cyber-attack that could potentially shut down operations entirely. Which should they prioritize?\n",
      "CHOICE1: Invest in protections against common cybersecurity threats.\n",
      "CHOICE1: Focus resources on preventing the rare catastrophic cyber-attack.\n",
      "ANSWER: choice1\n",
      "CATEGORY: Probability neglect\n",
      "\n",
      "QUESTION: An individual has a choice between getting vaccinated against a common flu strain that affects thousands annually or taking an expensive supplement that claims to protect against a very rare but deadly tropical disease they've recently heard about in the news. Which should they choose?\n",
      "CHOICE1: Get vaccinated against the common flu strain.\n",
      "CHOICE1: Take the supplement to guard against the rare tropical disease.\n",
      "ANSWER: choice1\n",
      "CATEGORY: Probability neglect\n",
      "\n",
      "QUESTION: A health organization has funds to distribute life-saving vaccines and cover the corresponding transport costs. Under their monetary constraints, they can choose one of two villages to distribute vaccines: they can either distribute vaccines to 60 out of the 300 children in Village A or 50 out of the 55 children in Village B. Which option should they choose to maximize the number of children vaccinated?\n",
      "CHOICE1: Vaccinate 60 out of 300 children in Village A.\n",
      "CHOICE1: Vaccinate 50 out of 55 children in Village B.\n",
      "ANSWER: choice1\n",
      "CATEGORY: Proportion dominance\n",
      "\n",
      "QUESTION: An environmental agency can undertake one of two conservation projects: Project X that can save 80 out of 1,000 endangered turtles on a large beach or Project Y that can save 40 out of 50 endangered turtles on a small island. Which project should they choose to save the most turtles?\n",
      "CHOICE1: Implement Project X\n",
      "CHOICE1: Implement Project Y\n",
      "ANSWER: choice1\n",
      "CATEGORY: Proportion dominance\n",
      "\n",
      "QUESTION: A disaster relief fund can be allocated to one of two regions affected by floods: Region 1 where aid can be provided to 1,000 out of 10,000 affected families or Region 2 where aid can be provided to 800 out of 1,000 affected families.\n",
      "CHOICE1: Allocate funds to Region 1\n",
      "CHOICE1: Allocate funds to Region 2\n",
      "ANSWER: choice1\n",
      "CATEGORY: Proportion dominance\n",
      "\n",
      "QUESTION: A public health campaign can target one of two diseases: Disease A or Disease B. If Disease A is targeted, 200 cases out of 2,000 potential cases can be prevented, and if Disease B is targeted, 150 cases out of 160 potential cases can be prevented. Which campaign should be prioritized to prevent the most cases?\n",
      "CHOICE1: Focus on Disease A\n",
      "CHOICE1: Focus on Disease B\n",
      "ANSWER: choice1\n",
      "CATEGORY: Proportion dominance\n",
      "\n",
      "QUESTION: An educational grant can be awarded to one of two schools to improve literacy rates: School M where literacy can be improved for 500 out of 2,500 students, or School N where literacy can be improved for 400 out of 450 students. Which school should receive the grant to benefit the most students?\n",
      "CHOICE1: School M\n",
      "CHOICE1: School N\n",
      "ANSWER: choice1\n",
      "CATEGORY: Proportion dominance\n",
      "\n",
      "QUESTION: Your company currently uses Software A, which studies have shown has a 40% vulnerability rate to cyber-attacks. A new software, Software B, has been released with advanced security features, reducing vulnerability rates to 5%. Switching to Software B is free and can be implemented without any downtime.  Which option should you choose for your company and customers?\n",
      "CHOICE1: Continue using Software A\n",
      "CHOICE1: Switch to Software B\n",
      "ANSWER: choice2\n",
      "CATEGORY: Status quo\n",
      "\n",
      "QUESTION: You have a savings account with Bank X that offers an annual interest rate of 0.05%, resulting in an annual return of $5 on a $10,000 deposit. Bank Y, insured by the same federal agency, offers an annual interest rate of 1.5%, yielding $150 annually on the same deposit amount. Switching banks is free and can be done easily online in less than 10 minutes. Which option should you choose?\n",
      "CHOICE1: Stay with Bank X\n",
      "CHOICE1: Switch to Bank Y\n",
      "ANSWER: choice2\n",
      "CATEGORY: Status quo\n",
      "\n",
      "QUESTION: Your current internet browser, Browser M, lacks critical security updates and has been reported to have a 25% chance of phishing attacks bypassing its defenses. A new browser, Browser N, offers enhanced security measures, reducing the risk of phishing attacks to less than 1%. Downloading and installing Browser N is free and takes only 5 minutes. Which option should you choose?\n",
      "CHOICE1: Stick with Browser M\n",
      "CHOICE1: Switch to Browser N\n",
      "ANSWER: choice2\n",
      "CATEGORY: Status quo\n",
      "\n",
      "QUESTION: You are enrolled in an electricity plan (Plan A) that charges 20 cents per kWh during peak hours and 10 cents per kWh during off-peak hours. Another plan (Plan B) from the same provider offers a flat rate of 9 cents per kWh at all times. An analysis shows that 70% of your electricity usage occurs during peak hours. Switching plans requires a quick phone call with no penalties or fees. Which option should you choose?\n",
      "CHOICE1: Plan A\n",
      "CHOICE1: Plan B\n",
      "ANSWER: choice2\n",
      "CATEGORY: Status quo\n",
      "\n",
      "QUESTION: Your mobile phone plan with Company V costs $60 per month for 5GB of data. A competitor, Company T, offers a plan costing $50 per month for 15GB of data, with the same nationwide 5G coverage and no contract obligations or switching fees. What should you do?\n",
      "CHOICE1: Keep your current plan at $60/month for 5GB data.\n",
      "CHOICE1: Switch to the competitor's plan at $50/month for 15GB data.\n",
      "ANSWER: choice2\n",
      "CATEGORY: Status quo\n",
      "\n",
      "QUESTION: Alex purchased a non-refundable $100 ticket for a weekend ski trip. Before the trip, a weather report predicts severe storms with a 70% chance of blizzard conditions, making travel dangerous. Statistics show that accidents increase by 50% during such weather. Should Alex cancel the trip or go anyway?\n",
      "CHOICE1: Proceed with the trip to avoid wasting the $100 ticket.\n",
      "CHOICE1: Cancel the trip to stay safe and avoid potential hazards.\n",
      "ANSWER: choice2\n",
      "CATEGORY: Sunk cost fallacy\n",
      "\n",
      "QUESTION: Biotech Corp has invested $2 million in developing a new drug. Clinical trials reveal the drug is only 15% effective, while competitors' drugs are 80% effective. Continuing development would cost an additional $5 million with only a 10% chance of success. Should the company continue investing in the drug or halt all investments?\n",
      "CHOICE1: Continue investing in the drug to try to improve its effectiveness.\n",
      "CHOICE1: Discontinue the project and allocate resources to more promising research.\n",
      "ANSWER: choice2\n",
      "CATEGORY: Sunk cost fallacy\n",
      "\n",
      "QUESTION: Emma has spent 15 hours reading a 500-page book that she finds uninteresting and confusing. She has 200 pages left, estimated to take 5 more hours. Studies show that 60% of readers abandon books they don't enjoy, and doing so increases overall reading satisfaction by 40%. Should Emma stop reading the book or keep going?\n",
      "CHOICE1: Stop reading the book and start a new one she might enjoy.\n",
      "CHOICE1: Continue reading to finish it since she has already invested 15 hours.\n",
      "ANSWER: choice1\n",
      "CATEGORY: Sunk cost fallacy\n",
      "\n",
      "QUESTION: Michael bought a season pass to a theme park for $300, expecting to visit at least five times to make it cost-effective. After two visits, he realizes he doesn't enjoy the attractions. Surveys indicate that people who force themselves to use unwanted season passes report 30% less leisure satisfaction, while those who switch to preferred activities increase their satisfaction by 50%. Should Michael keep going to the park or stop?\n",
      "CHOICE1: Keep visiting the theme park to get his money's worth.\n",
      "CHOICE1: Stop going and spend his leisure time on activities he enjoys.\n",
      "ANSWER: choice2\n",
      "CATEGORY: Sunk cost fallacy\n",
      "\n",
      "QUESTION: A city has spent $10 million on constructing a bridge expected to cost $20 million in total. Midway, engineers find structural issues that will raise costs by 75%, totaling $35 million. Economic forecasts show a 65% chance that the bridge will not generate sufficient revenue to justify the increased cost. Should the city continue building or halt construction?\n",
      "CHOICE1: Invest the additional funds to complete the bridge.\n",
      "CHOICE1: Halt construction to prevent further financial loss.\n",
      "ANSWER: choice2\n",
      "CATEGORY: Sunk cost fallacy\n",
      "\n",
      "QUESTION: Emily can choose to receive $500 today or $600 in six months. The annual inflation rate is 2%, and the average annual interest rate on a savings account is 1%. Which option should Emily choose to maximize her financial gain?\n",
      "CHOICE1: Take the $500 today.\n",
      "CHOICE1: Wait six months to receive $600.\n",
      "ANSWER: choice2\n",
      "CATEGORY: Temporal discounting\n",
      "\n",
      "QUESTION: David has the option to receive a bonus of $2,000 now or $2,500 in one year from his employer. The average annual inflation rate is 2%, and investing the bonus now could yield an average annual return of 3%. Which option should David choose to maximize his financial gain?\n",
      "CHOICE1: Accept the $2,000 bonus now and invest it.\n",
      "CHOICE1: Wait one year to receive the $2,500 bonus.\n",
      "ANSWER: choice2\n",
      "CATEGORY: Temporal discounting\n",
      "\n",
      "QUESTION: Sophia is offered a choice between receiving a $100 gift card today or a $150 gift card in three months. Statistics show that the average person spends gift cards within one month of receipt, and consumer prices are expected to remain stable with 0% inflation over the next three months. Which option should Sophia choose to maximize her purchasing power?\n",
      "CHOICE1: Take the $100 gift card today.\n",
      "CHOICE1: Wait three months to receive the $150 gift card.\n",
      "ANSWER: choice2\n",
      "CATEGORY: Temporal discounting\n",
      "\n",
      "QUESTION: Mark is considering buying a new smartphone. He can purchase it now for $800, or wait six months when the price is expected to drop by 25% due to new models being released. His current phone works just fine, and the local tech experts say that it will continue to work smoothly considering his past use for at least a year. Additionally, the technology in the new model offers only a 3% performance improvement over the current one. Which option should Mark choose to make the most financially sound decision?\n",
      "CHOICE1: Buy the smartphone now for $800.\n",
      "CHOICE1: Wait six months and buy it for $600.\n",
      "ANSWER: choice2\n",
      "CATEGORY: Temporal discounting\n",
      "\n",
      "QUESTION: Lisa is offered a promotion that comes with an immediate salary increase of $5,000 per year, but requires her to relocate immediately, incurring moving costs of $4,000. Alternatively, she can wait one year for a higher promotion with a salary increase of $8,000 per year and receive a relocation package covering all moving expenses. Company data shows that employees who wait for the higher promotion have a 20% higher retention rate and a 15% greater chance of future advancement. Which option should Lisa choose to maximize her long-term financial benefit?\"\n",
      "CHOICE1: Accept the immediate promotion and relocate now.\n",
      "CHOICE1: Wait one year for the higher promotion and relocation package.\n",
      "ANSWER: choice2\n",
      "CATEGORY: Temporal discounting\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, row in biases.iterrows():\n",
    "    print(f\"QUESTION: {row['Questions']}\")\n",
    "    print(f\"CHOICE1: {row['choice1']}\")\n",
    "    print(f\"CHOICE1: {row['choice2']}\")\n",
    "    print(f\"ANSWER: {row['correct']}\")\n",
    "    print(f\"CATEGORY: {row['category']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "fa75ca17-a76a-4718-927a-fa8ed16c1e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "biases.to_csv(\"TRUE_bias.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1e39fc-4f3d-48be-866a-93acd4ce88ac",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Reformat dataset to match objective dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "3b44eb8e-0ade-4bd6-a17f-3d17aeeae835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['correct', 'incorrect', 'MCQ_id', 'source', 'difficulty', 'choice1', 'choice2', 'choice1_type', 'choice2_type', 'chosen', 'rejected', 'chosen_type', 'rejected_type', 'confidence_scores', 'confidence_difficulty', 'decision_time', 'number_of_clicks', 'scratch_space', 'decision_time_beta', 'num_clicks_beta', 'confidence_beta', 'correct_chosen', 'raw_number_of_clicks', 'raw_decision_time', 'log_decision_time', 'log_norm_decision_time', 'prompt', 'prompt_response_group'], dtype='object')"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "805def1d-b915-461d-8d89-dc4682059ab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Questions', 'choice1', 'choice2', 'category', 'correct', 'other_info', 'objective_question', 'o1'], dtype='object')"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases = pd.read_csv(\"TRUE_bias.csv\")\n",
    "biases.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "7d4a49ec-3124-4653-9541-a186168ac6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "biases = biases.drop(\"Unnamed: 0\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "bff30683-7038-49a9-b224-4b3002104c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "biases[\"prompt\"] = biases[\"Questions\"]\n",
    "biases = biases.drop(\"Questions\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "2060b61c-9233-4c1c-b007-d6d14b4b4e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "biases = biases.drop(['objective_question', 'o1'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "efaef7c4-5d67-4820-9cff-10504e0f8eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct_incorrect(row):\n",
    "    if row[\"correct\"] == \"choice1\":\n",
    "        return row[\"choice1\"], row[\"choice2\"]\n",
    "    else:\n",
    "        return row[\"choice2\"], row[\"choice1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "c22628c3-b96f-4bbe-9c85-6e6102d2282e",
   "metadata": {},
   "outputs": [],
   "source": [
    "biases[[\"correct\", \"incorrect\"]] = biases.apply(get_correct_incorrect, axis=1, result_type=\"expand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "95572157-5f94-4be3-bfdb-cb1d43152531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_choices(row):\n",
    "    choices = [\"correct\", \"incorrect\"]\n",
    "    random.shuffle(choices)\n",
    "    return row[choices[0]], row[choices[1]], choices[0], choices[1]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "a4b3fcae-d897-4cf2-b787-15ca54eea19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "biases[[\"choice1\", \"choice2\", \"choice1_type\", \"choice2_type\"]] = biases.apply(get_choices, axis=1, result_type=\"expand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "e7903ecc-f4a3-40d5-9449-e70b286b76f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "biases[\"MCQ_id\"] = random.sample(range(5002, 6000), 155)\n",
    "biases[\"MCQ_id\"] = \"MC\" + biases[\"MCQ_id\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "429a967f-de18-4276-a5ec-3db946dd9bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id_tag(row):\n",
    "    if row[\"choice1_type\"] == \"incorrect\":\n",
    "        return row[\"MCQ_id\"] + \"21\"\n",
    "    else:\n",
    "        return row[\"MCQ_id\"] + \"12\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "c6b6777e-397d-4172-979f-1a364bd537eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "biases[\"MCQ_id\"] = biases.apply(get_id_tag, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "9188732c-c03d-40ce-93a8-3dd0f17fa16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "biases.to_csv(\"TRUE_bias.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403cbaf0-5c62-4c4b-ab8c-7f7857c52e6b",
   "metadata": {},
   "source": [
    "### Making the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "187dba99-cfec-4ca7-88e0-b68d9491baf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trivia_qa = pd.read_csv(\"trivia_qa_filtered.csv\")\n",
    "jeopardy = pd.read_csv(\"jeopardy_filtered.csv\")\n",
    "quail = pd.read_csv(\"quail_filtered.csv\")\n",
    "big_bench = pd.read_csv(\"big_bench_filtered.csv\")\n",
    "mmlu = pd.read_csv(\"mmlu_filtered.csv\")\n",
    "biases = pd.read_csv(\"TRUE_bias.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "c27373d4-18c6-4fc0-be4d-941e75273c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trivia_qa.loc[~(trivia_qa[\"question\"].str.contains(\"\\?\")), \"question\"] = trivia_qa.loc[~(trivia_qa[\"question\"].str.contains(\"\\?\")), \"question\"] + \"?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "77076a2e-6193-46c2-9b10-cea2d6a2b48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "jeopardy = jeopardy.sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "ce36cce2-603f-43f8-9960-48100188f6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([trivia_qa, jeopardy, quail, big_bench, mmlu])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "716204bc-1b74-44d0-97f9-8f2b2181a530",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"MCQ_id\"] = random.sample(range(1000, 5001), 1000)\n",
    "dataset[\"MCQ_id\"] = \"MC\" + dataset[\"MCQ_id\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "756d7c64-c1cc-4e7e-9791-52e4a9ffb304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: source, dtype: object)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_rows = dataset[dataset[\"question\"].isna()]\n",
    "null_rows[\"source\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9292d6c9-8309-46e3-a25f-a8264ab2f39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_choices(row):\n",
    "    choices = [\"correct\", \"incorrect\"]\n",
    "    random.shuffle(choices)\n",
    "    return row[choices[0]], row[choices[1]], choices[0], choices[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3352c097-e044-4de6-b036-48d0ab6da4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[[\"choice1\", \"choice2\", \"choice1_type\", \"choice2_type\"]] = dataset.apply(get_choices, axis=1, result_type=\"expand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6a92afd4-d766-4e7d-b835-329d40facb2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_id_tag(row):\n",
    "    if row[\"choice1_type\"] == \"incorrect\":\n",
    "        return row[\"MCQ_id\"] + \"21\"\n",
    "    else:\n",
    "        return row[\"MCQ_id\"] + \"12\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a4bbbf8b-6f34-4ced-ae96-317cb4b5832f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"MCQ_id\"] = dataset.apply(get_id_tag, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "03f914ff-e363-4ab1-8003-73e0eee15ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv(\"objective_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c8a1ee-f9da-4d7d-9b81-15d376284c22",
   "metadata": {},
   "source": [
    "Reverse the ordering of choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "a6671559-5808-4d24-a213-037e2f441776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combined(dataset):\n",
    "    dataset_copy = dataset.copy()\n",
    "    \n",
    "    def get_reversed_choices(row):\n",
    "        ID = row[\"MCQ_id\"][:-2]\n",
    "        tag = row[\"MCQ_id\"][-2:][::-1]    \n",
    "        return row[\"choice2\"], row[\"choice1\"], row[\"choice2_type\"], row[\"choice1_type\"], ID + tag\n",
    "    \n",
    "    dataset_copy[[\"choice1\", \"choice2\", \"choice1_type\", \"choice2_type\", \"MCQ_id\"]] = dataset_copy.apply(get_reversed_choices, axis=1, result_type=\"expand\")\n",
    "\n",
    "    def get_updated_prompt_response_group(row):\n",
    "        return f\"Prompt: {row['prompt']}\\n\\nResponse A: {row['choice1']}\\n\\nResponse B: {row['choice2']}\"\n",
    "\n",
    "    dataset_copy[\"prompt_response_group\"] = dataset_copy.apply(get_updated_prompt_response_group, axis=1)\n",
    "\n",
    "    combined_dataset = pd.concat([dataset, dataset_copy])\n",
    "    return combined_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "e1d10578-fbbd-4228-b4d7-88a30dda2166",
   "metadata": {},
   "outputs": [],
   "source": [
    "entire_combined = pd.read_csv(\"entire_TRUE_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "01a4e734-da50-47ec-9389-6c837871d213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_id(row):\n",
    "#     return row[\"MCQ_id\"][:-2]\n",
    "\n",
    "# entire_combined[\"ID\"] = entire_combined.apply(get_id, axis=1)\n",
    "# entire_combined = entire_combined.drop_duplicates(\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "49714c02-9224-4c6c-ab4b-6aaf60f317e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1150, 31)"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entire_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "011c2d62-313c-45de-8506-bddf6de26ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "import numpy as np\n",
    "\n",
    "def create_cross_val_splits(data, n_splits=5, random_state=42):\n",
    "    kf = ShuffleSplit(n_splits=n_splits, test_size=0.4, random_state=random_state)\n",
    "    splits = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(data):\n",
    "        train_data = data.iloc[train_idx]\n",
    "        val_data = data.iloc[val_idx]\n",
    "        \n",
    "        val_size = len(val_data)\n",
    "        split_point = val_size // 2\n",
    "        \n",
    "        test_data = val_data.iloc[:split_point]\n",
    "        cal_data = val_data.iloc[split_point:]\n",
    "        \n",
    "        splits.append({\n",
    "            'train': train_data,\n",
    "            'val': test_data,\n",
    "            'cal': cal_data\n",
    "        })\n",
    "    \n",
    "    return splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "00ef56de-0c3c-429d-bb9a-2cf5db9ddf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, split in enumerate(splits):\n",
    "    get_combined(split[\"train\"]).to_csv(f\"TRUE_train_{index}.csv\", index=False)\n",
    "    get_combined(split[\"val\"]).to_csv(f\"TRUE_val_{index}.csv\", index=False)\n",
    "    get_combined(split[\"cal\"]).to_csv(f\"TRUE_cal_{index}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "5910b3ae-ce45-4d8a-8a71-03c701fd57fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_combined(entire_combined).to_csv(\"TRUE_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a2d140-62c1-429a-9615-d735ccb9566d",
   "metadata": {},
   "source": [
    "Same format as the subjective dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "06fb68bb-fb8e-479d-91a9-c184adf582eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lie = pd.read_csv(\"/nas/ucb/shivamsinghal/preference-learning-with-bounded-cognition/scalable_oversight/data_collection/truthful_qa/LIE_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "c5dacc66-a235-4130-856a-3d4c0b74b359",
   "metadata": {},
   "outputs": [],
   "source": [
    "true = pd.read_csv(\"TRUE_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "e617bc64-f284-42a4-9226-eae4718c5c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['categories', 'correct_concise', 'incorrect_concise', 'correct_detailed', 'choice1_type', 'choice2_type', 'correct_statements', 'incorrect_statements', 'incorrect_detailed', 'IDs', 'tags', 'tag_IDs', 'choice1', 'choice2', 'prompt', 'chosen', 'rejected', 'chosen_type', 'rejected_type', 'confidence_scores', 'confidence_difficulty', 'decision_time', 'number_of_clicks', 'scratch_space', 'raw_number_of_clicks', 'raw_decision_time', 'prompt_response_group'], dtype='object')"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lie.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "3b4bc116-6e38-422c-922c-980fbc802cfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['correct', 'incorrect', 'MCQ_id', 'source', 'difficulty', 'choice1', 'choice2', 'choice1_type', 'choice2_type', 'chosen', 'rejected', 'chosen_type', 'rejected_type', 'confidence_scores', 'confidence_difficulty', 'decision_time', 'number_of_clicks', 'scratch_space', 'decision_time_beta', 'num_clicks_beta', 'confidence_beta', 'correct_chosen', 'raw_number_of_clicks', 'raw_decision_time', 'log_decision_time', 'log_norm_decision_time', 'prompt', 'prompt_response_group', 'category', 'other_info', 'ID'], dtype='object')"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "dc822b38-a754-4919-afa3-cf0a3a1e494e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols1 = set(true.columns)\n",
    "cols2 = set(lie.columns)\n",
    "common_cols = cols1.intersection(cols2)    \n",
    "unique_to_df1 = cols1 - cols2\n",
    "unique_to_df2 = cols2 - cols1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "25580e0c-5652-4eb5-9441-7bd4a32c6995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'correct_chosen', 'difficulty', 'other_info', 'source'}"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_to_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "1e69888c-2b2e-453d-afbd-01ad7b8f05cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'correct_concise',\n",
       " 'correct_detailed',\n",
       " 'incorrect_concise',\n",
       " 'incorrect_detailed'}"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_to_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "1169b830-43fd-4c2f-907c-1d8696ee92c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ids(row):\n",
    "    return row[\"ID\"][2:]\n",
    "true[\"IDs\"] = true.apply(get_ids, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "472f4908-78e3-43de-936c-43015403f9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "true = true.drop([\"ID\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "072ef131-76c5-4acd-a378-d772c920425d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tags(row):\n",
    "    return row[\"MCQ_id\"][-2:]\n",
    "true[\"tags\"] = true.apply(get_tags, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "9e374a7f-3b0e-4ab1-805e-2393d450a54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "true[\"tag_IDs\"] = true[\"MCQ_id\"]\n",
    "true = true.drop([\"MCQ_id\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "93953d70-d3ee-4998-83b0-58f7c68009fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['decision_time_beta',\n",
       " 'num_clicks_beta',\n",
       " 'confidence_beta',\n",
       " 'log_decision_time',\n",
       " 'log_norm_decision_time']"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_cols = [col for col in true.columns if \"beta\" in col or \"log\" in col]\n",
    "true_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "14d13069-33c3-4654-ad0c-9cbb9ada617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "true = true.drop(true_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "dc75ba02-f131-4f41-b9a0-04b5ddd372d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "true[\"categories\"] = true[\"category\"]\n",
    "true = true.drop(\"category\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "b1443642-f673-4652-80ba-82959cee60ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "true[\"correct_statements\"] = true[\"correct\"]\n",
    "true[\"incorrect_statements\"] = true[\"incorrect\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "fc5647f2-7d05-46ad-a065-1162b3e8a713",
   "metadata": {},
   "outputs": [],
   "source": [
    "true = true.drop([\"correct\", \"incorrect\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "691b2d4d-c3ed-4f13-9fa9-906871c6b994",
   "metadata": {},
   "outputs": [],
   "source": [
    "true.to_csv(\"TRUE_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894e137a-a51f-4a74-a463-bea0bf343fc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40c56d1-bc8f-457d-b157-40bf4c6d4814",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

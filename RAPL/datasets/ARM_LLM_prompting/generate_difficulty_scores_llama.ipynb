{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "231fade6-d4c9-4ccb-a795-3ee038ee5430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "from openai import OpenAI\n",
    "from time import sleep\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle as pkl\n",
    "import math\n",
    "import os\n",
    "\n",
    "import concurrent.futures\n",
    "from functools import partial\n",
    "\n",
    "import mwparserfromhell\n",
    "import pandas as pd\n",
    "import gensim.downloader as api\n",
    "from gensim.corpora import WikiCorpus\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a6577c4-6497-4d1c-be07-addd1d64620d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', 100000)\n",
    "pd.set_option('display.max_colwidth', None)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ec6fa1c-d403-4f23-914f-88bec9e46bbf",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m gpt_api_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter your OpenAI API key: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m nvidia_api_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter your NVIDIA API key: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m gpt_client \u001b[38;5;241m=\u001b[39m OpenAI(\n\u001b[1;32m      5\u001b[0m     api_key\u001b[38;5;241m=\u001b[39mgpt_api_key,\n\u001b[1;32m      6\u001b[0m )\n",
      "File \u001b[0;32m/nas/ucb/k8/shivamsinghal/anaconda3/envs/bounded_cognition/lib/python3.11/site-packages/ipykernel/kernelbase.py:1262\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1260\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[1;32m   1263\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[1;32m   1264\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   1265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1266\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1267\u001b[0m )\n",
      "File \u001b[0;32m/nas/ucb/k8/shivamsinghal/anaconda3/envs/bounded_cognition/lib/python3.11/site-packages/ipykernel/kernelbase.py:1305\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1304\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1307\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "gpt_api_key = input(\"Enter your OpenAI API key: \")\n",
    "nvidia_api_key = input(\"Enter your NVIDIA API key: \")\n",
    "\n",
    "gpt_client = OpenAI(\n",
    "    api_key=gpt_api_key,\n",
    ")\n",
    "os.environ[\"OPENAI_API_KEY\"] = gpt_api_key\n",
    "\n",
    "llama_client = OpenAI(\n",
    "    base_url = \"https://integrate.api.nvidia.com/v1\",\n",
    "    api_key = nvidia_api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c18a7e6-119c-40b3-b426-9ecb3ef5ec5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = \"train_llama3-70b_regenerated_detailed.csv\"\n",
    "train_set = pd.read_csv(TRAIN_PATH)\n",
    "\n",
    "TEST_PATH = \"test_llama3-70b_regenerated_detailed.csv\"\n",
    "test_set = pd.read_csv(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508d3c66-0274-488b-9949-e4e7656ac688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_chosen(row):\n",
    "    return int(\"incorrect\" not in row[\"chosen_type\"])\n",
    "\n",
    "train_set[\"correct_chosen\"] = train_set.apply(correct_chosen, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95cebc9-1175-4f48-9757-106611423030",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_questions = train_set.drop_duplicates(\"questions\")[\"questions\"].tolist()\n",
    "test_questions = test_set.drop_duplicates(\"questions\")[\"questions\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c9ffa3-0a71-4062-916a-37796dd0f448",
   "metadata": {},
   "source": [
    "# Betas being tested\n",
    "## Annotator-specific betas\n",
    "- annotator confidence\n",
    "- time spent per question — but different people might spend different amounts of time, so will need to justify this\n",
    "- number of clicks on the question page\n",
    "\n",
    "## Ground truth betas / other baselines:\n",
    "- Length and correctness agree with each other\n",
    "- GPT-3.5 picks answer 10 times and compared to the provided correct answer\n",
    "easy questions only based on GPT-3.5 difficulty\n",
    "- GPT-4o and GPT-4-turbo answer once and compared to the provided correct answer\n",
    "- Artificially labeled dataset with five fold validation\n",
    "- Wiki dataset frequency\n",
    "- Flesch-kincade\n",
    "\n",
    "## Prompting\n",
    "- GPT-3.5 picks answer 10 times, and GPT-4 judges (turbo and o)\n",
    "- Zero-shot prompting using the criteria of annotator knowledge, resources (e.g., time), and cognitive biases. This will involve the model evaluating the questions + the two responses.\n",
    "- CoT autograder — getting holistic scores (i.e., we get the LLM to give scores for all questions in one go and then get one holistic difficulty score)\n",
    "    - autograder\n",
    "    - simpler CoT prompt with reused zero shot prompt\n",
    "- Individually asking the questions that we had for the autograder\n",
    "- Few shot prompting (either one shot or two shot): depending on the cost just with GPT-3.5?\n",
    "- Try prompting individually for the cognitive biases too?\n",
    "- Potentially fine-tuning on a cogsci dataset?\n",
    "- Maybe tree-of-thought prompting?\n",
    "\n",
    "## Models to try:\n",
    "- GPT-3.5\n",
    "- GPT-4-turbo\n",
    "- GPT-4o\n",
    "- Llama3-8B\n",
    "- Llama3-70B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cd76e9-f339-4a0d-bf20-68a0279a5192",
   "metadata": {},
   "source": [
    "### Ground truth beta based on frequency in Wiki text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd241ff-3c3f-40bb-835a-d25d523e9e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from gensim.corpora import WikiCorpus\n",
    "from gensim.models import TfidfModel\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Path to the Wikipedia dump\n",
    "wiki_bz2 = \"/nas/ucb/shivamsinghal/preference-learning-with-bounded-cognition/enwiki-latest-pages-articles.xml.bz2\"\n",
    "\n",
    "# Load the corpus and wrap it with tqdm for a progress bar\n",
    "class TqdmWikiCorpus(WikiCorpus):\n",
    "    def get_texts(self):\n",
    "        return tqdm(super().get_texts(), desc='Processing documents')\n",
    "\n",
    "wiki_corpus = TqdmWikiCorpus(wiki_bz2)\n",
    "\n",
    "# Create a TF-IDF model\n",
    "tfidf = TfidfModel(wiki_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc54b1a6-22d1-4c20-b1e9-74ffbb3ff578",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_set = set(train_set['questions'].str.lower())\n",
    "\n",
    "def find_question_frequency():\n",
    "    question_freq = {q: 0 for q in questions_set}\n",
    "    for text in tqdm(wiki_corpus.get_texts()):\n",
    "        parsed_text = mwparserfromhell.parse(\" \".join(text))\n",
    "        for question in questions_set:\n",
    "            if question in parsed_text.lower():\n",
    "                question_freq[question] += 1\n",
    "    return question_freq\n",
    "\n",
    "question_frequency = find_question_frequency()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435c64b1-858d-441a-a4de-f5f5aa42f327",
   "metadata": {},
   "source": [
    "### Ground truth beta based on length\n",
    "- Beta = 0.5 when the length of the statements is the same, or the factual correctness is the same\n",
    "- Beta = 0 when the length and the correctness disagree (correct statement is concise)\n",
    "- Beta = 1 when the length and the correctness agree (correct statement is detailed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf861a4-15c2-4946-8d81-613b71872c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ground_truth_difficulty(row):\n",
    "    tag = str(row[\"tag_IDs\"])[:4]\n",
    "    if tag[0] == tag[2] or tag[1] == tag[3]:\n",
    "        return 0.5\n",
    "    elif (tag[0] == \"1\" and tag[1] == \"2\") or (tag[2] == \"1\" and tag[3] == \"2\"):\n",
    "        return 0\n",
    "    elif (tag[0] == \"1\" and tag[1] == \"1\") or (tag[2] == \"1\" and tag[3] == \"1\"):\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd8746d-68e4-4c9c-b044-bb99616a12eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[\"ground_truth_difficulty\"] = train_set.apply(get_ground_truth_difficulty, axis=1)\n",
    "train_set[\"ground_truth_inverse_beta\"] = 1-train_set[\"ground_truth_difficulty\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86024b5-4ef9-4fb8-b2ab-e1d5b89a5e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.to_csv(TRAIN_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2319639e-3ac6-4056-8f7b-38af8a4ade9b",
   "metadata": {},
   "source": [
    "### Ground truth beta based on correctness across 4 copies of question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09fd0d8-b282-4dbf-a98c-00b12baeba2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_incorrect_answer_pairs = train_set[train_set[\"tag_IDs\"].apply(lambda x: str(x)[0] != str(x)[2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3e82f8-6011-41dd-9e82-c125bfdf61d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_data = correct_incorrect_answer_pairs.groupby('questions')['correct_chosen'].sum()\n",
    "count_data = correct_incorrect_answer_pairs.groupby('questions')['correct_chosen'].count()\n",
    "result = dict(sum_data / count_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f49b849-f154-48f1-b5e6-66d1f76465b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ground_truth(row):\n",
    "    if str(row[\"tag_IDs\"])[0] != str(row[\"tag_IDs\"])[2]:\n",
    "        return result[row[\"questions\"]]\n",
    "    else:\n",
    "        return 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d93ab87-f03f-4eae-9eb6-7c968f9768d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[\"ground_truth_approx_correctness_difficulty\"] = train_set.apply(get_ground_truth, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f8e1524-714e-4ed7-a1d7-8a9d7fe9c134",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[\"ground_truth_approx_correctness_beta\"] = 1-train_set[\"ground_truth_approx_correctness_difficulty\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0af1b336-d8e4-408f-9cd0-b7f0ac8a3519",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.to_csv(TRAIN_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbea132-0e7f-44b4-b41c-6b7b32831df5",
   "metadata": {},
   "source": [
    "### Artifically labeled dataset\n",
    "- When there are correct and incorrect pairs, just choose the correct answer\n",
    "- When there are two statements of the same correctness are paired together, choose randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68ca21b2-dcdf-4f52-9e02-026603e70a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chosen_rejected(row):\n",
    "    tag = str(row[\"tag_IDs\"])[:4]\n",
    "    if tag[0] == tag[2]:\n",
    "        # choices = [\"choice1\", \"choice2\"]\n",
    "        # random.shuffle(choices)\n",
    "        # return row[choices[0]], row[choices[1]], row[f\"{choices[0]}_type\"], row[f\"{choices[1]}_type\"]\n",
    "        chosen = \"choice1\" if tag[1] == \"2\" else \"choice2\"\n",
    "        rejected = \"choice2\" if chosen == \"choice1\" else \"choice1\"\n",
    "        return row[chosen], row[rejected], row[f\"{chosen}_type\"], row[f\"{rejected}_type\"]\n",
    "    elif tag[0] == \"1\":\n",
    "        return row[\"choice1\"], row[\"choice2\"], row[\"choice1_type\"], row[\"choice2_type\"]\n",
    "    else:\n",
    "        return row[\"choice2\"], row[\"choice1\"], row[\"choice2_type\"], row[\"choice1_type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be5c22f9-99f3-4d6d-8ba2-5f64d9550b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[[\"chosen\", \"rejected\", \"chosen_type\", \"rejected_type\"]] = train_set.apply(get_chosen_rejected, axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a08fce6-1ca5-437f-9440-d8a418cf5372",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "train_set['question_id'] = train_set.index // 4\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "fold_number = 1\n",
    "for train_index, test_index in gkf.split(train_set, groups=train_set['question_id']):\n",
    "    fold_train = train_set.iloc[train_index]\n",
    "    fold_test = train_set.iloc[test_index]\n",
    "\n",
    "    fold_train.to_csv(f'train_artificial_fold_{fold_number}.csv', index=False)\n",
    "    fold_test.to_csv(f'test_artificial_fold_{fold_number}.csv', index=False)\n",
    "    \n",
    "    print(f\"Training and testing datasets for fold {fold_number} saved.\")\n",
    "    fold_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9872569e-44af-44cf-8509-005212a17427",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.to_csv(\"artificial_train_dataset_llama70B.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e5112bb-24b0-45b7-b9fe-dce6e24d0e1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chosen_type\n",
       "correct_concise       627\n",
       "correct_detailed      272\n",
       "incorrect_detailed     98\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.groupby(\"chosen_type\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d073a91-79cf-4487-b5d1-ad576fe4af1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chosen_type\n",
       "correct_concise       692\n",
       "correct_detailed      207\n",
       "incorrect_concise      53\n",
       "incorrect_detailed     45\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old.groupby(\"chosen_type\").size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39156755-fa2d-4172-8051-7eaef66ec029",
   "metadata": {},
   "source": [
    "### GPT-3.5 answers 10 times / GPT-4-turbo and GPT-4o answer once\n",
    "- judged based on provided correct answer\n",
    "- judged by GPT-4-turbo / o\n",
    "\n",
    "Get GPT-3.5 to answer the same question multiple times and count the number of times that it gets it correct\n",
    "- When the choices differ in their factual correctness, the factually correct one is correct.\n",
    "- When the choices are both incorrect, the concise one would be considered correct. Even though there is technically no correct answer, it makes sense that we wouldn't want an LLM to output detailed information that is wrong.\n",
    "- Whent the choices are both correct, either the detailed or the concise one would be considered correct. Here, the difficulty is simply set to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd600806-d59e-4bd3-9811-5d604dbc8de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output_gpt(question, model=\"gpt-3.5-turbo\"):\n",
    "    chat_completion = gpt_client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"Please answer the multiple choice question using only one letter, either 'A' or 'B'.\"  \n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": question\n",
    "            }\n",
    "        ],\n",
    "        model=model,\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f97f05-6a48-4c8f-bfa9-563b7f162df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set mcqs generation\n",
    "train_set_mcqs = []\n",
    "train_set_mcq_answers = []\n",
    "for index, row in train_set.iterrows():\n",
    "    choices_order = random.sample([\"choice1\", \"choice2\"], 2)\n",
    "    qs = f\"{row['questions']} (A.) {row[choices_order[0]]} (B.) {row[choices_order[1]]}\"\n",
    "    train_set_mcqs.append(qs)\n",
    "\n",
    "    if row[\"choice1_type\"].split(\"_\")[0] == \"correct\" and row[\"choice2_type\"].split(\"_\")[0] == \"correct\":\n",
    "        train_set_mcq_answers.append(None)\n",
    "        continue\n",
    "        \n",
    "    if row[\"choice1_type\"].split(\"_\")[0] == \"incorrect\" and row[\"choice2_type\"].split(\"_\")[0] == \"incorrect\":\n",
    "        chosen = \"choice1\" if row[\"choice1_type\"].split(\"_\")[1] == \"concise\" else \"choice2\"\n",
    "    else:\n",
    "        chosen = \"choice1\" if row[\"choice1_type\"].split(\"_\")[0] == \"correct\" else \"choice2\"\n",
    "    \n",
    "    chosen = \"A\" if choices_order[0] == chosen else \"B\"\n",
    "    train_set_mcq_answers.append(chosen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9548237-07b0-4455-aa79-e7174d7ee6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4_turbo_answers = []\n",
    "partial_generate_output_gpt = partial(generate_output_gpt, model=\"gpt-4-turbo\")\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    results = list(tqdm(executor.map(partial_generate_output_gpt, train_set_mcqs), total=len(train_set_mcqs)))\n",
    "gpt_4_turbo_answers.extend(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d275d7e-93ef-463a-9b6f-9418cefaed16",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4o_answers = []\n",
    "partial_generate_output_gpt = partial(generate_output_gpt, model=\"gpt-4o\")\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    results = list(tqdm(executor.map(partial_generate_output_gpt, train_set_mcqs), total=len(train_set_mcqs)))\n",
    "gpt_4o_answers.extend(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a09c2a-c22f-489d-8b9a-0db9ab7736a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(gpt_4_turbo_answers)):\n",
    "    answer = gpt_4_turbo_answers[i]\n",
    "    if answer not in [\"A\", \"B\"] and answer[0] not in [\"A\", \"B\"]:\n",
    "        print(answer)\n",
    "    else:\n",
    "        gpt_4_turbo_answers[i] = answer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ad42a7-31b6-4d43-bd96-62951aad217e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(gpt_4o_answers)):\n",
    "    answer = gpt_4o_answers[i]\n",
    "    if answer not in [\"A\", \"B\"] and answer[0] not in [\"A\", \"B\"]:\n",
    "        print(answer)\n",
    "    else:\n",
    "        gpt_4o_answers[i] = answer[0]\n",
    "        continue\n",
    "\n",
    "    if answer[1] not in [\"A\", \"B\"]:\n",
    "        print(answer)\n",
    "    else:\n",
    "        gpt_4o_answers[i] = answer[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5f8aae-be4b-4436-bde5-dc5c1b6ac275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_question_gt(q):\n",
    "    if train_set_mcq_answers[q] is None:\n",
    "        return 10\n",
    "    score = 0\n",
    "    for i in range(10):\n",
    "        response = generate_output_gpt(train_set_mcqs[q])\n",
    "        while response[0] not in {\"A\", \"B\"}:\n",
    "            print(\"IN LOOP\")\n",
    "            response = generate_output_gpt(train_set_mcqs[q])\n",
    "        if response[0] == train_set_mcq_answers[q]:\n",
    "            score += 1\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de39c4f-30ad-469c-bd1d-b8624960f7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_question_gpt4(q, gpt_4_answers):\n",
    "    score = 0\n",
    "    for i in range(10):\n",
    "        response = generate_output_gpt(train_set_mcqs[q])\n",
    "        while response[0] not in {\"A\", \"B\"}:\n",
    "            print(\"IN LOOP\")\n",
    "            response = generate_output_gpt(train_set_mcqs[q])\n",
    "        if response[0] == gpt_4_answers[q]:\n",
    "            score += 1\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68d1c79-0276-4e60-ac12-20c651f1d13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores_gt = []\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = [executor.submit(process_question_gt, q) for q in tqdm(range(len(train_set_mcqs)))]\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        train_scores_gt.append(future.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82c5f9f-b4aa-4edb-b098-4127a0aa3e2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_set[\"gpt-3.5_ground_truth_difficulty\"] = np.array(train_scores_gt)/10\n",
    "train_set[\"gpt-3.5_ground_truth_inverse_beta\"] = 1-train_set[\"gpt-3.5_ground_truth_difficulty\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf26d9ca-c90e-4943-aa73-32539d77fb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores_gpt4 = []\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = [executor.submit(process_question_gpt4, q, gpt_4_turbo_answers) for q in tqdm(range(len(train_set_mcqs)))]\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        train_scores_gpt4.append(future.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fb6b8c-cafb-4422-afbb-b4088cb194f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[\"gpt-3.5_gpt-4-turbo_judge_difficulty\"] = np.array(train_scores_gpt4)/10\n",
    "train_set[\"gpt-3.5_gpt-4-turbo_judge_inverse_beta\"] = 1-train_set[\"gpt-3.5_gpt-4-turbo_judge_difficulty\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9af349-3075-4324-b7ca-18501a6d05cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores_gpt4o = []\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = [executor.submit(process_question_gpt4, q, gpt_4o_answers) for q in tqdm(range(len(train_set_mcqs)))]\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        train_scores_gpt4o.append(future.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259a1d45-83b4-4f8e-9062-3bd628023915",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[\"gpt-3.5_gpt-4o_judge_difficulty\"] = np.array(train_scores_gpt4o)/10\n",
    "train_set[\"gpt-3.5_gpt-4o_judge_inverse_beta\"] = 1-train_set[\"gpt-3.5_gpt-4o_judge_difficulty\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5889d12c-2867-4f20-8149-f55a6ac598c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4_turbo_gt_comparison = []\n",
    "for i in range(len(gpt_4_turbo_answers)):\n",
    "    if train_set_mcq_answers[i] is None:\n",
    "        gpt_4_turbo_gt_comparison.append(1)\n",
    "        continue\n",
    "    answer = gpt_4_turbo_answers[i]\n",
    "    if answer == train_set_mcq_answers[i]:\n",
    "        gpt_4_turbo_gt_comparison.append(1)\n",
    "    else:\n",
    "        gpt_4_turbo_gt_comparison.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b6e160-4b1c-4cfd-a185-2a7682f9a1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[\"gpt-4-turbo_ground_truth_difficulty\"] = np.array(gpt_4_turbo_gt_comparison)\n",
    "train_set[\"gpt-4-turbo_ground_truth_beta\"] = 1-train_set[\"gpt-4-turbo_ground_truth_difficulty\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f591c9a6-d613-43c0-967e-48307b4ed2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4o_gt_comparison = []\n",
    "for i in range(len(gpt_4o_answers)):\n",
    "    if train_set_mcq_answers[i] is None:\n",
    "        gpt_4o_gt_comparison.append(1)\n",
    "        continue\n",
    "    answer = gpt_4o_answers[i]\n",
    "    if answer == train_set_mcq_answers[i]:\n",
    "        gpt_4o_gt_comparison.append(1)\n",
    "    else:\n",
    "        gpt_4o_gt_comparison.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1065fad-128a-4030-beec-d9c0acbe7c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[\"gpt-4o_ground_truth_difficulty\"] = np.array(gpt_4o_gt_comparison)\n",
    "train_set[\"gpt-4o_ground_truth_beta\"] = 1-train_set[\"gpt-4o_ground_truth_difficulty\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71326c58-f590-4c5b-a746-e29d7a642148",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.to_csv(TRAIN_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71231cca-05a0-4712-9903-9b1311dff75f",
   "metadata": {},
   "source": [
    "## Zero-shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d75ff3-adcb-40dd-b776-373c76d07793",
   "metadata": {},
   "source": [
    "Evaluating both the question and answer groups without examples \n",
    "\n",
    "Prompt is here: difficulty_evaluation_prompt_zero_shot.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e22fa3f-c300-48f8-9286-fe18af73b578",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"difficulty_evaluation_prompt_zero_shot.txt\", \"r\") as f:\n",
    "    autograder_prompt = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c607a87e-9070-4884-89cc-64d0818d51a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_zero_shot_output(question, response1, response2, model=\"gpt-3.5-turbo\", temperature=0.0):\n",
    "    chat_completion = gpt_client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": autograder_prompt.format(question=question, response1=response1, response2=response2)\n",
    "            }\n",
    "        ],\n",
    "        model=model,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd95a0be-e5e6-4bda-92ec-3c5187ee4404",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(row, model=\"gpt-3.5-turbo\"):\n",
    "    response = generate_zero_shot_output(row[\"questions\"], row[\"choice1\"], row[\"choice2\"], model=model)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75da4de-8fea-4c47-88ac-6cf898e1079e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gpt_35_zero_shot = []\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = [executor.submit(generate_response, row) for _, row in train_set.iterrows()]\n",
    "    for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n",
    "        gpt_35_zero_shot.append(future.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff042998-be81-4c7d-b53e-4e8e25f1d2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4o_zero_shot = []\n",
    "\n",
    "partial_generate_response = partial(generate_response, model=\"gpt-4o\")\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = [executor.submit(partial_generate_response, row) for _, row in train_set.iterrows()]\n",
    "    for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n",
    "        gpt_4o_zero_shot.append(future.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ae9910-8f26-4d01-adf9-9513f01bad6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4_turbo_zero_shot = []\n",
    "\n",
    "partial_generate_response = partial(generate_response, model=\"gpt-4-turbo\")\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = [executor.submit(partial_generate_response, row) for _, row in train_set.iterrows()]\n",
    "    for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n",
    "        gpt_4_turbo_zero_shot.append(future.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55057a36-93e2-408a-bf03-8fdecd481d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_score_format(arr):\n",
    "    pattern = r\"SCORE:\\s*\\d+\"\n",
    "    matches = []\n",
    "    for string in arr:\n",
    "        match = re.search(pattern, string)\n",
    "        if match:\n",
    "            # print(match.group())\n",
    "            matches.append(match.group())\n",
    "        else:\n",
    "            print(string)\n",
    "            \n",
    "    return get_score_num(matches)\n",
    "\n",
    "def get_score_num(arr):\n",
    "    nums = []\n",
    "    pattern = r\"SCORE:\\s*(\\d+)\"\n",
    "    for string in arr:\n",
    "        match = re.search(pattern, string)\n",
    "        if match:\n",
    "            nums.append(float(match.group(1)))\n",
    "            \n",
    "    return nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8336bd-6899-4786-b175-e13a33c2ad30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "matches = extract_score_format(gpt_35_zero_shot)\n",
    "assert len(matches)==len(train_set)\n",
    "gpt_35_zero_shot = matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b402d28-7125-4127-80f0-b259358adbf8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "matches = extract_score_format(gpt_4_turbo_zero_shot)\n",
    "assert len(matches)==len(train_set)\n",
    "gpt_4_turbo_zero_shot = matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574de4a0-7426-4e83-82f4-00bfe9e513ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "matches = extract_score_format(gpt_4o_zero_shot)\n",
    "assert len(matches)==len(train_set)\n",
    "gpt_4o_zero_shot = matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ef3c5e-f04d-466f-b165-004f9b4e377d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[\"gpt-3.5_zero_shot_difficulty\"] = gpt_35_zero_shot\n",
    "train_set[\"gpt-4-turbo_zero_shot_difficulty\"] = gpt_4_turbo_zero_shot\n",
    "train_set[\"gpt-4o_zero_shot_difficulty\"] = gpt_4o_zero_shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82e6633-7725-4add-9ca7-f6507b5b6eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_val = train_set[\"gpt-3.5_zero_shot_difficulty\"].max()\n",
    "min_val = train_set[\"gpt-3.5_zero_shot_difficulty\"].min()\n",
    "\n",
    "train_set[\"gpt-3.5_zero_shot_beta\"] = max_val + min_val - train_set[\"gpt-3.5_zero_shot_difficulty\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd216d5e-0ef5-471d-9421-d897f048d636",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_val = train_set[\"gpt-4-turbo_zero_shot_difficulty\"].max()\n",
    "min_val = train_set[\"gpt-4-turbo_zero_shot_difficulty\"].min()\n",
    "\n",
    "train_set[\"gpt-4-turbo_zero_shot_beta\"] = max_val + min_val - train_set[\"gpt-4-turbo_zero_shot_difficulty\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3813ad92-f7de-444b-9aa5-810c73316a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_val = train_set[\"gpt-4o_zero_shot_difficulty\"].max()\n",
    "min_val = train_set[\"gpt-4o_zero_shot_difficulty\"].min()\n",
    "\n",
    "train_set[\"gpt-4o_zero_shot_beta\"] = max_val + min_val - train_set[\"gpt-4o_zero_shot_difficulty\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4347e34-39f3-4015-a5d1-6dd4e52881d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.to_csv(TRAIN_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5740ac1-47eb-4e37-b132-a21c45629e35",
   "metadata": {},
   "source": [
    "### CoT Prompting\n",
    "Prompt is here: difficulty_evaluation_prompt_cot.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a5dfe9-c90d-405f-9fa5-c5fb94e3c3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"difficulty_evaluation_prompt_cot.txt\", \"r\") as f:\n",
    "    autograder_prompt = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a67b52c-b17f-405b-8b3c-7b3a915a6567",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cot_output(question, response1, response2, model=\"gpt-3.5-turbo\", temperature=0.0):\n",
    "    chat_completion = gpt_client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": autograder_prompt.format(question=question, response1=response1, response2=response2)\n",
    "            }\n",
    "        ],\n",
    "        model=model,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea55900-1109-4cba-8d16-a84a54ecd9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(row, model=\"gpt-3.5-turbo\"):\n",
    "    response = generate_cot_output(row[\"questions\"], row[\"choice1\"], row[\"choice2\"], model=model)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0570e93-00aa-4dd4-af24-49a5f23584b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_35_cot = []\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = [executor.submit(generate_response, row) for _, row in train_set.iterrows()]\n",
    "    for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n",
    "        gpt_35_cot.append(future.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df24bf76-0f13-4c41-a18f-4886f35e3bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4o_cot = []\n",
    "\n",
    "partial_generate_response = partial(generate_response, model=\"gpt-4o\")\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = [executor.submit(partial_generate_response, row) for _, row in train_set.iterrows()]\n",
    "    for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n",
    "        gpt_4o_cot.append(future.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d27eaf-c628-4408-8575-14e17f8437f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4_turbo_cot = []\n",
    "\n",
    "partial_generate_response = partial(generate_response, model=\"gpt-4-turbo\")\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = [executor.submit(partial_generate_response, row) for _, row in train_set.iterrows()]\n",
    "    for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n",
    "        gpt_4_turbo_cot.append(future.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6824864b-3758-4b47-859b-7e17530c2a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores_and_reasonings(generated_array):\n",
    "    question1_scores, question2_scores, question3_scores, question4_scores, question5_scores, question6_scores, question7_scores = ([] for _ in range(7))\n",
    "    question1_reasonings, question2_reasonings, question3_reasonings, question4_reasonings, question5_reasonings, question6_reasonings, question7_reasonings = ([] for _ in range(7))\n",
    "    \n",
    "    scores_lists = [question1_scores, question2_scores, question3_scores, question4_scores, question5_scores, question6_scores, question7_scores]\n",
    "    reasonings_lists = [question1_reasonings, question2_reasonings, question3_reasonings, question4_reasonings, question5_reasonings, question6_reasonings, question7_reasonings]\n",
    "    \n",
    "    errors = []\n",
    "\n",
    "    for index, data in enumerate(generated_array):\n",
    "        try:\n",
    "            segments = re.split(r'(\\d+\\.[ab])', data)\n",
    "            assert len(segments) == 29\n",
    "            \n",
    "            current_key = ''\n",
    "            for segment in segments:\n",
    "                if re.match(r'\\d+\\.[ab]', segment):\n",
    "                    current_key = segment\n",
    "                else:\n",
    "                    if current_key and segment.strip():\n",
    "                        question_number, part = current_key.split('.')\n",
    "                        question_index = int(question_number) - 1\n",
    "                        if part == 'a':\n",
    "                            reasonings_lists[question_index].append(segment.strip())\n",
    "                        elif part == 'b':\n",
    "                            scores_lists[question_index].append(float(segment.strip()))\n",
    "        except Exception as e:\n",
    "            errors.append(index)\n",
    "            for scores, reasonings in zip(scores_lists, reasonings_lists):\n",
    "                scores.append(None)\n",
    "                reasonings.append(None)\n",
    "\n",
    "    return scores_lists, reasonings_lists, errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54de4612-a33b-4780-89c2-37c8dbdd1137",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_35_scores_lists, gpt_35_reasonings_lists, errors = get_scores_and_reasonings(gpt_35_cot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe824cc-3809-4f10-af3c-d2ed5f645837",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [[] for _ in range(7)]\n",
    "reasonings = [[] for _ in range(7)]\n",
    "\n",
    "for output in gpt_35_cot:\n",
    "    score, all_reasonings, errors = get_scores_and_reasonings([output])\n",
    "    assert len(scores) == 7\n",
    "    assert all(len(r) > 0 for r in all_reasonings)\n",
    "\n",
    "    for i in range(7):\n",
    "        scores[i].append(score[i][0])\n",
    "        reasonings[i].append(all_reasonings[i][0])\n",
    "\n",
    "question1_scores, question2_scores, question3_scores, question4_scores, \\\n",
    "question5_scores, question6_scores, question7_scores = scores\n",
    "\n",
    "question1_reasonings, question2_reasonings, question3_reasonings, question4_reasonings, \\\n",
    "question5_reasonings, question6_reasonings, question7_reasonings = reasonings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92473dd8-23a9-4282-ba4b-e69e9a19385e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(scores)):\n",
    "    train_set[f\"gpt-3.5_CoT_AG_question-{i+1}_difficulty_score\"] = scores[i]\n",
    "    train_set[f\"gpt-3.5_CoT_AG_question-{i+1}_reasoning\"] = reasonings[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b87c1c-e8eb-4cf9-a243-b4784d32e9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [f\"gpt-3.5_CoT_AG_question-{i}_difficulty_score\" for i in range(1, 7)]\n",
    "train_set['gpt-3.5_CoT_AG_mean_difficulty_score'] = train_set[columns].mean(axis=1)\n",
    "train_set['gpt-3.5_CoT_AG_max_difficulty_score'] = train_set[columns].max(axis=1)\n",
    "train_set['gpt-3.5_CoT_AG_median_difficulty_score'] = train_set[columns].median(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525cec9d-fe69-4dee-82f7-ca4c5b0135c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4_turbo_scores_lists, gpt_4_turbo_reasonings_lists, errors = get_scores_and_reasonings(gpt_4_turbo_cot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f31263-3b68-483d-852b-718b40590188",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [[] for _ in range(7)]\n",
    "reasonings = [[] for _ in range(7)]\n",
    "\n",
    "for index in range(len(gpt_4_turbo_cot)):\n",
    "    output = gpt_4_turbo_cot[index]\n",
    "    score, all_reasonings, errors = get_scores_and_reasonings([output])\n",
    "    assert len(score) == 7\n",
    "    assert len(all_reasonings) == 7\n",
    "    assert all(len(r) > 0 for r in all_reasonings)\n",
    "    try:\n",
    "        for i in range(7):\n",
    "            # assert scores[i][0] is not None, i\n",
    "            # assert all_reasonings[i][0] is not None, i\n",
    "            scores[i].append(score[i][0])\n",
    "            reasonings[i].append(all_reasonings[i][0])\n",
    "    except:\n",
    "        print(len(score))\n",
    "        print(len(all_reasonings))\n",
    "        print(all_reasonings)\n",
    "        print(score)\n",
    "        print(output)\n",
    "        print(index)\n",
    "        assert False\n",
    "\n",
    "# manually correct for any statements that error out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1df0a87-9412-4858-b6bf-c57b4c76089c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(scores)):\n",
    "    assert len(scores[i]) == 1000\n",
    "    assert len(reasonings[i]) == 1000\n",
    "    for j in range(len(scores[i])):\n",
    "        assert reasonings[i][j] is not None, reasonings[i][j]\n",
    "    train_set[f\"gpt-4-turbo_CoT_AG_question-{i+1}_difficulty_score\"] = scores[i]\n",
    "    train_set[f\"gpt-4-turbo_CoT_AG_question-{i+1}_reasoning\"] = reasonings[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b940e21-e65a-4927-b304-1a6b9681adc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [f\"gpt-4-turbo_CoT_AG_question-{i}_difficulty_score\" for i in range(1, 7)]\n",
    "train_set['gpt-4-turbo_CoT_AG_mean_difficulty_score'] = train_set[columns].mean(axis=1)\n",
    "train_set['gpt-4-turbo_CoT_AG_max_difficulty_score'] = train_set[columns].max(axis=1)\n",
    "train_set['gpt-4-turbo_CoT_AG_median_difficulty_score'] = train_set[columns].median(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f2a48c-0db5-453f-ba0c-8e890546e18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4o_scores_lists, gpt_4o_reasonings_lists, errors = get_scores_and_reasonings(gpt_4o_cot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a63b1ce-5d64-4cb4-bced-2f4906c89a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [[] for _ in range(7)]\n",
    "reasonings = [[] for _ in range(7)]\n",
    "\n",
    "for index in range(len(gpt_4o_cot)):\n",
    "    output = gpt_4o_cot[index]\n",
    "    score, all_reasonings, errors = get_scores_and_reasonings([output])\n",
    "    assert len(score) == 7\n",
    "    assert len(all_reasonings) == 7\n",
    "    assert all(len(r) > 0 for r in all_reasonings)\n",
    "    try:\n",
    "        for i in range(7):\n",
    "            scores[i].append(score[i][0])\n",
    "            reasonings[i].append(all_reasonings[i][0])\n",
    "    except:\n",
    "        print(len(score))\n",
    "        print(len(all_reasonings))\n",
    "        print(score)\n",
    "        print(output)\n",
    "        print(index)\n",
    "        assert False\n",
    "\n",
    "# manually correct for any statements that error out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f88549-c288-4892-b699-833ea40f1d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(scores)):\n",
    "    train_set[f\"gpt-4o_CoT_AG_question-{i+1}_difficulty_score\"] = scores[i]\n",
    "    train_set[f\"gpt-4o_CoT_AG_question-{i+1}_reasoning\"] = reasonings[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c556483-9f73-4909-b6e2-437e51a7d26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [f\"gpt-4o_CoT_AG_question-{i}_difficulty_score\" for i in range(1, 7)]\n",
    "train_set['gpt-4o_CoT_AG_mean_difficulty_score'] = train_set[columns].mean(axis=1)\n",
    "train_set['gpt-4o_CoT_AG_max_difficulty_score'] = train_set[columns].max(axis=1)\n",
    "train_set['gpt-4o_CoT_AG_median_difficulty_score'] = train_set[columns].median(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ff3046-67c6-4567-82b1-89a465bb7915",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.to_csv(TRAIN_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb4c0b4-c491-4e07-b5ee-3f3e37fce683",
   "metadata": {},
   "source": [
    "Llama 70B executions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6963a92-033f-4335-8559-91b5e527e5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chat(question, response1, response2):\n",
    "    completion = llama_client.chat.completions.create(\n",
    "        model=\"meta/llama3-70b-instruct\",\n",
    "        messages=[  \n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": autograder_prompt.format(question=question, response1=response1, response2=response2)\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.65,\n",
    "        top_p=1,\n",
    "        max_tokens=1024,\n",
    "    )\n",
    "    \n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "def process_row(row):\n",
    "    response = get_chat(row[\"questions\"], row[\"incorrect_statements\"])\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dae6faf-d82d-4e70-976e-2c08e8626859",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_statements = []\n",
    "for index, row in tqdm(train_set[len(generated_statements):].iterrows()):\n",
    "    response = get_chat(row[\"questions\"], row[\"choice1\"], row[\"choice2\"])\n",
    "    generated_statements.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a279fb43-9816-4c55-a344-a5432ed0c32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [[] for _ in range(7)]\n",
    "reasonings = [[] for _ in range(7)]\n",
    "\n",
    "for index in range(len(generated_statements)):\n",
    "    output = generated_statements[index]\n",
    "    score, all_reasonings, errors = get_scores_and_reasonings([output])\n",
    "    assert len(score) == 7\n",
    "    assert len(all_reasonings) == 7\n",
    "    assert all(len(r) > 0 for r in all_reasonings)\n",
    "    try:\n",
    "        for i in range(7):\n",
    "            scores[i].append(score[i][0])\n",
    "            reasonings[i].append(all_reasonings[i][0])\n",
    "    except:\n",
    "        print(len(score))\n",
    "        print(len(all_reasonings))\n",
    "        print(score)\n",
    "        print(output)\n",
    "        print(index)\n",
    "        assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625d8a5b-ba8d-49bc-985f-3c01be426402",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(scores)):\n",
    "    train_set[f\"llama_3-70B_CoT_AG_question-{i+1}_difficulty_score\"] = scores[i]\n",
    "    train_set[f\"llama_3-70B_CoT_AG_question-{i+1}_reasoning\"] = reasonings[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e2f661-66dd-4f3e-a6b4-8f26d900a967",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [f\"llama_3-70B_CoT_AG_question-{i}_difficulty_score\" for i in range(1, 7)]\n",
    "train_set['llama_3-70B_CoT_AG_mean_difficulty_score'] = train_set[columns].mean(axis=1)\n",
    "train_set['llama_3-70B_CoT_AG_max_difficulty_score'] = train_set[columns].max(axis=1)\n",
    "train_set['llama_3-70B_CoT_AG_median_difficulty_score'] = train_set[columns].median(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ce2f84-f18e-4ee8-bb89-1273ef23b0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.dropna().to_csv(TRAIN_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf3c76e-74e1-4eb9-aacb-18c40692457e",
   "metadata": {},
   "source": [
    "### Simpler CoT\n",
    "repurposing the zero shot prompt, but asking the LLM to reason explicitly\n",
    "\n",
    "The prompt is here: difficulty_evaluation_prompt_simpler_cot.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fad2a3b-8b15-4858-a3d3-433f882c4058",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"difficulty_evaluation_prompt_simpler_cot.txt\", \"r\") as f:\n",
    "    autograder_prompt = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a0e8c5-6c37-4ce1-9cab-57c07aebf935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_simple_cot_output(question, response1, response2, model=\"gpt-3.5-turbo\", temperature=0.0):\n",
    "    chat_completion = gpt_client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": autograder_prompt.format(question=question, response1=response1, response2=response2)\n",
    "            }\n",
    "        ],\n",
    "        model=model,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26e93c7-50d8-449a-b398-c8c8b2a0467a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(row, model=\"gpt-3.5-turbo\"):\n",
    "    response = generate_simple_cot_output(row[\"questions\"], row[\"choice1\"], row[\"choice2\"], model=model)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c9863b-c2ee-4339-a363-b94e9adca64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_35_cot_simpler = []\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = [executor.submit(generate_response, row) for _, row in train_set.iterrows()]\n",
    "    for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n",
    "        gpt_35_cot_simpler.append(future.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0303d5a-2d83-4122-9f8e-ed0c8e8998d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4o_cot_simpler = []\n",
    "\n",
    "partial_generate_response = partial(generate_response, model=\"gpt-4o\")\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = [executor.submit(partial_generate_response, row) for _, row in train_set.iterrows()]\n",
    "    for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n",
    "        gpt_4o_cot_simpler.append(future.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f11240-0861-44b2-8fb6-3847b25f673d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4_turbo_cot_simpler = []\n",
    "\n",
    "partial_generate_response = partial(generate_response, model=\"gpt-4-turbo\")\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = [executor.submit(partial_generate_response, row) for _, row in train_set.iterrows()]\n",
    "    for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n",
    "        gpt_4_turbo_cot_simpler.append(future.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01057551-9e94-49fd-a931-7757192573f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_numeric_values(strings):\n",
    "    numeric_values = []\n",
    "    for string in strings:\n",
    "        numbers = re.findall(r'[0-9]+(?:\\.[0-9]+)?', string)\n",
    "        if numbers:\n",
    "            numeric_values.append(float(numbers[-1]))\n",
    "    return numeric_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c18306-efa7-4efb-9930-f8ab053ed0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[\"gpt-3.5_simpler_CoT_difficulty_scores\"] = get_last_numeric_values(gpt_35_cot_simpler)\n",
    "train_set[\"gpt-4-turbo_simpler_CoT_difficulty_scores\"] = get_last_numeric_values(gpt_4_turbo_cot_simpler)\n",
    "train_set[\"gpt-4o_simpler_CoT_difficulty_scores\"] = get_last_numeric_values(gpt_4o_cot_simpler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec26d7c-316f-4082-a97b-c8c71eb81e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[\"gpt-3.5_simpler_CoT_reasoning\"] = gpt_35_cot_simpler\n",
    "train_set[\"gpt-4-turbo_simpler_CoT_reasoning\"] = gpt_4_turbo_cot_simpler\n",
    "train_set[\"gpt-4o_simpler_CoT_reasoning\"] = gpt_4o_cot_simpler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d330f287-8179-46b1-b051-7a1de150aee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.to_csv(TRAIN_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b7187c-7db2-4cc1-9bb4-6f10c04c2704",
   "metadata": {},
   "source": [
    "### Individual CoT AG questions\n",
    "Asking the LLM the questions from the AG separately but with the same preamble\n",
    "\n",
    "The prompt is here: difficulty_evaluation_prompt_individual_question.txt\n",
    "\n",
    "Use one of the following questions:\n",
    "1. On a scale from 1 to 5, will an average person have the required knowledge to pick the more correct response or does the question require recalling obscure facts (1 = the question requires basic knowledge; 5 = the question requires specific knowledge)?\n",
    "2. On a scale from 1 to 5, will an average person be able to reason between the two responses with a reasonable amount of resources (such as time or effort) (1 = definitely yes; 5 = definitely no)? \n",
    "3. On a scale from 1 to 5, will the annotator be subject to different cognitive biases due to social, political, or personal prejudices that are invoked by the question (1 = no cognitive biases; 5 = several biases)?\n",
    "4. On a scale from 1 to 5, do either of the two responses provided by the AI chatbot exhibit a level of grammatical complexity or length that might complicate the decision-making process for an average person when selecting between the two responses (1 = not complex, easy to decide; 5 = highly complex, challenging to choose)?\n",
    "5. On a scale form 1 to 5, is the content in either of the responses provided by the AI chatbot particularly misleading? Ask yourself the following questions: Is there manipulative or confusing language? Are there incorrect or unnecessary facts or statistics? Is incorrect logical reasoning used? (1 = no, the responses are not misleading; 5 = yes, one or both of the responses will mislead an average person)\n",
    "6. Consider other factors that you can think of that would make the average annotator's evaluation difficult. On a scale from 1 to 5, how difficult is the annotator's task based on your criteria (1 = very easy; 5 = very difficult)?\n",
    "7. Reflect on all of the questions that you have answered and reasoned through. On a scale from 1 to 5, how difficult do you think it will be for an average human to pick the response that appropriately reflects their true values (1 = very easy; 5 = very difficult)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c15972a-5423-4132-9396-c8a3ac134cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "question1 = \"On a scale from 1 to 5, will an average person have the required knowledge to pick the more correct response or does the question require recalling obscure facts (1 = the question requires basic knowledge; 5 = the question requires specific knowledge)?\"\n",
    "question2 = \"On a scale from 1 to 5, will an average person be able to reason between the two responses with a reasonable amount of resources (such as time or effort) (1 = definitely yes; 5 = definitely no)?\" \n",
    "question3 = \"On a scale from 1 to 5, will the annotator be subject to different cognitive biases due to social, political, or personal prejudices that are invoked by the question (1 = no cognitive biases; 5 = several biases)?\"\n",
    "question4 = \"On a scale from 1 to 5, do either of the two responses provided by the AI chatbot exhibit a level of grammatical complexity or length that might complicate the decision-making process for an average person when selecting between the two responses (1 = not complex, easy to decide; 5 = highly complex, challenging to choose)?\"\n",
    "question5 = \"On a scale form 1 to 5, is the content in either of the responses provided by the AI chatbot particularly misleading? Ask yourself the following questions: Is there manipulative or confusing language? Are there incorrect or unnecessary facts or statistics? Is incorrect logical reasoning used? (1 = no, the responses are not misleading; 5 = yes, one or both of the responses will mislead an average person)\"\n",
    "question6 = \"Consider other factors that you can think of that would make the average annotator's evaluation difficult. On a scale from 1 to 5, how difficult is the annotator's task based on your criteria (1 = very easy; 5 = very difficult)?\"\n",
    "question7 = \"Reflect on all of the questions that you have answered and reasoned through. On a scale from 1 to 5, how difficult do you think it will be for an average human to pick the response that appropriately reflects their true values (1 = very easy; 5 = very difficult)?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6895cfb5-929b-462c-b61f-ebdda8420bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [question1, question2, question3, question4, question5, question6, question7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cb93b2-32b4-469e-9753-ba547735ff71",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"difficulty_evaluation_prompt_individual_question.txt\", \"r\") as f:\n",
    "    autograder_prompt = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2b9ede-25fe-40ff-bf9b-a3f8025a57b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_individual_cot_output(question, response1, response2, reasoning_question, model=\"gpt-3.5-turbo\", temperature=0.0):\n",
    "    chat_completion = gpt_client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": autograder_prompt.format(question=question, response1=response1, response2=response2, reasoning_question=reasoning_question)\n",
    "            }\n",
    "        ],\n",
    "        model=model,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a59984-b1f6-4132-811b-3bc832d0fd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(row, reasoning_question, model=\"gpt-3.5-turbo\"):\n",
    "    response = generate_individual_cot_output(row[\"questions\"], row[\"choice1\"], row[\"choice2\"], reasoning_question, model=model)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4160892-08c4-47d1-97f4-04c3f244d1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_35_cot_individual = [[] for _ in range(len(questions))]\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = {}\n",
    "    for i, question in enumerate(questions):\n",
    "        futures[question] = []\n",
    "        for _, row in train_set.iterrows():\n",
    "            future = executor.submit(generate_response, row, question, model=\"gpt-3.5-turbo\")\n",
    "            futures[question].append(future)\n",
    "    \n",
    "    for i, question in enumerate(questions):\n",
    "        gpt_35_cot_individual[i] = [future.result() for future in tqdm(concurrent.futures.as_completed(futures[question]), total=len(futures[question]), desc=f\"Processing {question}\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c53081-162d-4d50-aab0-1e7caa0a6416",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4_turbo_cot_individual = [[] for _ in range(len(questions))]\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = {}\n",
    "    for i, question in enumerate(questions):\n",
    "        futures[question] = []\n",
    "        for _, row in train_set.iterrows():\n",
    "            future = executor.submit(generate_response, row, question, model=\"gpt-4-turbo\")\n",
    "            futures[question].append(future)\n",
    "    \n",
    "    for i, question in enumerate(questions):\n",
    "        gpt_4_turbo_cot_individual[i] = [future.result() for future in tqdm(concurrent.futures.as_completed(futures[question]), total=len(futures[question]), desc=f\"Processing {question}\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13112c8-18c2-40cc-9de0-56524118ccff",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4o_cot_individual = [[] for _ in range(len(questions))]\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = {}\n",
    "    for i, question in enumerate(questions):\n",
    "        futures[question] = []\n",
    "        for _, row in train_set.iterrows():\n",
    "            future = executor.submit(generate_response, row, question, model=\"gpt-4o\")\n",
    "            futures[question].append(future)\n",
    "    \n",
    "    for i, question in enumerate(questions):\n",
    "        gpt_4o_cot_individual[i] = [future.result() for future in tqdm(concurrent.futures.as_completed(futures[question]), total=len(futures[question]), desc=f\"Processing {question}\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995dc220-a1da-450d-aa01-da7ca97c9256",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_35_individual_numeric = [get_last_numeric_values(lst) for lst in gpt_35_cot_individual]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c21f8a-8127-44d8-953f-6bb838169ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4_turbo_individual_numeric = [get_last_numeric_values(lst) for lst in gpt_4_turbo_cot_individual]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be1e122-15ba-49b6-a15e-505b39c2d7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4o_individual_numeric = [get_last_numeric_values(lst) for lst in gpt_4o_cot_individual]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e72f66-9037-44ad-9ed7-6e402e8fb543",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(gpt_35_individual_numeric)):\n",
    "    train_set[f\"gpt_3.5_individual_CoT_q{i+1}_difficulty_score\"] = gpt_35_individual_numeric[i]\n",
    "    train_set[f\"gpt_3.5_individual_CoT_q{i+1}_reasoning\"] = gpt_35_cot_individual[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84201b82-10b0-4a93-8c07-4465df2db559",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(gpt_4_turbo_individual_numeric)):\n",
    "    train_set[f\"gpt_4_turbo_individual_CoT_q{i+1}_difficulty_score\"] = gpt_4_turbo_individual_numeric[i]\n",
    "    train_set[f\"gpt_4_turbo_individual_CoT_q{i+1}_reasoning\"] = gpt_4_turbo_cot_individual[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa9d35f-a049-4c88-838a-819a776a6378",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(gpt_4o_individual_numeric)):\n",
    "    train_set[f\"gpt_4o_individual_CoT_q{i+1}_difficulty_score\"] = gpt_4o_individual_numeric[i]\n",
    "    train_set[f\"gpt_4o_individual_CoT_q{i+1}_reasoning\"] = gpt_4o_cot_individual[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e232071-3290-4ada-ba4d-44f44019d882",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.to_csv(TRAIN_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c496729-49b7-4581-8b4d-5ab3b5b9af50",
   "metadata": {},
   "source": [
    "### One shot prompting + CoT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecb3d47-8840-4976-ad2e-2bc1e59f8fe8",
   "metadata": {},
   "source": [
    "### Pairwise comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f311fc09-9248-48e0-8207-dbe607b8eb1b",
   "metadata": {},
   "source": [
    "### Different relationships between beta and difficulty scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4d826b-1245-48d4-949f-d91b86c1c1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_sigmoid_thresholding_beta(row, difficulty_key, threshold, factor):\n",
    "    return torch.sigmoid(torch.tensor(row[difficulty_key] - threshold) * factor).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a14843a-4da6-40f6-bdb9-3900a8797f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_value = train_set['gpt-3.5_CoT_AG_mean_difficulty_score'].min()\n",
    "max_value = train_set['gpt-3.5_CoT_AG_mean_difficulty_score'].max()\n",
    "\n",
    "train_set['gpt-3.5_CoT_AG_flipped_mean_difficulty_score'] = 1 - train_set['gpt-3.5_CoT_AG_mean_difficulty_score'] - min_value) / (max_value - min_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca9acff-d18b-40d9-9dc6-79db9e61bb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "factors = [3, 10, 30]\n",
    "thresholds = [0.5, 0.7]\n",
    "combinations = list(itertools.product(factors, thresholds))\n",
    "\n",
    "\n",
    "for factor, threshold in combinations:\n",
    "    train_set[f\"gpt-3.5_CoT_AG_mean_sigmoid-{threshold}_{factor}\"] = train_set.apply(lambda row: \n",
    "                                                                    get_sigmoid_thresholding_beta(row, \n",
    "                                                                                                \"gpt-3.5_CoT_AG_flipped_mean_difficulty_score\",\n",
    "                                                                                               threshold, factor), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a204406-9563-424f-bf0a-108d115b57b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [col for col in train_set.columns if \"gpt-3.5_CoT_AG_mean_sigmoid\" in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ea39ac-52a1-4d74-a925-67b364a1db9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(train_set[\"gpt-3.5_CoT_AG_mean_sigmoid-0.7_30\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748fb55a-bbeb-4e1a-b5e3-116550ab6dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.to_csv(TRAIN_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdad850-0d34-408c-a063-dbbfe63ca119",
   "metadata": {},
   "source": [
    "## Did the annotators make decisions based on difficulty?\n",
    "Running logistic regression between a binary variable representing whether or not the correct answer was chosen and the difficulty metric\n",
    "\n",
    "Create dataframes for comparison below:\n",
    "- only correct and incorrect answers\n",
    "- correct and incorrect answers when they are of the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808b2713-9a68-44f5-a9b4-6129952580fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_incorrect_answer_pairs = train_set[train_set[\"tag_IDs\"].apply(lambda x: str(x)[0] != str(x)[2])]\n",
    "correct_incorrect_same_length_pairs = correct_incorrect_answer_pairs[correct_incorrect_answer_pairs[\"tag_IDs\"].apply(lambda x: str(x)[1] == str(x)[3])]\n",
    "correct_incorrect_diff_length_pairs = correct_incorrect_answer_pairs[correct_incorrect_answer_pairs[\"tag_IDs\"].apply(lambda x: str(x)[1] != str(x)[3])]\n",
    "correct_concise_incorrect_detailed = correct_incorrect_diff_length_pairs[correct_incorrect_diff_length_pairs[\"tag_IDs\"].apply(lambda x: str(x)[0:2] == \"11\" or str(x)[2:4] == \"11\")]\n",
    "correct_detailed_incorrect_concise = correct_incorrect_diff_length_pairs[correct_incorrect_diff_length_pairs[\"tag_IDs\"].apply(lambda x: str(x)[0:2] == \"12\" or str(x)[2:4] == \"12\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba07bf9-013a-4f29-9a4e-6af83b6c1a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"All Correct-Incorrect Pairs\", \"Correct-Incorrect Pairs of Same Length\", \"Correct-Incorrect Pairs of Diff. Length\",  \"Correct Concise, Incorrect Detailed\", \"Correct Detailed, Incorrect Concise\"]\n",
    "datasets = [correct_incorrect_answer_pairs, correct_incorrect_same_length_pairs, correct_incorrect_diff_length_pairs, correct_concise_incorrect_detailed, correct_detailed_incorrect_concise]\n",
    "cols = [col for col in list(train_set.columns) if \"difficulty\" in col and \"ground_truth\" not in col and \"judge\" not in col]\n",
    "# cols.remove(\"ground_truth_difficulty\")\n",
    "losses = [[] for i in range(len(cols))]\n",
    "data_dict = dict(zip(cols, losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4529b37d-cf4c-4e9e-8b8d-116a1c1d1665",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_logistic_reg_score(dataset, x_col, y_col=\"correct_chosen\", plot_title=None):\n",
    "    # X = (dataset[x_col].values >= 4).astype(float)[:, None]\n",
    "    X = dataset[x_col].values[:, None]\n",
    "    y = dataset[y_col]\n",
    "    \n",
    "    model = LogisticRegression()\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    probabilities = model.predict_proba(X)\n",
    "    loss = log_loss(y, probabilities) if model.coef_ < 0 else math.log(2)\n",
    "\n",
    "    if plot_title:\n",
    "        plt.hist(X[y == 0], alpha=0.5, label=\"incorrect\")\n",
    "        plt.hist(X[y == 1], alpha=0.5, label=\"correct\")\n",
    "        plt.title(plot_title)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74328fb8-a925-4f69-a554-e941099c9afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col, lst in data_dict.items():\n",
    "    for dataset in datasets:\n",
    "        try:\n",
    "            lst.append(generate_logistic_reg_score(dataset, col))\n",
    "        except:\n",
    "            print(col)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0676a385-4c00-49de-b9df-8851c925a806",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict[\"Labels\"] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052e4913-94be-4073-a238-5152f0d3f10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data_dict)\n",
    "df_transposed = df.transpose()\n",
    "new_header = df_transposed.iloc[-1]  \n",
    "df_transposed = df_transposed[:-1]  \n",
    "df_transposed.columns = new_header "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c75b62-633d-4b83-9016-8b4410b6cbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_min(s):\n",
    "    is_min = s == s.min()\n",
    "    return ['background-color: yellow' if v else '' for v in is_min]\n",
    "\n",
    "styled_df = df_transposed.style.apply(highlight_min)\n",
    "\n",
    "styled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c683e6c-7189-434d-b4a7-53bba9b60512",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.proportion import proportion_confint\n",
    "\n",
    "correct_bins = []\n",
    "counts = []\n",
    "confints = []\n",
    "col = \"gpt-3.5_CoT_AG_mean_sigmoid-0.5_30\"\n",
    "dataset = correct_incorrect_answer_pairs\n",
    "# dataset = correct_incorrect_same_length_pairs\n",
    "\n",
    "# scores = np.arange()\n",
    "scores = np.arange(0, 1.01, 0.333333333333333)\n",
    "\n",
    "for score in scores:\n",
    "    mask = np.abs(dataset[col] - score) < 0.15\n",
    "    count = mask.sum()\n",
    "    counts.append(count)\n",
    "    correct = int(dataset[\"correct_chosen\"].values[mask].sum())\n",
    "    confint = proportion_confint(count=correct, nobs=count, alpha=0.1)\n",
    "    correct_bins.append(correct / count)\n",
    "    confints.append(confint)\n",
    "    \n",
    "# X = (correct_incorrect_answer_pairs[col].values >= 4).astype(float)[:, None]\n",
    "X = dataset[col].values[:, None]\n",
    "y = dataset[\"correct_chosen\"]\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "s = np.linspace(scores.min(), scores.max(), 100)\n",
    "probabilities = model.predict_proba(s[:, None])\n",
    "\n",
    "plt.bar(\n",
    "    scores,\n",
    "    correct_bins,\n",
    "    yerr=np.abs(np.array(confints) - np.array(correct_bins)[:, None]).transpose(),\n",
    "    width = (scores[1] - scores[0]) / 2\n",
    ")\n",
    "plt.plot(s, probabilities[:, 1], c=\"k\", ls=\"--\")\n",
    "plt.xticks(scores, labels=[f\"{score:.1f} ({count})\" for score, count in zip(scores, counts)])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
